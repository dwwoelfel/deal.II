<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>The deal.II Testsuite</title>
  <link href="../screen.css" rel="StyleSheet">
  <meta name="author" content="the deal.II authors <authors@dealii.org>">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013 by the deal.II authors">
  <meta name="date" content="$Date$">
  <meta name="svn_id" content="$Id$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>

    <p class="todo">
      The deal.II testsuite consists of two parts, the
      <a href="#build_tests">build tests</a> and the
      <a href="#regression_tests">regression tests</a>. While the build tests
      just check if the
      library can be compiled on different systems and with different (versions
      of) compilers, the regression tests are actually run and their output
      compared with previously stored. These two testsuites are
      described below.
    </p>

    <p class="todo">
      deal.II has a testsuite that, at the time this article is written
      (mid-2013), has some 2,900 small programs (growing by roughly one per
      day) that we run every time we make a change to make sure that no
      existing functionality is broken. The expected output is also stored in
      our subversion archive, and when you run a test you are notified if a
      test fails. These days, every time we add a significant piece of
      functionality, we add at least one new test to the testsuite, and we
      also do so if we fix a bug, in both cases to make sure that future
      changes do not break what we have just checked in. In addition, some
      machines run the tests every night and send the results back home; this
      is then converted into
      <a href="http://dealii.mathsim.eu/cgi-bin/regression_quick.pl"
      target="body">a webpage showing the status of our regression tests</a>.
    </p>

    <div class="toc">
      <ol>
        <li><a href="#setup">Set up the testsuite</a></li>
        <ol>
          <li><a href="#setupdownload">Download the testsuite</a></li>
          <li><a href="#setupconfigure">Prepare the testsuite</a></li>
        </ol>
        <li><a href="#run">Run the testsuite</a></li>
        <ol>
          <li><a href="#runoutput">How to interpret the output</a></li>
        </ol>
        <li><a href="#layout">Testsuite development</a></li>
        <ol>
          <li><a href="#layoutgeneral">General layout</a></li>
          <li><a href="#layoutcomparisonfile">Comparison file</a></li>
          <li><a href="#layoutaddtests">Adding new tests</a></li>
        </ol>
        <li><a href="#submit">Submit test results</a></li>
        <li><a href="#build_tests">The build tests</a></li>
      </ol>
    </div>

    <a name="setup"></a>
    <h2>Set up the testsuite</h2>

    <p class="todo"> Here, some text is missing</p>

    <a name="setupdownload"></a>
    <h3>Download the testsuite</h3>

    <p>
      In order to run the testsuite you have to download it first. The
      easiest way is to directly check out the testsuite along with deal.II
      from the subversion repository. Go to an empty directory where you
      want to test deal.II and do this:
      <pre>

    $ svn checkout https://svn.dealii.org/trunk .
      </pre>
      (Do not forget the dot "." at the end.) This should leave you with
      two folders:
      <pre>

    ./deal.II
    ./tests
      </pre>
    </p>

    <p>
      <b>Note:</b> If you want to check out the testsuite separately, you
      can do so with
      <pre>

    $ svn checkout https://svn.dealii.org/trunk/tests
      </pre>
    </p>

    <p>
      <b>Note:</b> CMake will pick up any testsuite that is located in a
      <code>tests</code> folder next to the source directory
      (<code>../tests</code>). If your test directory is at a different
      location you have to hint during configuration by specifying
      <code>TEST_DIR</code>:
      <pre>

    $ cmake -DTEST_DIR=&lt;...&gt;
      </pre>
    </p>

    <a name="setupconfigure"></a>
    <h3>Prepare the testsuite</h3>

    <p>
      To enable the testsuite, configure and build deal.II in a build
      directory as normal (installation is not necessary). After that you
      can setup the testsuite via the "setup_test" target:
      <pre>

    $ make setup_test
      </pre>
      This will set up all tests supported by the current configuration
      (and not otherwise disabled due to <code>TEST_PICKUP_REGEX</code>).
      Now, the testsuite can be run in the _build directory_ via the
      <code>ctest</code> command (as will be explained in the next
      section).
    </p>
    Additionally there are also the following targets available:
      <pre>

    $ make clean_test - runs the 'clean' target in every testsuite subproject

    $ make prune_test - removes all testsuite subprojects
      </pre>

    <p>
      The testsuite uses the following CMake variables:
      <pre>

    TEST_PICKUP_REGEX
      - A regular expression to filter tests. If this is a nonempty string
        only tests that match the regular expression will be set up. An empty
        string is interpreted as a catchall.

    TEST_DIFF
      - the diff tool and command line to use for comparison. If numdiff is
        available it defaults to "numdiff -a 1e-6 -q", otherwise plain diff
        is used.

    TEST_TIME_LIMIT
      - The time limit (in seconds) a single test is allowed to run. Defaults
        to 180 seconds
      </pre>
      These options can be set as environment variables prior to the call to the
      setup_test target:
      <pre>

    $ TEST_PICKUP_REGEX="^build_tests/" TEST_TIME_LIMIT="120" make setup_test
      </pre>
    </p>

    <p>
      <b>Note:</b> Specifying these options via environment variables is
      volatile, i.e. if <code>$ make setup_test</code> is invoked a second
      time without the variables set in environment, the option will be
      reset to the default value. If you want to set these options
      permanently, set them via cmake as CMake variable in the build
      directory:
      <pre>

    $ cmake -DTEST_PICKUP_REGEX="&lt;regular expression&gt;" .
      </pre>
      Please also note that a variable set via cmake always _overrides_ one
      set via environment. If you wish to reset such a variable again,
      undefine it in the cache:
      <pre>

    $ cmake -UTEST_PICKUP_REGEX .
      </pre>
    </p>

    <a name="run"></a>
    <h2>Run the testsuite</h2>

    <p>
      Now, the testsuite can be run in the _build directory_ via
      <pre>

    $ ctest [-j x]
      </pre>
      where x is the number of concurrent tests that should be run. The
      testsuite is huge (!) and will need around 12h on current computer
      running single threaded. If you only want to run a subset of tests
      matching a regular expression, you can use
      <pre>

    $ ctest [-j x] -R '&lt;regular expression&gt;'
      </pre>
    </p>

    <p>
      <b>Note:</b> You can also invoke <code>ctest</code> under
      <code>BUILD_DIR/tests</code> or any subdirectory under
      <code>BUILD_DIR/tests</code>. This will only invoke the tests that
      are located under the subdirectory.
    </p>

    <p>
      To get verbose output of tests (which is otherwise just logged into
      <code>Testing/Temporary/LastTest.log</code>) specify
      <pre>

    $ ctest -V [...]
      </pre>
      Alternatively, if you're just interested in verbose output of failing
      tests, <code>--output-on-failure</code>.
    </p>

    <p>
      <b>Note:</b>
      Not all tests succeed on every machine even if all computations are
      correct, because your machine generates slightly different floating
      point outputs. To increase the number of tests that work correctly,
      install the
      <a href="http://www.nongnu.org/numdiff/">numdiff</a> tool that compares
      stored and newly created output files based on floating point
      tolerances. To use it, simply export it via the <code>PATH</code>
      environment variable so that it can be found during
      <code>make setup_test</code>.
    </p>

    <p>
      In a similar vain, there is also an option to disable tests matching a
      regular exression:
      <pre>

    $ ctest -E '&lt;regular expression&gt;' [...]
      </pre>
    </p>

    <a name="runoutput"></a>
    <h3>How to interpret the output</h3>

    <p>
      A typical output of a <code>ctest</code> invocation looks like:
      <pre>

    $ ctest -j4 -R "base/thread_validity"
    Test project /tmp/trunk/build
          Start 747: base/thread_validity_01.debug
          Start 748: base/thread_validity_01.release
          Start 775: base/thread_validity_05.debug
          Start 776: base/thread_validity_05.release
     1/24 Test #776: base/thread_validity_05.release ...   Passed    1.89 sec
     2/24 Test #748: base/thread_validity_01.release ...   Passed    1.89 sec
          Start 839: base/thread_validity_03.debug
          Start 840: base/thread_validity_03.release
     3/24 Test #747: base/thread_validity_01.debug .....   Passed    2.68 sec
    [...]
          Start 1077: base/thread_validity_08.debug
          Start 1078: base/thread_validity_08.release
    16/24 Test #1078: base/thread_validity_08.release ...***Failed    2.86 sec
    18/24 Test #1077: base/thread_validity_08.debug .....***Failed    3.97 sec
    [...]

    92% tests passed, 2 tests failed out of 24

    Total Test time (real) =  20.43 sec

    The following tests FAILED:
            1077 - base/thread_validity_08.debug (Failed)
            1078 - base/thread_validity_08.release (Failed)
    Errors while running CTest
      </pre>
      If a test failed (like <code>base/thread_validity_08.debug</code> in above
      example output), you might want to find out what exactly went wrong.
      So, invoke <code>ctest</code> to just run the above test with verbose
      output:
      <pre>

    $ ctest -V -R "base/thread_validity_08.debug"
    [...]
    test 1077
        Start 1077: base/thread_validity_08.debug

    1077: Test command: [...]
    1077: Test timeout computed to be: 600
    1077: Test base/thread_validity_08.debug: RUN
    1077: ===============================   OUTPUT BEGIN  ===============================
    1077: Built target thread_validity_08.debug
    1077: Generating thread_validity_08.debug/output
    1077: terminate called without an active exception
    1077: /bin/sh: line 1: 18030 Aborted [...]/thread_validity_08.debug
    1077: base/thread_validity_08.debug: BUILD successful.
    1077: base/thread_validity_08.debug: RUN failed. Output:
    1077: DEAL::OK.
    1077: gmake[3]: *** [thread_validity_08.debug/output] Error 1
    1077: gmake[2]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/all] Error 2
    1077: gmake[1]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/rule] Error 2
    1077: gmake: *** [thread_validity_08.debug.diff] Error 2
    1077:
    1077:
    1077: base/thread_validity_08.debug: ******    RUN failed    *******
    1077:
    1077: ===============================    OUTPUT END   ===============================
      </pre>
      So this specific test aborted in the <code>RUN</code> stage.
    </p>

    <p>
      The general output for a successful test <code>&lt;test&gt;</code> in
      category <code>&lt;category&gt;</code> for build type
      <code>&lt;build&gt;</code> is
      <pre>

    xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED
    xx: ===============================   OUTPUT BEGIN  ===============================
    xx: [...]
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED.
    xx: ===============================    OUTPUT END   ===============================
      </pre>
      And for a test that fails in stage <code>&lt;stage&gt;</code>:
      <pre>

    xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt;
    xx: ===============================   OUTPUT BEGIN  ===============================
    xx: [...]
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt; failed. [...]
    xx:
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: ******    &lt;stage&gt; failed    *******
    xx: ===============================    OUTPUT END   ===============================
      </pre>
      Hereby, <code>&lt;stage&gt;</code> indicates the stage in which the
      test failed:
      <ul>
        <li>
          <code>CONFIGURE</code>: only for test in the "build_tests"
          category: The test project failed in the configuration stage
        </li>
        <li>
          <code>BUILD</code>: compilation error occured
        </li>
        <li>
          <code>RUN</code>: the test executable could not be run / aborted
        </li>
        <li>
          <code>DIFF</code>: the test output differs from the reference output
        </li>
        <li>
          <code>PASSED</code>: the test run successful
        </li>
      </ul>



    <a name="layout"></a>
    <h3>Testsuite development</h3>

    <p class="todo"> Here, some text is missing</p>



    <a name="layoutgeneral"></a>
    <h3>General layout</h3>

    <p>
      A test usually consists of a source file and an output file for
      comparison (under the testsuite directory <code>tests</code>):
      <pre>

    category/test.cc
    category/test[...].output
      </pre>
      <code>test.cc</code> must be a regular executable (i.e. having an
      <code>int main()</code> routine). It will be compiled, linked and
      run. The executable should not output anything to <code>cout</code>
      (at least under normal circumstances, i.e. no error condition),
      instead the executable should output to a file <code>output</code>
      under the current working directory.
    </p>
    <p>
      In detail, for a regular test the following 3 stages will be run:
      <ul>
        <li>
          <code>BUILD</code>: The build stage generates an executable in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;</code>.
        </li>
        <li>
          <code>RUN</code>: The run stages invokes the executable that
          generates an output file
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/output</code>.
          If the run fails (e.g. because the program aborts with an error
          code) the file <code>output</code> is renamed to
          <code>failing_output</code>.
        </li>
        <li>
          <code>DIFF</code>: As a last stage the generated output file will
          be compared to
          <code>SOURCE_DIR/tests/&lt;category&gt;/&lt;test&gt;[...].output</code>.
          and stored in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/diff</code>.
          If the diff fails  the file <code>diff</code> is renamed to
          <code>failing_diff</code>.
        </li>
      </ul>

    <p>
    </p>

    <a name="layoutcomparisonfile"></a>
    <h3>Comparison file</h3>

    <p>
      The full file signature for a comparison file is
      <pre>

    category/test.[with_&lt;feature&gt;=&lt;on|off&gt;.]*[mpirun=&lt;x&gt;.][&lt;debug|release&gt;.]output
      </pre>
      which is explained in detail below.
    </p>

    <h4>Restrict tests for build configurations</h4>
    <p>
      Normally, a test will be set up for debug and release configuration
      (if deal.II was configured with combined <code>DebugRelease</code>
      build type) or for the available build configuration (if deal.II was
      configured either with <code>Debug</code> or with
      <code>Release</code> only build type).
      If a specific test can only be run in debug or release configurations but
      not in both it is possible to restrict the setup by prepeding
      <code>.debug</code> or <code>.release</code> directly before
      <code>.output</code>, e.g.:
      <pre>

    category/test.debug.output
      </pre>
      This way, test will only be set up to build and run against the debug
      library.
    </p>

    <p>
      <b>Note:</b> It is possible to provide both configuration types at the
      same time:
      <pre>

        category/test.debug.output
        category/test.release.output
      </pre>
      This will set up two seperate tests, one for the debug configuration that
      will be tested against test.debug.output, and similarly one for release.

    <h4>Restrict tests for feature configurations</h4>
    <p>
      In a similar vain as for build configurations, it is possible to restrict
      tests to specific feature configurations, e.g.:
      <pre>

    category/test.with_umfpack=on.output, or
    category/test.with_zlib=off.output
      </pre>
      These tests will only be set up if the specified feature was configured
      accordingly.
    </p>

    <p>
      <b>Note:</b> It is possible to provide different output files for disabled/enabled
      features, e.g.
      <pre>

    category/test.with_64bit_indices=on.output
    category/test.with_64bit_indices=off.output
      </pre>
    </p>
    <p>
      <b>Note:</b> It is possible to declare multiple constraints subsequently, e.g.
      <pre>

    category/test.with_umfpack=on.with_zlib=on.output
      </pre>
    </p>
    <p>
      <b>Note:</b> Quite a number of test categories are already guarded so
      that the contained tests will only be set up if the feature is
      enabled. In this case a feature constraint in the output file name is
      redundant and should be avoided. (Folders with guards are
      <code>distributed_grids</code>, <code>lapack</code>,
      <code>metis</code>, <code>petsc</code>, <code>slepc</code>,
      <code>trilinos</code>, <code>umfpack</code>, <code>gla</code>,
      <code>mpi</code>)
    </p>

    <h4>Run mpi tests with mpirun</h4>
    <p>
      If a test should be run with mpirun in parallel, specify the number x of
      simultaneous processes in the following way:
      <pre>

    category/test.mpirun=x.output
      </pre>
    </p>
    <p>
      <b>Note:</b> It is possible to provide multiple output files for different mpirun
      values.


    <a name="layoutaddtests"></a>
    <h3>Adding new tests</h3>

    <p>
    As mentioned above, we add a new test every
    time we add new functionality to the library or fix a bug. If you
    want to contribute code to the library, you should do this
    as well. Here's how: you need a testcase,
    a subdirectory with the same name as the test, and a file with the
    expected output.
    </p>

    <h4>The testcase</h4>
    <p>
    For the testcase, we usually start from a template like this:
    <pre class="cmake"> <!-- TODO -->
// ---------------------------------------------------------------------
// $Id$
//
// Copyright (C) 2013 by the deal.II authors
//
// This file is part of the deal.II library.
//
// The deal.II library is free software; you can use it, redistribute
// it, and/or modify it under the terms of the GNU Lesser General
// Public License as published by the Free Software Foundation; either
// version 2.1 of the License, or (at your option) any later version.
// The full text of the license can be found in the file LICENSE at
// the top level of the deal.II distribution.
//
// ---------------------------------------------------------------------


// a short (a few lines) description of what the program does

#include "../tests.h"
#include <iostream>
#include <fstream>

// all include files you need here


int main ()
{
  std::ofstream logfile("output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
    </pre>

    <p>You open an output file <code>output</code> in the current working
    directory and then write all output you generate to it, through the
    <code>deallog</code> stream. The <code>deallog</code> stream works like
    any other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In above
    case, we only write a zero to the output file. Most tests actually
    write computed data to the output file to make sure that whatever we
    compute is what we got when the test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>,
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located.
    </p>

    <p>
    We have started to create more atomic tests which
    are usually very small and test only a single aspect of the
    library, often only a single function. These tests go into the
    <code>bits/</code> directory and often have names that are
    composed of the name of the class being tested and a two-digit
    number, e.g., <code>dof_tools_11</code>. There are
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>An expected output</h4>

    <p>
      In order to run your new test, copy it to an appropriate category and
      create an empty comparison file for it:
      <pre>

    category/my_new_test.cc
    category/my_new_test.output
      </pre>
      Now, rerun
      <pre>

    $ make setup_test
      </pre>
      so that your new test is picked up. After that it is possible to
      invoke it with
      <pre>

    $ ctest -V -R "category/my_new_test"
      </pre>
    </p>

    <p>
      If you run your new test executable this way, the test should compile
      and run successfully but fail in the diff stage (due to the empty
      comparison file). You will get an output file
      <code>BUILD_DIR/category/my_new_test/output</code> that should be
      used to compare all future runs with. If the test is relatively
      simple, it is often a good idea to look at the output and make sure
      that the output is actually what you had expected. However, if you do
      complex operations, this may sometimes be impossible, and in this
      case we are quite happy with any reasonable output file just to make
      sure that future invokations of the test yield the same results.
    </p>

    <p>
      The next step is to copy and rename this output file to the source
      directory and replace the original comparison file with it:
      <pre>

    category/my_new_test.output
      </pre>
      At this point running the test again should be successful:
      <pre>

    $ ctest -V -R "category/my_new_test"
      </pre>
    </p>

    <h4>Checking in</h4>

    <p>
      Tests are a way to make sure everything keeps working. If they
      aren't automated, they are no good. We are therefore very
      interested in getting new tests. If you have subversion write access
      already, you can add the new test and the expected output
      file:
      <pre>

    svn add category/my_new_test.cc
    svn add category/my_new_test.output
    svn commit -m "New test"
      </pre>
      If you don't have subversion write access, talk to us in the
      discussion group; writing testcases is a worthy and laudable task,
      and we would like to encourage it by giving people the opportunity to
      contribute!
    </p>



    <a name="submit"></a>
    <h2>Submit test results</h2>

    <p class="todo">
      Explain how to use <code>run_testsuite.cmake</code> in all imaginable
      ways...
    </p>



    <a name="build_tests"></a>
    <h2>The build tests</h2>

    <p class="todo">
      Update this section
    </p>

    <p>
      With our build tests, we check if deal.II can be compiled on
      different systems and with different compilers as well as
      different configuration options. Results are collected in a
      database and can be accessed <a
      href="http://www.dealii.org/testsuite.html">online</a>.<p>

      <p>Running the build test suite is simple and we encourage deal.II
      users with configurations not found on the <a
      href="http://www.dealii.org/testsuite.html">test suite page</a> to
      participate. Assuming you checked out deal.II into the directory
      <code>dealtest</code>, running it is as simple as:
      <pre>

    cd dealtest
    svn update
    ./contrib/utilities/build_test
    mail build-tests@dealii.org &lt; *.log
  ( rm *.log )
      </pre>
    </p>

    <p>
      The <code>build_test</code> script supports the following options:
      <pre>

    SOURCEDIR     - the source directory to use (otherwise the current directory is used)
    CONFIGFILE    - A cmake configuration file for the build test
    LOGDIR        - directory for the log file
    LOGFILE       - the logfile to use, defaults to
                        $LOGDIR/$BRANCH.$CONFIGFILE.<unix time>.log

    CMAKE         - the cmake executable to use
    SVN           - svn info command to use, defaults to
                        svn info $(SOURCEDIR)
    TMPDIR        - defaults to "/tmp"
    CLEAN_TMPDIR  - defaults to "true"
    RUN_EXAMPLES  - defaults to "true"
      </pre>
      An example configuration file can be found <a
      href="../users/Config.sample">here</a>. Options can be passed either via
    environment
      <pre>

    export CONFIGFILE=MyConfiguration.conf
    ./contrib/utilities/build_test
      </pre>
      or directly on the command line:
      <pre>

    ./contrib/utilities/build_test CONFIGFILE=myConfiguration.conf
      </pre>
    </p>

    <p>
      A status indicator should appear on the build test website after some
      time (results are collected and processed by a program that is run
      periodically, but not immediately after a mail has been received).
    </p>

    <h3>Dedicated build tests</h3>

    <p>
      There is a detailed example for dedicated build tests on the <a
        href="https://code.google.com/p/dealii/wiki/BuildTests">wiki</a>.
    </p>


    <hr />
    <address>
      <a href="../authors.html" target="body">The deal.II Authors</a>
      $Date$
    </address>
    <div class="right">
      <a href="http://validator.w3.org/check?uri=referer" target="_top">
        <img style="border:0" src="http://www.w3.org/Icons/valid-html401" alt="Valid HTML 4.01!"></a>
      <a href="http://jigsaw.w3.org/css-validator/check/referer" target="_top">
        <img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"></a>
    </div>

  </body>
</html>
