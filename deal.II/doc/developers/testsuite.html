<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>The deal.II Testsuite</title>
  <link href="../screen.css" rel="StyleSheet">
  <meta name="author" content="the deal.II authors <authors@dealii.org>">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013 by the deal.II authors">
  <meta name="date" content="$Date$">
  <meta name="svn_id" content="$Id$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>

    TODO: Das ist nicht mehr aktuell
    <p>The deal.II testsuite consists of two parts, the
    <a href="#build_tests">build tests</a> and the
    <a href="#regression_tests">regression tests</a>. While the build tests
    just check if the
    library can be compiled on different systems and with different (versions
    of) compilers, the regression tests are actually run and their output
    compared with previously stored. These two testsuites are
    described below.</p>

    <div class="toc">
      <ol>
        <li><a href="#setup">Set up the testsuite</a></li>
        <ol>
          <li><a href="#setupdownload">Download the testsuite</a></li>
          <li><a href="#setupconfigure">Prepare the testsuite</a></li>
        </ol>
        <li><a href="#run">Run the testsuite</a></li>
        <ol>
          <li><a href="#runoutput">Interpreting the output</a></li>
        </ol>
        <li><a href="#build_tests">The build tests</a></li>
        <li><a href="#regression_tests">The regression tests</a></li>
      </ol>
    </div>

    <a name="setup"></a>
    <h2>Set up the testsuite</h2>

    <a name="setupdownload"></a>
    <h3>Download the testsuite</h3>

    <p>
      In order to run the testsuite you have to download it first. The
      easiest way is to directly check out the testsuite along with deal.II
      from the subversion repository. Go to an empty directory where you
      want to test deal.II and do this:
      <pre>

    $ svn checkout https://svn.dealii.org/trunk .
      </pre>
      (Do not forget the dot "." at the end.) This should leave you with
      two folders:
      <pre>

    ./deal.II
    ./tests
      </pre>
    </p>

    <p>
      <b>Note:</b> If you want to check out the testsuite separately, you
      can do so with
      <pre>

    $ svn checkout https://svn.dealii.org/trunk/tests
      </pre>
    </p>

    <p>
      <b>Note:</b> CMake will pick up any testsuite that is located in a
      <code>tests</code> folder next to the source directory
      (<code>../tests</code>). If your test directory is at a different
      location you have to hint during configuration by specifying
      <code>TEST_DIR</code>:
      <pre>

    $ cmake -DTEST_DIR=&lt;...&gt;
      </pre>
    </p>

    <a name="setupconfigure"></a>
    <h3>Prepare the testsuite</h3>

    <p>
      To enable the testsuite, configure and build deal.II in a build
      directory as normal (installation is not necessary). After that you
      can setup the testsuite via the "setup_test" target:
      <pre>

    $ make setup_test
      </pre>
      This will set up all tests supported by the current configuration
      (and not otherwise disabled due to <code>TEST_PICKUP_REGEX</code>).
      Now, the testsuite can be run in the _build directory_ via the
      <code>ctest</code> command (as will be explained in the next
      section).
    </p>
    Additionally there are also the following targets available:
      <pre>

    $ make clean_test - runs the 'clean' target in every testsuite subproject

    $ make prune_test - removes all testsuite subprojects
      </pre>

    <p>
      The testsuite uses the following CMake variables:
      <pre>

    TEST_PICKUP_REGEX
      - A regular expression to filter tests. If this is a nonempty string
        only tests that match the regular expression will be set up. An empty
        string is interpreted as a catchall.

    TEST_DIFF
      - the diff tool and command line to use for comparison. If numdiff is
        available it defaults to "numdiff -a 1e-6 -q", otherwise plain diff
        is used.

    TEST_TIME_LIMIT
      - The time limit (in seconds) a single test is allowed to run. Defaults
        to 180 seconds
      </pre>
      These options can be set as environment variables prior to the call to the
      setup_test target:
      <pre>

    $ TEST_PICKUP_REGEX="^build_tests/" TEST_TIME_LIMIT="120" make setup_test
      </pre>
    </p>

    <p>
      <b>Note:</b> Specifying these options via environment variables is
      volatile, i.e. if <code>$ make setup_test</code> is invoked a second
      time without the variables set in environment, the option will be
      reset to the default value. If you want to set these options
      permanently, set them via cmake as CMake variable in the build
      directory:
      <pre>

    $ cmake -DTEST_PICKUP_REGEX="&lt;regular expression&gt;" .
      </pre>
      Please also note that a variable set via cmake always _overrides_ one
      set via environment. If you wish to reset such a variable again,
      undefine it in the cache:
      <pre>

    $ cmake -UTEST_PICKUP_REGEX .
      </pre>
    </p>

    <a name="run"></a>
    <h2>Run the testsuite</h2>

    <p>
      Now, the testsuite can be run in the _build directory_ via
      <pre>

    $ ctest [-j x]
      </pre>
      where x is the number of concurrent tests that should be run. The
      testsuite is huge (!) and will need around 12h on current computer
      running single threaded. If you only want to run a subset of tests
      matching a regular expression, you can use
      <pre>

    $ ctest [-j x] -R '&lt;regular expression&gt;'
      </pre>
    </p>

    <p>
      <b>Note:</b> You can also invoke <code>ctest</code> under
      <code>BUILD_DIR/tests</code> or any subdirectory under
      <code>BUILD_DIR/tests</code>. This will only invoke the tests that
      are located under the subdirectory.
    </p>

    <p>
      To get verbose output of tests (which is otherwise just logged into
      <code>Testing/Temporary/LastTest.log</code>) specify
      <pre>

    $ ctest -V [...]
      </pre>
      Alternatively, if you're just interested in verbose output of failing
      tests, <code>--output-on-failure</code>.
    </p>

    <p>
      <b>Note:</b>
      Not all tests succeed on every machine even if all computations are
      correct, because your machine generates slightly different floating
      point outputs. To increase the number of tests that work correctly,
      install the
      <a href="http://www.nongnu.org/numdiff/">numdiff</a> tool that compares
      stored and newly created output files based on floating point
      tolerances. To use it, simply export it via the <code>PATH</code>
      environment variable so that it can be found during
      <code>make setup_test</code>.
    </p>

    <p>
      In a similar vain, there is also an option to disable tests matching a
      regular exression:
      <pre>

    $ ctest -E '&lt;regular expression&gt;' [...]
      </pre>
    </p>

    <a name="runoutput"></a>
    <h3>Interpreting the output</h3>

    <p>
      A typical output of a <code>ctest</code> invocation looks like:
      <pre>

    $ ctest -j4 -R "base/thread_validity"
    Test project /tmp/trunk/build
          Start 747: base/thread_validity_01.debug
          Start 748: base/thread_validity_01.release
          Start 775: base/thread_validity_05.debug
          Start 776: base/thread_validity_05.release
     1/24 Test #776: base/thread_validity_05.release ...   Passed    1.89 sec
     2/24 Test #748: base/thread_validity_01.release ...   Passed    1.89 sec
          Start 839: base/thread_validity_03.debug
          Start 840: base/thread_validity_03.release
     3/24 Test #747: base/thread_validity_01.debug .....   Passed    2.68 sec
    [...]
          Start 1077: base/thread_validity_08.debug
          Start 1078: base/thread_validity_08.release
    16/24 Test #1078: base/thread_validity_08.release ...***Failed    2.86 sec
    18/24 Test #1077: base/thread_validity_08.debug .....***Failed    3.97 sec
    [...]

    92% tests passed, 2 tests failed out of 24

    Total Test time (real) =  20.43 sec

    The following tests FAILED:
            1077 - base/thread_validity_08.debug (Failed)
            1078 - base/thread_validity_08.release (Failed)
    Errors while running CTest
      </pre>
      If a test failed (like <code>base/thread_validity_08.debug</code> in above
      example output), you might want to find out what exactly went wrong.
      So, invoke <code>ctest</code> to just run the above test with verbose
      output:
    <pre>

      $ ctest -V -R "base/thread_validity_08.debug"
      [...]
      test 1077
          Start 1077: base/thread_validity_08.debug

      1077: Test command: [...]
      1077: Test timeout computed to be: 600
      1077: Test base/thread_validity_08.debug: RUN
      1077: ===============================   OUTPUT BEGIN  ===============================
      1077: Built target thread_validity_08.debug
      1077: Generating thread_validity_08.debug/output
      1077: terminate called without an active exception
      1077: /bin/sh: line 1: 18030 Aborted [...]/thread_validity_08.debug
      1077: base/thread_validity_08.debug: BUILD successful.
      1077: base/thread_validity_08.debug: RUN failed. Output:
      1077: DEAL::OK.
      1077: gmake[3]: *** [thread_validity_08.debug/output] Error 1
      1077: gmake[2]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/all] Error 2
      1077: gmake[1]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/rule] Error 2
      1077: gmake: *** [thread_validity_08.debug.diff] Error 2
      1077:
      1077:
      1077: base/thread_validity_08.debug: ******    RUN failed    *******
      1077:
      1077: ===============================    OUTPUT END   ===============================
    </pre>
      So this specific test aborted in the <code>RUN</code> stage.






    <br />
    <br />
    <br />
    <br />
    <br />
    <br />
    <br />
    <br />
    <hr />
    <hr />
    <hr />
    <hr />
    <a name="build_tests"></a>
    <h2>The build tests</h2>

    <p>
      With our build tests, we check if deal.II can be compiled on
      different systems and with different compilers as well as
      different configuration options. Results are collected in a
      database and can be accessed <a
      href="http://www.dealii.org/testsuite.html">online</a>.<p>

      <p>Running the build test suite is simple and we encourage deal.II
      users with configurations not found on the <a
      href="http://www.dealii.org/testsuite.html">test suite page</a> to
      participate. Assuming you checked out deal.II into the directory
      <code>dealtest</code>, running it is as simple as:
      <pre>

    cd dealtest
    svn update
    ./contrib/utilities/build_test
    mail build-tests@dealii.org &lt; *.log
  ( rm *.log )
      </pre>
    </p>

    <p>
      The <code>build_test</code> script supports the following options:
      <pre>

    SOURCEDIR     - the source directory to use (otherwise the current directory is used)
    CONFIGFILE    - A cmake configuration file for the build test
    LOGDIR        - directory for the log file
    LOGFILE       - the logfile to use, defaults to
                        $LOGDIR/$BRANCH.$CONFIGFILE.<unix time>.log

    CMAKE         - the cmake executable to use
    SVN           - svn info command to use, defaults to
                        svn info $(SOURCEDIR)
    TMPDIR        - defaults to "/tmp"
    CLEAN_TMPDIR  - defaults to "true"
    RUN_EXAMPLES  - defaults to "true"
      </pre>
      An example configuration file can be found <a
      href="../users/Config.sample">here</a>. Options can be passed either via
    environment
      <pre>

    export CONFIGFILE=MyConfiguration.conf
    ./contrib/utilities/build_test
      </pre>
      or directly on the command line:
      <pre>

    ./contrib/utilities/build_test CONFIGFILE=myConfiguration.conf
      </pre>
    </p>

    <p>
      A status indicator should appear on the build test website after some
      time (results are collected and processed by a program that is run
      periodically, but not immediately after a mail has been received).
    </p>

    <h3>Dedicated build tests</h3>

    <p>
      There is a detailed example for dedicated build tests on the <a
        href="https://code.google.com/p/dealii/wiki/BuildTests">wiki</a>.
    </p>



    <a name="regression_tests"></a>
    <h2>The regression tests</h2>

    <p>
      deal.II has a testsuite that, at the time this article is written
      (mid-2013), has some 2,900 small programs (growing by roughly one per
      day) that we run every time we make a change to make sure that no
      existing functionality is broken. The expected output is also stored in
      our subversion archive, and when you run a test you are notified if a
      test fails. These days, every time we add a significant piece of
      functionality, we add at least one new test to the testsuite, and we
      also do so if we fix a bug, in both cases to make sure that future
      changes do not break what we have just checked in. In addition, some
      machines run the tests every night and send the results back home; this
      is then converted into
      <a href="http://dealii.mathsim.eu/cgi-bin/regression_quick.pl"
      target="body">a webpage showing the status of our regression tests</a>.
    </p>

    <p>
      If you develop parts of deal.II, want to add something, or fix a bug
      in it, we encourage you to use our testsuite. This page documents
      some aspects of it.
    </p>



    <h3>Running it</h3>




    <h3>Adding new tests</h3>

    <p>
    As mentioned above, we add a new test every
    time we add new functionality to the library or fix a bug. If you
    want to contribute code to the library, you should do this
    as well. Here's how: you need a testcase,
    a subdirectory with the same name as the test, and a file with the
    expected output.
    </p>

    <h4>The testcase</h4>
    <p>
    For the testcase, we usually start from a template like this:
    <pre>

// ---------------------------------------------------------------------
// $Id$
//
// Copyright (C) 2013 by the deal.II authors
//
// This file is part of the deal.II library.
//
// The deal.II library is free software; you can use it, redistribute
// it, and/or modify it under the terms of the GNU Lesser General
// Public License as published by the Free Software Foundation; either
// version 2.1 of the License, or (at your option) any later version.
// The full text of the license can be found in the file LICENSE at
// the top level of the deal.II distribution.
//
// ---------------------------------------------------------------------


// a short (a few lines) description of what the program does

#include "../tests.h"
#include <iostream>
#include <fstream>

// all include files you need here


int main ()
{
  std::ofstream logfile("my_new_test/output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
    </pre>

    <p>You open an output file in a directory with the same
    name as your test, and then write
    all output you generate to it,
    through the <code>deallog</code> stream.   The <code>deallog</code>
    stream works like any
    other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In
    above case, we only write a zero to the output
    file. Most tests actually write computed data to the output file
    to make sure that whatever we compute is what we got when the
    test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>,
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located.
    </p>

    <p>
    We have started to create more atomic tests which
    are usually very small and test only a single aspect of the
    library, often only a single function. These tests go into the
    <code>bits/</code> directory and often have names that are
    composed of the name of the class being tested and a two-digit
    number, e.g., <code>dof_tools_11</code>. There are
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>A directory with the same name as the test</h4>

    <p> You have to create a subdirectory
    with the same name as your test to hold the output from the test.

    <p> One convenient way to create this subdirectory with the correct
    properties is to use svn copy.
    <pre>

    svn copy existing_test_directory my_new_test
    </pre>

    <p>
    Once you have done this, you can try to run
    <pre>

      make my_new_test/output
    </pre>
    This should compile, link, and run your test. Running your test
    should generate the desired output file.
    </p>



    <h4>An expected output</h4>

    <p>
    If you run your new test executable, you will get an output file
    <code>mytestname/output</code> that should be used to compare all future
    runs with. If the test
    is relatively simple, it is often a good idea to look at the
    output and make sure that the output is actually what you had
    expected. However, if you do complex operations, this may
    sometimes be impossible, and in this case we are quite happy with
    any reasonable output file just to make sure that future
    invokations of the test yield the same results.
    </p>

    <p>
    The next step is to copy this output file to the place where the
    scripts can find it when they compare with newer runs. For this, you first
    have to understand how correct results are verified. It works in the
    following way: for each test, we have subdirectories
    <code>testname/cmp</code> where we store the expected results in a file
    <code>testname/cmp/generic</code>. If you create a new test, you should
    therefore create this directory, and copy the output of your program,
    <code>testname/output</code> to <code>testname/cmp/generic</code>.
    </p>

    <p>
    Why <code>generic</code>? The reason is that sometimes test results
    differ slightly from platform to platform, for example because numerical
    roundoff is different due to different floating point implementations on
    different CPUs. What this means is that sometimes a single stored output is
    not enough to verify that a test functioned properly: if you happen to be
    on a platform different from the one on which the generic output was
    created, your test will always fail even though it produces almost exactly
    the same output.
    </p>

    <p>
    To avoid this, what the makefiles do is to first check whether an output
    file is stored for this test and your particular configuration (platform
    and compiler). If this isn't the case, it goes through a hierarchy of files
    with related configurations, and only if none of them does it take the
    generic output file. It then compares the output of your test run with the
    first file it found in this process. To make things a bit clearer, if you
    are, for example, on a <code>i686-pc-linux-gnu</code> box and use
    <code>gcc4.0</code> as your compiler, then the following files will be
    sought (in this order):
    <pre>

testname/cmp/i686-pc-linux-gnu+gcc4.0
testname/cmp/i686-pc-linux-gnu+gcc3.4
testname/cmp/i686-pc-linux-gnu+gcc3.3
testname/cmp/generic
    </pre>
    (This list is generated by the <code>tests/hierarchy.pl</code> script.)
    Your output will then be compared with the first one that is actually
    found. The virtue of this is that we don't have to store the output files
    from all possible platforms (this would amount to gigabytes of data), but
    that we only have store an output file for gcc4.0 if it differs from that
    of gcc3.4, and for gcc3.4 if it differs from gcc3.3. If all of them are the
    same, we would only have the generic output file.
    </p>

    <p>
    Most of the time, you will be able to generate output files only
    for your own platform and compiler, and that's alright: someone
    else will create the output files for other platforms
    eventually. You only have to copy your output file to
    <code>testname/cmp/generic</code>.
    </p>

    <p>
    At this point you can run
    <pre>

      make my_new_test/OK
    </pre>
    which should compare the present output with what you have just
    copied into the compare directory. This should, of course,
    succeed, since the two files should be identical.
    </p>

    <p>
    On the other hand, if you realize that an existing test fails on your
    system, but that the differences (as shown when running with
    <code>verbose=on</code>, see above) are only marginal and around the 6th or
    8th digit, then you should check in your output file for the platform you
    work on. For this, you could copy <code>testname/output</code> to
    <code>testname/cmp/myplatform+compiler</code>, but your life can be easier
    if you simply type
    <pre>

      make my_new_test/ref
    </pre>
    which takes your output and copies it to the right place automatically.
    </p>




    <h4>Checking in</h4>

    <p>
    Tests are a way to make sure everything keeps working. If they
    aren't automated, they are no good. We are therefore very
    interested in getting new tests. If you have subversion write access
    already, you can add the new test and the expected output
    file:
    <pre>

      svn add bits/my_new_test.cc
      svn add bits/my_new_test
      svn add bits/my_new_test/cmp
      svn add bits/my_new_test/cmp/generic
      svn commit -m "New test" bits/my_new_test*
    </pre>
    In addition, you should do the following in order to avoid that the files
    generated while running the testsuite show up in the output of <code>svn
    status</code> commands:
    <pre>

      svn propset svn:ignore "obj.*
        exe
        output
        status
        OK" bits/my_new_test
      svn commit -m "Ignore generated files." bits/my_new_test
    </pre>
    Note that the list of files given in quotes to the propset command extends
    over several lines.
    </p>

    <p>
    If you don't have subversion write access, talk to us in the discussion group;
    writing testcases is a worthy and laudable task, and we would
    like to encourage it by giving people the opportunity to
    contribute!
    </p>

    <hr />
    <address>
      <a href="../authors.html" target="body">The deal.II Authors</a>
      $Date$
    </address>
    <div class="right">
      <a href="http://validator.w3.org/check?uri=referer" target="_top">
        <img style="border:0" src="http://www.w3.org/Icons/valid-html401" alt="Valid HTML 4.01!"></a>
      <a href="http://jigsaw.w3.org/css-validator/check/referer" target="_top">
        <img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"></a>
    </div>

  </body>
</html>
