<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                 "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>The deal.II Testsuite</title>
  <link href="../screen.css" rel="StyleSheet">
  <meta name="author" content="the deal.II authors <authors@dealii.org>">
  <meta name="copyright" content="Copyright (C) 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013 by the deal.II authors">
  <meta name="date" content="$Date$">
  <meta name="svn_id" content="$Id$">
  <meta name="keywords" content="deal dealii finite elements fem triangulation">
  <meta http-equiv="content-language" content="en">
</head>
<body>


    <h1>The deal.II Testsuite</h1>

    <p>
      The deal.II testsuite consists of two parts: 
      <i>build tests</i> and the
      <i>regression testsuite</i>. While the build tests
      are used to check that the
      library can be compiled on different systems and with different (versions
      of) compilers, the regression tests are actually run and their output
      compared with previously stored output files to verify that what
      worked yesterday still works today. These two testsuites are
      described below.
    </p>

    <p>
      deal.II has a testsuite that has, at the time this article is written
      (mid-2013), some 2,900 small programs (growing by roughly one per day)
      that we run every time we make a change to make sure that no existing
      functionality is broken. The expected output for every test is stored in
      our subversion archive, and when you run a test you are notified if a
      test produces different output. These days, every time we add a
      significant piece of functionality, we add at least one new test to the
      testsuite, and we also do so if we fix a bug, in both cases to make sure
      that future changes do not break what we have just checked in. Machines
      running tests send results
      back home and these are then converted into
      <a href="http://dealii.mathsim.eu/cgi-bin/regression_quick.pl"
      target="body">a webpage showing the status of our regression tests</a>.
    </p>

    <div class="toc">
      <ol>
        <li><a href="#quick">Quick instructions</a></li>
        <li><a href="#setup">Setting up the testsuite</a></li>
        <ol>
          <li><a href="#setupdownload">Downloading the testsuite</a></li>
          <li><a href="#setupconfigure">Preparing the testsuite</a></li>
        </ol>
        <li><a href="#run">Running the testsuite</a></li>
        <ol>
          <li><a href="#runoutput">How to interpret the output</a></li>
        </ol>
        <li><a href="#layout">Testsuite development</a></li>
        <ol>
          <li><a href="#layoutgeneral">General layout</a></li>
          <li><a href="#restrictbuild">Restricting tests for build configurations</a></li>
          <li><a href="#restrictfeature">Restricting tests for feature configurations</a></li>
          <li><a href="#mpi">Running tests with MPI</a></li>
          <li><a href="#layoutaddtests">Adding new tests</a></li>
        </ol>
        <li><a href="#submit">Submitting test results</a></li>
        <li><a href="#build_tests">Build tests</a></li>
	  <ol>
	    <li><a href="#dedicatedbuilds">Dedicated build tests</a></li>
          </ol>
      </ol>
    </div>

    <a name="quick"></a>
    <h2>Quick instructions</h2>

    <p>
      If you're impatient, use the following commands:
      <pre>
    $ mkdir new_directory
    $ cd new_directory
    $ svn checkout https://svn.dealii.org/trunk .
    $ mkdir build
    $ cd build
    $ cmake ../deal.II
    $ make -j16
    $ make -j16 setup_tests
    $ ctest -j16
      </pre>
      The exact meaning of all of these commands will be explained in much
      greater detail below.
    </p>

    <a name="setup"></a>
    <h2>Setting up the testsuite</h2>

    <p> In order to run it, you need to download and set up the testsuite
      first. The following paragraphs detail how to do that.
    </p>

    <a name="setupdownload"></a>
    <h3>Downloading the testsuite</h3>

    <p>
      To download the testsuite, check it out from the subversion repository,
      along with deal.II. To this end, go to an empty directory where you
      want to test deal.II and do this:
      <pre>

    $ svn checkout https://svn.dealii.org/trunk .
      </pre>
      (The period at the end puts everything from under <code>trunk/</code>
      into the current directory, rather than creating a
      new <code>trunk/</code> directory.) You will then have
      two folders:
      <pre>

    ./deal.II
    ./tests
      </pre>
    </p>

    <p>
      <b>Note:</b> CMake will pick up any testsuite that is located in a
      <code>tests</code> folder next to the source directory
      (<code>../tests</code>). If your test directory is at a different
      location you have to hint during configuration by specifying
      <code>TEST_DIR</code>:
      <pre>

    $ cmake -DTEST_DIR=&lt;...&gt;
      </pre>
    </p>

    <a name="setupconfigure"></a>
    <h3>Preparing the testsuite</h3>

    <p>
      To enable the testsuite, configure and build deal.II in a build
      directory as normal (installation is not necessary). After that you
      can setup the testsuite via the "setup_tests" target:s
      <pre>

    $ make setup_tests
      </pre>
      This will set up all tests supported by the current configuration.
      The testsuite can now be run in the current <i>build directory</i> as
      described below.
    </p>

    <p>
      Setup can be fine-tuned using the following commands:
      <pre>

    $ make clean_tests - runs the 'clean' target in every testsuite subproject

    $ make prune_tests - removes all testsuite subprojects
      </pre>

    <p>
      In addition, when setting up the testsuite, the following environment
      variables can be used to override default behavior when
      calling <code>make setup_tests</code>:
      <pre>

    TEST_PICKUP_REGEX
      - A regular expression to select only a subset of tests during setup.
        An empty string is interpreted as a catchall (this is the default).

    TEST_DIFF
      - The diff tool and command line to use for comparison. If numdiff is
        available it defaults to "numdiff -a 1e-6 -q", otherwise plain diff
        is used.

    TEST_TIME_LIMIT
      - The time limit (in seconds) a single test is allowed to take. Defaults
        to 180 seconds
      </pre>
    </p>

    <p>
      <b>Note:</b> Specifying these options via environment variables is
      volatile, i.e. if <code>make setup_tests</code> is invoked a second
      time without the variables set in environment, the option will be
      reset to the default value. If you want to set these options
      permanently, set them via cmake as CMake variable in the build
      directory:
      <pre>

    $ cmake -DTEST_PICKUP_REGEX="&lt;regular expression&gt;" .
      </pre>
      A variable set via cmake always <i>overrides</i> one
      set via environment.
    </p>

    <a name="run"></a>
    <h2>Running the testsuite</h2>

    <p>
      The testsuite can now be run in the <i>build directory</i> via
      <pre>

    $ ctest [-j N]
      </pre>
      Here, <code>N</code> is the number of concurrent tests that should be
      run, in the same way as you can say <code>make -jN</code>. The testsuite
      is huge and will need around 12h on current computers 
      running single threaded.
    </p>

    <p>
      If you only want to run a subset of tests
      matching a regular expression, or if you want to exclude tests matching
      a regular expression, you can use
      <pre>

    $ ctest [-j N] -R '&lt;positive regular expression&gt;'
    $ ctest [-j N] -E '&lt;negative regular expression&gt;'
      </pre>
    </p>

    <p>
      <b>Note:</b>
      Not all tests succeed on every machine even if all computations are
      correct, because your machine generates slightly different floating
      point outputs. To increase the number of tests that work correctly,
      install the
      <a href="http://www.nongnu.org/numdiff/">numdiff</a> tool that compares
      stored and newly created output files based on floating point
      tolerances. To use it, simply export where the <code>numdiff</code>
      executable can be found via the <code>PATH</code> 
      environment variable so that it can be found during
      <code>make setup_tests</code>.
    </p>

    <a name="runoutput"></a>
    <h3>How to interpret the output</h3>

    <p>
      A typical output of a <code>ctest</code> invocation looks like:
      <pre>

    $ ctest -j4 -R "base/thread_validity"
    Test project /tmp/trunk/build
          Start 747: base/thread_validity_01.debug
          Start 748: base/thread_validity_01.release
          Start 775: base/thread_validity_05.debug
          Start 776: base/thread_validity_05.release
     1/24 Test #776: base/thread_validity_05.release ...   Passed    1.89 sec
     2/24 Test #748: base/thread_validity_01.release ...   Passed    1.89 sec
          Start 839: base/thread_validity_03.debug
          Start 840: base/thread_validity_03.release
     3/24 Test #747: base/thread_validity_01.debug .....   Passed    2.68 sec
    [...]
          Start 1077: base/thread_validity_08.debug
          Start 1078: base/thread_validity_08.release
    16/24 Test #1078: base/thread_validity_08.release ...***Failed    2.86 sec
    18/24 Test #1077: base/thread_validity_08.debug .....***Failed    3.97 sec
    [...]

    92% tests passed, 2 tests failed out of 24

    Total Test time (real) =  20.43 sec

    The following tests FAILED:
            1077 - base/thread_validity_08.debug (Failed)
            1078 - base/thread_validity_08.release (Failed)
    Errors while running CTest
      </pre>
      If a test failed (like <code>base/thread_validity_08.debug</code> in above
      example output), you might want to find out what exactly went wrong. To
      this end, you can search
      through <code>Testing/Temporary/LastTest.log</code> for the exact output
      of the test, or you can rerun this one test, specifying <code>-V</code>
      to select verbose output of tests:
      <pre>

    $ ctest -V -R "base/thread_validity_08.debug"
    [...]
    test 1077
        Start 1077: base/thread_validity_08.debug

    1077: Test command: [...]
    1077: Test timeout computed to be: 600
    1077: Test base/thread_validity_08.debug: RUN
    1077: ===============================   OUTPUT BEGIN  ===============================
    1077: Built target thread_validity_08.debug
    1077: Generating thread_validity_08.debug/output
    1077: terminate called without an active exception
    1077: /bin/sh: line 1: 18030 Aborted [...]/thread_validity_08.debug
    1077: base/thread_validity_08.debug: BUILD successful.
    1077: base/thread_validity_08.debug: RUN failed. Output:
    1077: DEAL::OK.
    1077: gmake[3]: *** [thread_validity_08.debug/output] Error 1
    1077: gmake[2]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/all] Error 2
    1077: gmake[1]: *** [CMakeFiles/thread_validity_08.debug.diff.dir/rule] Error 2
    1077: gmake: *** [thread_validity_08.debug.diff] Error 2
    1077:
    1077:
    1077: base/thread_validity_08.debug: ******    RUN failed    *******
    1077:
    1077: ===============================    OUTPUT END   ===============================
      </pre>
      So this specific test aborted in the <code>RUN</code> stage.
    </p>

    <p>
      The general output for a successful test <code>&lt;test&gt;</code> in
      category <code>&lt;category&gt;</code> for build type
      <code>&lt;build&gt;</code> is
      <pre>

    xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED
    xx: ===============================   OUTPUT BEGIN  ===============================
    xx: [...]
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: PASSED.
    xx: ===============================    OUTPUT END   ===============================
      </pre>
      And for a test that fails in stage <code>&lt;stage&gt;</code>:
      <pre>

    xx: Test &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt;
    xx: ===============================   OUTPUT BEGIN  ===============================
    xx: [...]
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: &lt;stage&gt; failed. [...]
    xx:
    xx: &lt;category&gt;/&lt;test&gt;.&lt;build&gt;: ******    &lt;stage&gt; failed    *******
    xx: ===============================    OUTPUT END   ===============================
      </pre>
      Hereby, <code>&lt;stage&gt;</code> indicates the stage in which the
      test failed:
      <ul>
        <li>
          <code>CONFIGURE</code>: only for test in the "build_tests"
          category: The test project failed in the configuration stage
        </li>
        <li>
          <code>BUILD</code>: compilation error occured
        </li>
        <li>
          <code>RUN</code>: the test executable could not be run / aborted
        </li>
        <li>
          <code>DIFF</code>: the test output differs from the reference output
        </li>
        <li>
          <code>PASSED</code>: the test run successful
        </li>
      </ul>
      Typically, tests fail because the output has changed, and you will see
      this in the DIFF phase of the test.
    </p>


    <a name="layout"></a>
    <h2>Testsuite development</h2>

    <p>
      The following outlines what you need to know if you want to understand
      how the testsuite actually works, for example because you may want to
      add tests along with the functionality you are currently developing.
    </p>



    <a name="layoutgeneral"></a>
    <h3>General layout</h3>

    <p>
      A test usually consists of a source file and an output file for
      comparison (under the testsuite directory <code>tests</code>):
      <pre>

    category/test.cc
    category/test.output
      </pre>
      <code>category</code> will be one of the existing subdirectory
      under <code>tests/</code>, e.g., <code>lac/</code>, <code>base/</code>,
      or <code>mpi/</code>. Historically, we have grouped tests into the
      directories <code>base/, lac/, deal.II/</code> depending on their
      functionality, and <code>bits/</code> if they were small unit tests, but
      in practice we have not always followed this rigidly. There are also
      more specialized directories <code>trilinos/, petsc/,
      serialization/, mpi/</code> etc, whose meaning is more obvious.
      <code>test.cc</code> must be a regular executable (i.e. having an
      <code>int main()</code> routine). It will be compiled, linked and
      run. The executable should not output anything to <code>cout</code>
      (at least under normal circumstances, i.e. no error condition),
      instead the executable should output to a file <code>output</code>
      in the current working directory. In practice, we rarely write the
      source files completely from scratch, but we find an existing test that
      already does something similar and copy/modify it to fit our needs.
    </p>
    <p>
      For a normal test, <code>ctest</code> will typically run the following 3
      stages:
      <ul>
        <li>
          <code>BUILD</code>: The build stage generates an executable in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;</code>.
        </li>
        <li>
          <code>RUN</code>: The run stage then invokes the executable in the
          directory where it is located. By convention, each test puts its
          output into a file simply called <code>output</code>, which will
          then be located in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/output</code>.
          If the run fails (e.g. because the program aborts with an error
          code) the file <code>output</code> is renamed to
          <code>failing_output</code>.
        </li>
        <li>
          <code>DIFF</code>: As a last stage the generated output file will
          be compared to
          <code>SOURCE_DIR/tests/&lt;category&gt;/&lt;test&gt;[...].output</code>.
          and stored in
          <code>BUILD_DIR/tests/&lt;category&gt;/&lt;test&gt;/diff</code>.
          If the diff fails  the file <code>diff</code> is renamed to
          <code>failing_diff</code>.
        </li>
      </ul>
    </p>


    <a name="restrictbuild"></a>
    <h3>Restricting tests for build configurations</h3>

    <p>
      Comparison file can actually be named in a more complex way than
      just <code>category/test.output</code>:
      <pre>

    category/test.[with_&lt;feature&gt;=&lt;on|off&gt;.]*[mpirun=&lt;x&gt;.][&lt;debug|release&gt;.]output
      </pre>
      Normally, a test will be set up so that it runs twice, once in debug and
      once in release configuration.
      If a specific test can only be run in debug or release configurations but
      not in both it is possible to restrict the setup by prepeding
      <code>.debug</code> or <code>.release</code> directly before
      <code>.output</code>, e.g.:
      <pre>

    category/test.debug.output
      </pre>
      This way, the test will only be set up to build and run against the debug
      library. If a test should run in both configurations but, for some
      reason, produces different output (e.g., because it triggers an
      assertion in debug mode), then you can just provide two different output
      files:
      <pre>

    category/test.debug.output
    category/test.release.output
      </pre>
    </p>


    <a name="restrictfeature"></a>
    <h3>Restricting tests for feature configurations</h3>
    <p>
      In a similar vain as for build configurations, it is possible to restrict
      tests to specific feature configurations, e.g.:
      <pre>

    category/test.with_umfpack=on.output, or
    category/test.with_zlib=off.output
      </pre>
      These tests will only be set up if the specified feature was configured.
      It is possible to provide different output files for disabled/enabled
      features, e.g.
      <pre>

    category/test.with_64bit_indices=on.output
    category/test.with_64bit_indices=off.output
      </pre>
      It is also possible to declare multiple constraints subsequently, e.g.
      <pre>

    category/test.with_umfpack=on.with_zlib=on.output
      </pre>
    </p>
    <p>
      <b>Note:</b> The tests in some subdirectories of <code>tests/</code> are
      automatically run only if some feature is enabled. In this case a
      feature constraint encoded in the output file name is 
      redundant and should be avoided. In particular, this holds for
      subdirectories 
      <code>distributed_grids</code>, <code>lapack</code>,
      <code>metis</code>, <code>petsc</code>, <code>slepc</code>,
      <code>trilinos</code>, <code>umfpack</code>, <code>gla</code>, and
      <code>mpi</code>
    </p>


    <a name="mpi"></a>
    <h3>Running tests with MPI</h3>
    <p>
      If a test should be run with MPI in parallel, the number of MPI
      processes <code>N</code> with which a program needs to be run for
      comparison with a given output file is specified as follows:
      <pre>

    category/test.mpirun=N.output
      </pre>
      It is quite typical for an MPI-enabled test to have multiple output
      files for different numbers of MPI processes.
    </p>


    <a name="layoutaddtests"></a>
    <h3>Adding new tests</h3>

    <p>
      We typically add one or more new tests every
      time we add new functionality to the library or fix a bug. If you
      want to contribute code to the library, you should do this
      as well. Here's how: you need a testcase and a file with the
      expected output.
    </p>

    <h4>The testcase</h4>
    <p>
      For the testcase, we usually start from one of the existing tests, copy
      and modify it to where it does what we'd like to test. Alternatively,
      you can also start from a template like this:
    <pre>
// ---------------------------------------------------------------------
// $Id$
//
// Copyright (C) 2013 by the deal.II authors
//
// This file is part of the deal.II library.
//
// The deal.II library is free software; you can use it, redistribute
// it, and/or modify it under the terms of the GNU Lesser General
// Public License as published by the Free Software Foundation; either
// version 2.1 of the License, or (at your option) any later version.
// The full text of the license can be found in the file LICENSE at
// the top level of the deal.II distribution.
//
// ---------------------------------------------------------------------


// a short (a few lines) description of what the program does

#include "../tests.h"
#include &lt;iostream&gt;
#include &lt;fstream&gt;

// all include files you need here


int main ()
{
  std::ofstream logfile("output");
  deallog.attach(logfile);
  deallog.depth_console(0);

  // your testcode here:
  int i=0;
  deallog << i << std::endl;

  return 0;
}
    </pre>

    <p>This code opens an output file <code>output</code> in the current working
    directory and then writes all output you generate to it, through the
    <code>deallog</code> stream. The <code>deallog</code> stream works like
    any other <code>std::ostream</code> except that it does a few more
    things behind the scenes that are helpful in this context. In above
    case, we only write a zero to the output file. Most tests of course
    write computed data to the output file to make sure that whatever we
    compute is what we got when the test was first written.
    </p>

    <p>
    There are a number of directories where you can put a new test.
    Extensive tests of individual classes or groups of classes
    have traditionally been into the <code>base/</code>,
    <code>lac/</code>, <code>deal.II/</code>, <code>fe/</code>,
    <code>hp/</code>, or <code>multigrid/</code> directories, depending on
    where the classes that are tested are located. More atomic tests often go
    into <code>bits/</code>. There are also
    directories for PETSc and Trilinos wrapper functionality.
    </p>

    <h4>An expected output</h4>

    <p>
      In order to run your new test, copy it to an appropriate category and
      create an empty comparison file for it:
      <pre>

    category/my_new_test.cc
    category/my_new_test.output
      </pre>
      Now, rerun
      <pre>

    $ make setup_tests
      </pre>
      so that your new test is picked up. After that it is possible to
      invoke it with
      <pre>

    $ ctest -V -R "category/my_new_test"
      </pre>
    </p>

    <p>
      If you run your new test executable this way, the test should compile
      and run successfully but fail in the diff stage (because of the empty
      comparison file). You will get an output file
      <code>BUILD_DIR/category/my_new_test/output</code>. Take a look at it to
      make sure that the output is what you had expected. (For complex tests,
      it may sometimes be impossible to say whether the output is correct, and
      in this case we sometimes just take it to make
      sure that future invokations of the test yield the same results.)
    </p>

    <p>
      The next step is to copy and rename this output file to the source
      directory and replace the original comparison file with it:
      <pre>

    category/my_new_test.output
      </pre>
      At this point running the test again should be successful:
      <pre>

    $ ctest -V -R "category/my_new_test"
      </pre>
    </p>


    <h4>Checking in</h4>

    <p>
      Tests are a way to make sure everything keeps working. If they
      aren't automated, they are no good. We are therefore very
      interested in getting new tests. If you have subversion write access
      already, you can add the new test and the expected output
      file:
      <pre>

    svn add category/my_new_test.cc
    svn add category/my_new_test.output
    svn commit -m "New test"
      </pre>
      If you don't have subversion write access, talk to us in the
      discussion group; writing testcases is a worthy and laudable task,
      and we would like to encourage it by giving people the opportunity to
      contribute!
    </p>



    <a name="submit"></a>
    <h2>Submitting test results</h2>

    <p class="todo">
      Explain how to use <code>run_testsuite.cmake</code> in all imaginable
      ways...
    </p>



    <a name="build_tests"></a>
    <h2>Build tests</h2>

    <p>
      Build tests are used to check that deal.II can be compiled on
      different systems and with different compilers as well as
      different configuration options. Results are collected in a
      database and can be accessed <a
      href="http://www.dealii.org/testsuite.html">online</a>.<p>

      <p>Running the build test suite is simple and we encourage deal.II
      users with configurations not found on the <a
      href="http://www.dealii.org/testsuite.html">test suite page</a> to
      participate. Assuming you checked out deal.II into the directory
      <code>dealtest</code>, running it is as simple as:
      <pre>

    cd dealtest
    svn update
    ./contrib/utilities/build_test
    mail build-tests@dealii.org &lt; *.log
  ( rm *.log )
      </pre>
    </p>

    <p>
      What this does is to create a temporary directory, compile and build
      deal.II in it, and for good measure also build the tutorial
      programs. The fourth of the commands above then sends the resulting
      status files to a daemon that presents this information on the website
      linked to above.
    </p>

    <p>
      The <code>build_test</code> script supports the following options:
      <pre>

    SOURCEDIR     - the source directory to use (otherwise the current directory is used)
    CONFIGFILE    - A cmake configuration file for the build test
    LOGDIR        - directory for the log file
    LOGFILE       - the logfile to use, defaults to
                        $LOGDIR/$BRANCH.$CONFIGFILE.<unix time>.log

    CMAKE         - the cmake executable to use
    SVN           - svn info command to use, defaults to
                        svn info $(SOURCEDIR)
    TMPDIR        - defaults to "/tmp"
    CLEAN_TMPDIR  - defaults to "true"
    RUN_EXAMPLES  - defaults to "true"
      </pre>
      An example configuration file can be found <a
      href="../users/Config.sample">here</a>. Options can be passed either via
    environment variables
      <pre>

    export CONFIGFILE=MyConfiguration.conf
    ./contrib/utilities/build_test
      </pre>
      or directly on the command line:
      <pre>

    ./contrib/utilities/build_test CONFIGFILE=myConfiguration.conf
      </pre>
    </p>

    <p>
      A status indicator should appear on the build test website after some
      time (results are collected and processed by a program that is run
      periodically, but not immediately after a mail has been received).
    </p>


    <a name="dedicatedbuilds"></a>
    <h3>Dedicated build tests</h3>

    <p>
      Build tests work best if they run automatically and periodically.
      There is a detailed example for such dedicated build tests on the <a
        href="https://code.google.com/p/dealii/wiki/BuildTests">wiki</a>.
    </p>


    <hr />
    <address>
      <a href="../authors.html" target="body">The deal.II Authors</a>
      $Date$
    </address>
    <div class="right">
      <a href="http://validator.w3.org/check?uri=referer" target="_top">
        <img style="border:0" src="http://www.w3.org/Icons/valid-html401" alt="Valid HTML 4.01!"></a>
      <a href="http://jigsaw.w3.org/css-validator/check/referer" target="_top">
        <img style="border:0;width:88px;height:31px" src="http://jigsaw.w3.org/css-validator/images/vcss" alt="Valid CSS!"></a>
    </div>

  </body>
</html>
