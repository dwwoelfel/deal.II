<a name="Results"></a>
<h1>Results</h1>

<h3>Output of the program and graphical visualization</h3>

<h4>2D calculations</h4>

Running the program with the space dimension set to 2 in
<code>main()</code> yields the following output:
@code
examples/step-31> make run
============================ Remaking Makefile.dep
==============optimized===== step-31.cc
============================ Linking step-31
============================ Running step-31
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure
@endcode

The entire computation above takes about 30 seconds on a reasonably
quick (for 2007 standards) machine.

What we see immediately from this is that the number of (outer)
iterations does not increase as we refine the mesh. This confirms the
statement in the introduction that preconditioning the Schur
complement with the mass matrix indeed yields a matrix spectrally
equivalent to the identity matrix (i.e. with eigenvalues bounded above
and below independently of the mesh size or the relative sizes of
cells). In other words, the mass matrix and the Schur complement are
spectrally equivalent.

In the images below, we show the grids for the first six refinement
steps in the program.  Observe how the grid is refined in regions
where the solution rapidly changes: On the upper boundary, we have
Dirichlet boundary conditions that are -1 in the left half of the line
and 1 in the right one, so there is an aprupt change at $x=0$. Likewise,
there are changes from Dirichlet to Neumann data in the two upper
corners, so there is need for refinement there as well:

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-22.2d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-22.2d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-22.2d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-22.2d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-22.2d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-22.2d.mesh-5.png
    </td>
  </tr>
</table>

Finally, following is a plot of the flow field. It shows fluid
transported along with the moving upper boundary and being replaced by
material coming from below:

@image html step-22.2d.solution.png

This plot uses the capability of VTK-based visualization programs (in
this case of VisIt) to show vector data; this is the result of us
declaring the velocity components of the finite element in use to be a
set of vector components, rather than independent scalar components in
the <code>StokesProblem@<dim@>::output_results</code> function of this
tutorial program.



<h4>3D calculations</h4>

In 3d, the screen output of the program looks like this:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81)
   Assembling...
   Computing preconditioner...
   Solving...  13 outer CG Schur complement iterations for pressure.

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 6
   Number of active cells: 170720
   Number of degrees of freedom: 4625150 (4432407+192743)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.
@endcode

Again, we see that the number of outer iterations does not increase as
we refine the mesh. Nevertheless, the compute time increases
significantly: for each of the iterations above separately, it takes a
few seconds, a few seconds, 1min, 5min, 29min, 3h12min, and 21h39min
for the finest level with more than 4.5 million unknowns. This
superlinear (in the number of unknowns) increase is due to first the
superlinear number of operations to compute the ILU decomposition, and
secondly the fact
that our inner solver is not ${\cal O}(N)$: a simple experiment shows
that as we keep refining the mesh, the average number of
ILU-preconditioned CG iterations to invert the velocity-velocity block
$A$ increases.

We will address the question of how possibly to improve our solver <a
href="#improved-solver">below</a>.

As for the graphical output, the grids generated during the solution
look as follow: 

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-22.3d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-22.3d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-22.3d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-22.3d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-22.3d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-22.3d.mesh-5.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-22.3d.mesh-6.png
    </td>

    <td ALIGN="center">
    </td>
  </tr>
</table>

Again, they show essentially the location of singularities introduced
by boundary conditions. The vector field computed makes for an
interesting graph:

@image html step-22.3d.solution.png

The isocountours shown here as well are those of the pressure
variable, showing the singularity at the point of discontinuous
velocity boundary conditions.



<h3>Sparsity pattern</h3>

As explained during the generation of the sparsity pattern, it is
important to have the numbering of degrees of freedom in mind when
using preconditioners like incomplete LU decompositions. This is most
conveniently visualized using the distribution of nonzero elements in
the stiffness matrix.

If we don't do anything special to renumber degrees of freedom (i.e.,
without using DoFRenumbering::Cuthill_McKee, but with using
DoFRenumbering::component_wise to ensure that degrees of freedom are
appropriately sorted into their corresponding blocks of the matrix and
vector, then we get the following image after the first adaptive
refinement in two dimensions:

@image html step-22.2d.sparsity-nor.png

In order to generate such a graph, you have to insert a piece of
code like the following to the end of the setup step. 
@code
  {
    std::ofstream out ("sparsity_pattern.gpl");
    sparsity_pattern.print_gnuplot(out);
  }
@endcode

It is clearly visible that the nonzero entries are spread over almost the
whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a
Gaussian elimination (LU decomposition) without fill-in elements, which means
that more tentative fill-ins left out will result in a worse approximation of
the complete decomposition.

In this program, we have thus chosen a more advanced renumbering of
components.  The renumbering with DoFRenumbering::Cuthill_McKee and grouping
the components into velocity and pressure yields the following output:

@image html step-22.2d.sparsity-ren.png

It is apparent that the situation has improved a lot. Most of the elements are
now concentrated around the diagonal in the (0,0) block in the matrix. Similar
effects are also visible for the other blocks. In this case, the ILU
decomposition will be much closer to the full LU decomposition, which improves
the quality of the preconditioner. (It may be interesting to note that the
sparse direct solver UMFPACK does some internal renumbering of the equations
before actually generating a sparse LU decomposition; that procedure leads to
a very similar pattern to the one we got from the Cuthill-McKee algorithm.)

Finally, we want to have a closer
look at a sparsity pattern in 3D. We show only the (0,0) block of the
matrix, again after one adaptive refinement. Apart from the fact that the matrix
size has increased, it is also visible that there are many more entries
in the matrix. Moreover, even for the optimized renumbering, there will be a
considerable amount of tentative fill-in elements. This illustrates why UMFPACK 
is not a good choice in 3D - a full decomposition needs many new entries that
 eventually won't fit into the physical memory (RAM):

@image html step-22.3d.sparsity_uu-ren.png



<h2>Possible Extensions</h2>

<a name="improved-solver">
<h3>Improved linear solver</h3>
</a>

We have seen in the section of computational results that the number of outer
iterations does not depend on the mesh size, which is optimal in a sense of
scalability. This does, however, not apply to the solver as a whole:
we did not look at the number of inner iterations when generating the inverse of
the matrix $A$ and the mass matrix $M_p$. Of course, this is unproblematic in
the 2D case where we precondition $A$ with a direct solver and the
<code>vmult</code> operation of the inverse matrix structure will converge in
one single CG step, but this changes in 3D where we need to apply the ILU
preconditioner.  There, the number of required preconditioned CG steps to
invert $A$ increases as the mesh is refined. For 
the 3D results obtained above, each <code>vmult</code> operation involves
on average approximately 14, 23, 36, 59, 72, 101, ... inner CG iterations in
the refinement steps shown above. (On the other hand,
the number of iterations for applying the inverse pressure mass matrix is
always about 5-6, both in two and three dimensions.)  To summarize, most work 
is spent on solving linear systems with the same matrix $A$ over and over again.
What makes this appear even worse is the fact that we 
actually invert a matrix that is about 95 precent the size of the total system
matrix and stands for 85 precent of the non-zero entries in the sparsity
pattern. Hence, the natural question is whether it is reasonable to solve a
system with matrix $A$ about 15 times when calculating the solution to the
block system.
  
The answer is, of course, that we can do that in a few other (most of the time
better) ways.
Nevertheless, it has to be remarked that an indefinite system as the one
resulting from the discretization of the Stokes problem puts indeed much higher
demands on the linear algebra than standard elliptic problems as we have seen
in the early tutorial programs. The improvements are still rather 
unsatisfactory, if one compares with an elliptic problem of similar size.

<h4>Better preconditioner for the inner CG solver</h4>
The first way to improve the situation would be to choose a preconditioner that 
makes CG for the (0,0) matrix $A$ converge in a mesh-independent number of 
iterations, say 10 to 30. We have seen such a canditate in 
@ref step_16 "step-16": multigrid.

<h4>Block Schur complement preconditioner</h4>
But even with a good preconditioner for $A$ at hand, there would still 
be need for the repeated solution of the same linear system (with different 
right hand sides, though). The approach we are going to discuss here is how this
can be avoided. If we persist in calculating the Schur complement, there is no 
other possibility.

The alternative is to attack the block system at one time and use an approximate
Schur complement as an efficient preconditioner. The basic idea is as
follows: If we find a block preconditioner $P$ such that the matrix
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is simple, then an iterative solver with that preconditioner will converge in a
few iterations. Using the Schur complement $S = B A^{-1} B^T$, one finds that
@f{eqnarray*}
  P^{-1}
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
@f}
would appear to be a good choice since
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    I & A^{-1} B^T \\ 0 & 0
  \end{array}\right).
@f}
This is the approach taken by the paper by Silvester and Wathen referenced to
in the introduction. In this case, a Krylov-based iterative method would 
converge in two steps if exact inverses of $A$ and $S$ were applied, since
there are only two distinct eigenvalues 0 and 1 of the matrix. We shall discuss
below which solver is adequate for this problem.

Since $P$ is aimed to be a preconditioner only, we shall only use
approximations to the inverse of the Schur complement $S$ and the matrix $A$.

Hence, an improved solver for the Stokes system is going to look like the 
following: The Schur
complement will be approximated by the pressure mass matrix $M_p$, and we use
a preconditioner to $A$ (without an InverseMatrix class around it) to
approximate $A^{-1}$. This two-component system builds a preconditioner for
the block system. Here comes the class that implements the block Schur
complement preconditioner. The <code>vmult</code> operation for block vectors
according to the derivation above can be specified by three successive
operations:
@code
template <class PreconditionerA, class PreconditionerMp>
class BlockSchurPreconditioner : public Subscriptor
{
  public:
    BlockSchurPreconditioner (const BlockSparseMatrix<double>         &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp>  &Mpinv,
          const PreconditionerA &Apreconditioner);

  void vmult (BlockVector<double>       &dst,
              const BlockVector<double> &src) const;

  private:
    const SmartPointer<const BlockSparseMatrix<double> > system_matrix;
    const SmartPointer<const InverseMatrix<SparseMatrix<double>, 
                       PreconditionerMp > > m_inverse;
    const PreconditionerA &a_preconditioner;
    
    mutable Vector<double> tmp;

};

template <class PreconditionerA, class PreconditionerMp>
BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::BlockSchurPreconditioner(
          const BlockSparseMatrix<double>                            &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp> &Mpinv,
          const PreconditionerA &Apreconditioner
          )
                :
                system_matrix           (&S),
                m_inverse               (&Mpinv),
                a_preconditioner        (Apreconditioner),
                tmp                     (S.block(1,1).m())
{}

        // Now the interesting function, the multiplication of
        // the preconditioner with a BlockVector. 
template <class PreconditionerA, class PreconditionerMp>
void BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::vmult (
                                     BlockVector<double>       &dst,
                                     const BlockVector<double> &src) const
{
        // Form u_new = A^{-1} u
  a_preconditioner.vmult (dst.block(0), src.block(0));
        // Form tmp = - B u_new + p 
        // (<code>SparseMatrix::residual</code>
        // does precisely this)
  system_matrix->block(1,0).residual(tmp, dst.block(0), src.block(1));
        // Change sign in tmp
  tmp *= -1;
        // Multiply by approximate Schur complement 
        // (i.e. a pressure mass matrix)
  m_inverse->vmult (dst.block(1), tmp);
}
@endcode

Since we act on the whole block system now, we also have to live with one
disadvantage, though: we need to perform the solver iterations on
the full block system instead of the smaller pressure space.

Now we turn to the question which solver we should use for the block
system. The first observation is that the resulting preconditioned matrix cannot
be solved with CG since it is neither positive definite nor symmetric.

The deal.II libraries implement several solvers that are appropriate for the
problem at hand. One choice is the solver @ref SolverBicgstab "BiCGStab", which
was used for the solution of the unsymmetric advection problem in step-9. The
second option, the one we are going to choose, is @ref SolverGMRES "GMRES
(generalized minimum residual)". Both methods have their advantages - there
are problems where one of the two candidates clearly outperforms the other, and 
vice versa.
<a href="http://en.wikipedia.org/wiki/GMRES#Comparison_with_other_solvers">Wikipedia</a>'s 
article on the GMRES method gives a comparative presentation.
A more comprehensive and well-founded comparsion can be read e.g. in the book by
J.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6).

For our specific problem with the ILU preconditioner for $A$, we certainly need
to perform hundreds of iterations on the block system for large problem sizes
(we won't beat CG!). Actually, this disfavors GMRES: During the GMRES
iterations, a basis of Krylov vectors is successively build up and some
operations are performed on these vectors. The more vectors are in this basis, 
the more operations and memory will be needed. To not let these demands grow
excessively, deal.II limits the size of the basis to 30 vectors by default.
Then, the basis is rebuilt. This implementation of the GMRES method is called
GMRES(k), where $k$ is 30 in our case. What we have gained by this restriction, 
namely bounded operations and memory requirements, will be compensated by
the fact that we use an incomplete basis - this will increase the number of
required iterations.

BiCGStab, on the other hand, won't get slower when many iterations are needed
(one iteration uses only results from one preceeding step and
not all the steps as GMRES). Besides the fact the BiCGStab is more expensive per
step since two matrix-vector products have to be performed (compared to one for
CG or GMRES), there is one main reason which makes BiCGStab not appropriate for
this problem: The preconditioner applies the inverse of the pressure
mass matrix by using the InverseMatrix class. Since the application of the
inverse matrix to a vector is done only in approximative way (an exact inverse
is too expensive), this will also affect the solver. In the case of BiCGStab, 
the Krylov vectors will not be orthogonal due to this perturbation. While
this is uncritical for a small number of steps (up to about 50), it ruins the
performance of the solver when these perturbations have grown to a significant
magnitude in the coarse of iterations.

Some experiments with BiCGStab have been performed and it was found to
be faster than GMRES up to refinement cycle 3 (in 3D), but became very slow 
for cycles 5 and 6 (even slower than the original Schur complement), so the
solver is useless in this situation. Choosing a sharper tolerance for the
inverse matrix class (<code>1e-10*src.l2_norm()</code> instead of 
<code>1e-6*src.l2_norm()</code>) made BiCGStab perform well also for cycle 4, 
but did not change the failure on the very large systems.

GMRES is of course also effected by the approximate inverses, but it is not as
sensitive to orthogonality and retains a relatively good performance also for
large sizes, see the results below.

With this said, we turn to the realization of the solver call with GMRES:

@code
      SparseMatrix<double> pressure_mass_matrix;
      pressure_mass_matrix.reinit(sparsity_pattern.block(1,1));
      pressure_mass_matrix.copy_from(system_matrix.block(1,1));
      system_matrix.block(1,1) = 0;

      SparseILU<double> pmass_preconditioner;
      pmass_preconditioner.initialize (pressure_mass_matrix, 
        SparseILU<double>::AdditionalData());
      
      InverseMatrix<SparseMatrix<double>,SparseILU<double> >
        m_inverse (pressure_mass_matrix, pmass_preconditioner);

      BlockSchurPreconditioner<typename InnerPreconditioner<dim>::type,
                               SparseILU<double> > 
        preconditioner (system_matrix, m_inverse, *A_preconditioner);
      
      SolverControl solver_control (system_matrix.m(),
                                    1e-6*system_rhs.l2_norm());
      
      SolverGMRES<BlockVector<double> > gmres(solver_control);
      
      gmres.solve(system_matrix, solution, system_rhs,
                  preconditioner);
      
      hanging_node_constraints.distribute (solution);
      
      std::cout << " "
                << solver_control.last_step()
                << " block GMRES iterations";
@endcode

Obviously, one needs to add the include file @ref SolverGMRES 
"<lac/solver_gmres.h>" in order to make this run.
We call the solver with a BlockVector template in order to enable
GMRES to operate on block vectors and matrices.
Note also that we need to set the (1,1) block in the system
matrix to zero (we saved the pressure mass matrix there which is not part of the
problem) after we copied the information to another matrix.

Using the Timer class, we collect some statistics that compare the runtime
of the block solver with the one from the problem implementation above (on
a different machine than the one for which timings were reported
above). Besides the solution if the two systems we also check if the solutions
of the two variants are close to each other (i.e. this solver gives indeed the
same solution as before) and calculate the infinity
norm of the vector difference.

Let's first see the results in 2D:
@code
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85) [0.007999 s] 
   Assembling... [0.020997 s]
   Computing preconditioner... [0.004999 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [0.010998 s]
      Block Schur preconditioner:  11 GMRES iterations [0.009999 s]
   difference l_infty between solution vectors: 3.18714e-06

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201) [0.024996 s] 
   Assembling... [0.051992 s]
   Computing preconditioner... [0.018997 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [0.034995 s]
      Block Schur preconditioner:  12 GMRES iterations [0.035994 s]
   difference l_infty between solution vectors: 9.32671e-06

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443) [0.06399 s] 
   Assembling... [0.123981 s]
   Computing preconditioner... [0.053992 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [0.109983 s]
      Block Schur preconditioner:  12 GMRES iterations [0.110983 s]
   difference l_infty between solution vectors: 4.26989e-06

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001) [0.150977 s] 
   Assembling... [0.287956 s]
   Computing preconditioner... [0.137979 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [0.292956 s]
      Block Schur preconditioner:  12 GMRES iterations [0.304953 s]
   difference l_infty between solution vectors: 1.0266e-05

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197) [0.345948 s] 
   Assembling... [0.6519 s]
   Computing preconditioner... [0.414937 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [0.758885 s]
      Block Schur preconditioner:  13 GMRES iterations [0.844872 s]
   difference l_infty between solution vectors: 3.13139e-05

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605) [0.751885 s] 
   Assembling... [1.39779 s]
   Computing preconditioner... [1.24681 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [1.65375 s]
      Block Schur preconditioner:  13 GMRES iterations [1.89871 s]
   difference l_infty between solution vectors: 8.59663e-05

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411) [1.51877 s] 
   Assembling... [2.89056 s]
   Computing preconditioner... [3.99639 s]
   Solving...
      Schur complement:  11 outer CG iterations for p  [3.73143 s]
      Block Schur preconditioner:  13 GMRES iterations [4.23036 s]
   difference l_infty between solution vectors: 0.00022514
@endcode

We see that there is no huge difference in the solution time between
the block Schur complement preconditioner solver and 
the Schur complement itself. The
reason is simple: we used a direct solve as preconditioner for the latter - so
there won't be any substantial gain by avoiding the inner iterations. We see
that the number of iterations has slightly increased for GMRES, but all in all
the two choices are fairly similar and result in similar solution times as well.

The picture of course changes in 3D:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81) [0.099985 s]
   Assembling... [0.59191 s]
   Computing preconditioner... [0.056991 s]
   Solving...
      Schur complement:  13 outer CG iterations for p  [0.355946 s]
      Block Schur preconditioner:  23 GMRES iterations [0.054992 s]
   difference l_infty between solution vectors: 1.11101e-05

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261) [0.699894 s]
   Assembling... [2.71159 s]
   Computing preconditioner... [0.311953 s]
   Solving...
      Schur complement:  14 outer CG iterations for p  [2.90556 s]
      Block Schur preconditioner:  48 GMRES iterations [0.52692 s]
   difference l_infty between solution vectors: 2.44514e-05

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055) [3.9674 s]
   Assembling... [12.5721 s]
   Computing preconditioner... [1.67874 s]
   Solving...
      Schur complement:  14 outer CG iterations for p  [23.0945 s]
      Block Schur preconditioner:  93 GMRES iterations [5.08123 s]
   difference l_infty between solution vectors: 4.03209e-05

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133) [18.8971 s]
   Assembling... [56.3194 s]
   Computing preconditioner... [7.43587 s]
   Solving...
      Schur complement:  15 outer CG iterations for p  [248.74 s]
      Block Schur preconditioner: 196 GMRES iterations [64.4772 s]
   difference l_infty between solution vectors: 7.33337e-05

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149) [141.082 s]
   Assembling... [208.273 s]
   Computing preconditioner... [27.8068 s]
   Solving...
      Schur complement:  15 outer CG iterations for p  [1312 s]
      Block Schur preconditioner: 435 GMRES iterations [550.743 s]
   difference l_infty between solution vectors: 0.000194438

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093) [2064.04 s]
   Assembling... [811.105 s]
   Computing preconditioner... [111.035 s]
   Solving...
      Schur complement:  14 outer CG iterations for p  [6496.29 s]
      Block Schur preconditioner:  1243 GMRES iterations [6847.67 s]
   difference l_infty between solution vectors: 0.000429411      
@endcode

Here, the block preconditioned solver is clearly superior to the Schur
complement, but the advantage gets less for more mesh points. This was expected 
- see the discussion above. It is still necessary to invert the
mass matrix iteratively, which means more work if we need more (outer)
iterations. It is also apparent that GMRES scales worse with the problem size
than CG (as explained above).
Nonetheless, the improvement by a factor of 3-5 for moderate problem sizes
is quite impressive.

<h4>No block matrices and vectors</h4>
Another possibility that can be taken into account is to not set up a block
system, but rather solve the system of velocity and pressure all at once. The
alternatives are direct solve with UMFPACK (2D) or GMRES with ILU
preconditioning (3D). It should be straightforward to try that.
