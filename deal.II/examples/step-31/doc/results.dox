<a name="Results"></a>
<h1>Results</h1>

<h3>Output of the program and graphical visualization</h3>

<h4>2D calculations</h4>

Running the program with the space dimension set to 2 in
<code>main()</code> yields the following output:
@code
examples/step-31> make run
============================ Remaking Makefile.dep
==============optimized===== step-31.cc
============================ Linking step-31
============================ Running step-31
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure
@endcode
The entire computation above takes about 30 seconds on a reasonably
quick (for 2007 standards) machine.

What we see immediately from this is that the number of (outer)
iterations does not increase as we refine the mesh. This confirms the
statement in the introduction that preconditioning the Schur
complement with the mass matrix indeed yields a matrix spectrally
equivalent to the identity matrix (i.e. with eigenvalues bounded above
and below independently of the mesh size or the relative sizes of
cells). In other words, the mass matrix and the Schur complement are
spectrally equivalent.

In the images below, we show the grids for the first six refinement
steps in the program.  Observe how the grid is refined in regions
where the solution rapidly changes: On the upper boundary, we have
Dirichlet boundary conditions that are -1 in the left half of the line
and 1 in the right one, so there is an aprupt change at $x=0$. Likewise,
there are changes from Dirichlet to Neumann data in the two upper
corners, so there is need for refinement there as well:

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-5.png
    </td>
  </tr>
</table>

Finally, following is a plot of the flow field. It shows fluid
transported along with the moving upper boundary and being replaced by
material coming from below:

@image html step-31.2d.solution.png

This plot uses the capability of VTK-based visualization programs (in
this case of VisIt) to show vector data; this is the result of us
declaring the velocity components of the finite element in use to be a
set of vector components, rather than independent scalar components in
the <code>StokesProblem@<dim@>::output_results</code> function of this
tutorial program.



<h4>3D calculations</h4>

In 3d, the screen output of the program looks like this:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81)
   Assembling...
   Computing preconditioner...
   Solving...  13 outer CG Schur complement iterations for pressure.

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 6
   Number of active cells: 170720
   Number of degrees of freedom: 4625150 (4432407+192743)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.
@endcode

Again, we see that the number of outer iterations does not increase as
we refine the mesh. Nevertheless, the compute time increases
significantly: for each of the iterations above separately, it takes a
few seconds, a few seconds, 1min, 5min, 29min, 3h12min, and 21h39min
for the finest level with more than 4.5 million unknowns. This
superlinear (in the number of unknowns) increase is due to first the
superlinear number of operations to compute the ILU decomposition, and
secondly the fact
that our inner solver is not ${\cal O}(N)$: a simple experiment shows
that as we keep refining the mesh, the average number of
ILU-preconditioned CG iterations to invert the velocity-velocity block
$A$ increases from 12, 22, 35, 55, .....

We will address the question of how possibly to improve our solver <a
href="improved-solver">below</a>.

As for the graphical output, the grids generated during the solution
look as follow: 

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-5.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-6.png
    </td>

    <td ALIGN="center">
    </td>
  </tr>
</table>

Again, they show essentially the location of singularities introduced
by boundary conditions. The vector field computed makes for an
interesting graph:

@image html step-31.3d.solution.png

The isocountours shown here as well are those of the pressure
variable, showing the singularity at the point of discontinuous
velocity boundary conditions.



<h3>Sparsity pattern</h3>

As explained during the generation of the sparsity pattern, it is
important to have the numbering of degrees of freedom in mind when
using preconditioners like incomplete LU decompositions. This is most
conveniently visualized using the distribution of nonzero elements in
the stiffness matrix.

If we don't do anything special to renumber degrees of freedom (i.e.,
without using DoFRenumbering::Cuthill_McKee, but with using
DoFRenumbering::component_wise to ensure that degrees of freedom are
appropriately sorted into their corresponding blocks of the matrix and
vector), then we get the following image after the first adaptive
refinement in two dimensions:

@image html step-31.2d.sparsity-nor.png

In order to generate such a graph, you have to insert a piece of
code like the following to the end of the setup step.  Note that it is
not possible to directly output a BlockSparsityPattern, so we need to
generate some temporary objects that will be released again in order
to not slow down the program.
@code
  {
    SparsityPattern complete_sparsity_pattern;
    CompressedSparsityPattern csp (dof_handler.n_dofs(),
                                   dof_handler.n_dofs());
    DoFTools::make_sparsity_pattern(dof_handler, csp);
    hanging_node_constraints.condense (csp);
    complete_sparsity_pattern.copy_from(csp);
    std::ofstream out ("sparsity_pattern.gpl");
    complete_sparsity_pattern.print_gnuplot(out);
  }
@endcode



It is clearly visible that the dofs are spread over almost the whole matrix.
This makes preconditioning by ILU inefficient: ILU generates a Gaussian
elimination (LU decomposition) without fill-in elements, which means that more 
tentative fill-ins left out will result in a worse approximation of the complete 
decomposition.

In this program, we have thus chosen a more advanced renumbering of components.
The renumbering with Cuthill_McKee and grouping the components into velocity
and pressure yields the following output. 

@image html step-31.2d.sparsity-ren.png

It is apparent that the situation has improved a lot. Most of the
elements are now concentrated around the diagonal in the (0,0) block in the
matrix. Similar effects are also visible for the other blocks. In this case, the
ILU decomposition will be much closer to the full LU decomposition, which
improves the quality of the preconditioner. It is also worthwile to note that
the sparse direct solver UMFPACK does some internal renumbering of the equations 
before actually generating a sparse LU decomposition that leads to a 
similar pattern as the one we got from Cuthill_McKee.

Finally, we want to have a closer
look at a sparsity pattern in 3D. We show only the (0,0) block of the
matrix, again after one adaptive refinement. Apart from the fact that the matrix
size has increased, it is also visible that there are many more entries
in the matrix. Moreover, even for the optimized renumbering, there will be a
considerable amount of tentative fill-in elements. This illustrates why UMFPACK 
is not a good choice in 3D - a full decomposition needs many new entries that
 eventually won't fit into the physical memory (RAM).

@image html step-31.3d.sparsity_uu-ren.png


<h2>Possible Extensions</h2>

<a name="improved-solver">
<h3>Improved linear solver</h3>
</a>

We have seen in the section of computational results that the number of outer
iterations does not depend on the mesh size, which is optimal in a sense of
scalability. This is however only partly true, since we did not look at the
number of iterations for the inner iterations, i.e. generating the inverse of
the matrix $A$ and the mass matrix $M_p$. Of course, this is
unproblematic in the 2D case where we precondition with a direct solver and the
vmult operation of the inverse matrix structure will converge in one single CG
step, but this changes in 3D where we need to apply the ILU preconditioner.
There, the number of required step basically doubles with half the element size,
so the work gets more and more for larger systems. For the 3D results obtained
above, each vmult operation involves approx. 14, 23, 36, 59, 72, 101 etc. inner 
CG iterations. (On the other hand, the number of iterations for applying the 
inverse pressure mass matrix is always about 10-11.)
To summarize, most work is spent on
creating the same matrix inverse over and over again. It is a natural question
to ask whether we can do that any better and avoid inverting the same 
(complicated) matrix several times.
  
The answer is, of course, that we can do that in a few other (most of the time
better) ways.

The first way would be to choose a preconditioner that makes CG
for the (0,0) matrix converge in a mesh-independent number of iterations, say
10 to 30. We have seen such a canditate in @ref step_16 "step-16": multigrid.

<h4>Block Schur complement preconditioner</h4>
But even in this situation there would still be need for the repeated solution
of the same linear system. The question is how this can be avoided. If we
persist in calculating the Schur complement, there is no other possibility.
The alternative is to attack the block system at one time and use an approximate
Schur complement as an efficient preconditioner. The basic attempt is as
follows: If we find a block preconditioner $P$ such that the matrix
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is simple, then an iterative solver with that preconditioner will converge in a
few iterations. Using the Schur complement $S = B A^{-1} B^T$, one finds that
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    I & A^{-1} B^T \\ 0 & 0
  \end{array}\right),
@f}
see also the paper by Silvester and Wathen referenced to in the introduction. In
this case, a Krylov-based iterative method will converge in two steps, since
there are only two distinct eigenvalues 0 and 1 of this matrix. Though, it is
not possible to use CG since the block matrix is not positive definite (it has
indeed both positive and negative eigenvalues, i.e. it is indefinite). Instead
one has to choose e.g. the iterative solver GMRES, which is more expensive
per iteration (more inner products to be calculated), but is applicable also to
indefinite (and non-symmetric) matrices.

Since $P$ is aimed to be a preconditioner only, we shall only use
approximations to the Schur complement $S$ and the matrix $A$. So an
improved solver for the Stokes system is going to look like the following: 
The Schur
complement will be approximated by the pressure mass matrix $M_p$, and we use a
preconditioner to $A$ (without an InverseMatrix class around it) to approximate 
$A$. The advantage of this approach is however partly compensated by the fact 
that we need to perform the solver iterations on the full block system i
nstead of the smaller pressure space.

An implementation of such a solver could look like this:

First the class for the block Schur complement preconditioner, which implements
a vmult operation of the preconditioner for block vectors. Note that the
preconditioner $P$ described above is implemented by three successive
operations.
@code
template <class Preconditioner>
class BlockSchurPreconditioner : public Subscriptor
{
  public:
    BlockSchurPreconditioner (const BlockSparseMatrix<double>           &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionSSOR<> > &Mpinv,
          const Preconditioner &Apreconditioner);

  void vmult (BlockVector<double>       &dst,
              const BlockVector<double> &src) const;

  private:
    const SmartPointer<const BlockSparseMatrix<double> > system_matrix;
    const SmartPointer<const InverseMatrix<SparseMatrix<double>, 
                       PreconditionSSOR<> > > m_inverse;
    const Preconditioner &a_preconditioner;
    
    mutable BlockVector<double> tmp;

};

template <class Preconditioner>
BlockSchurPreconditioner<Preconditioner>::BlockSchurPreconditioner(
          const BlockSparseMatrix<double>                               &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionSSOR<> > &Mpinv,
          const Preconditioner &Apreconditioner
          )
                :
                system_matrix           (&S),
                m_inverse               (&Mpinv),
                a_preconditioner        (Apreconditioner),
                tmp                     (2)
{
        // We have to initialize the <code>BlockVector@</code>
        // tmp to the correct sizes in the respective blocks
    tmp.block(0).reinit(S.block(0,0).m());
    tmp.block(1).reinit(S.block(1,1).m());
    tmp.collect_sizes();
}

        // Now the interesting function, the multiplication of
        // the preconditioner with a BlockVector. 
template <class Preconditioner>
void BlockSchurPreconditioner<Preconditioner>::vmult (
                                     BlockVector<double>       &dst,
                                     const BlockVector<double> &src) const
{
        // Form u_new = A^{-1} u
  a_preconditioner.vmult (dst.block(0), src.block(0));
        // Form tmp.block(1) = - B u_new + p 
        // (<code>SparseMatrix::residual</code>
        // does precisely this)
  system_matrix->block(1,0).residual(tmp.block(1), 
                                     dst.block(0), src.block(1));
        // Change sign in tmp.block(1)
  tmp.block(1) *= -1;
        // Multiply by approximate Schur complement 
        // (i.e. a pressure mass matrix)
  m_inverse->vmult (dst.block(1), tmp.block(1));
}
@endcode

The actual solver call can be realized as follows.

@code
      SparseMatrix<double> pressure_mass_matrix;
      pressure_mass_matrix.reinit(sparsity_pattern.block(1,1));
      pressure_mass_matrix.copy_from(system_matrix.block(1,1));
      system_matrix.block(1,1) = 0;
            
      PreconditionSSOR<> pmass_preconditioner;
      pmass_preconditioner.initialize (pressure_mass_matrix, 1.2);
      
      InverseMatrix<SparseMatrix<double>,PreconditionSSOR<> >
        m_inverse (pressure_mass_matrix, pmass_preconditioner);
        
      BlockSchurPreconditioner<typename InnerPreconditioner<dim>::type> 
        preconditioner (system_matrix, m_inverse, *A_preconditioner);
      
      SolverControl solver_control (system_matrix.m(),
                                    1e-6*system_rhs.l2_norm());
      
      SolverGMRES<BlockVector<double> > gmres(solver_control);
      
      gmres.solve(system_matrix, solution, system_rhs,
                  preconditioner);
      
      hanging_node_constraints.distribute (solution);
      
      std::cout << " "
                << solver_control.last_step()
                << " block GMRES iterations";
@endcode

Obviously, one needs to add the include file @ref SolverGMRES 
"<lac/solver_gmres.h>" in order to make this run. 
We call the solver with a BlockVector template in order to enable
GMRES to operate on block vectors and matrices.
Note also that we need to set the (1,1) block in the system
matrix to zero (we saved the pressure mass matrix there which is not part of the
problem) after we copied to information to another matrix.

Using the timer class, we collected some statistics that compare the runtime of
the block solver with the one used in the problem implementation above. Besides
the solution of the two systems we also check if the solutions to the two
systems are close to each other, i.e. we calculate the infinity norm of the
vector difference.

Let's first see the results in 2D:
@code
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85) [0.025624 s] 
   Assembling... [0.052874 s]
   Computing preconditioner... [0.016394 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.028722 s]
      Block Schur preconditioner: 11 GMRES iterations [0.027471 s]
   max difference l_infty in the two solution vectors: 6.20426e-06

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201) [0.07247 s] 
   Assembling... [0.129607 s]
   Computing preconditioner... [0.045196 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.074961 s]
      Block Schur preconditioner: 12 GMRES iterations [0.080309 s]
   max difference l_infty in the two solution vectors: 1.48144e-06

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443) [0.168264 s] 
   Assembling... [0.298043 s]
   Computing preconditioner... [0.122656 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.19247 s]
      Block Schur preconditioner: 12 GMRES iterations [0.211514 s]
   max difference l_infty in the two solution vectors: 4.30711e-06

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001) [0.390052 s] 
   Assembling... [0.694365 s]
   Computing preconditioner... [0.315401 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [0.54384 s]
      Block Schur preconditioner: 12 GMRES iterations [0.588273 s]
   max difference l_infty in the two solution vectors: 1.03471e-05

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197) [0.874274 s] 
   Assembling... [1.58498 s]
   Computing preconditioner... [0.813296 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [1.22453 s]
      Block Schur preconditioner: 13 GMRES iterations [1.45711 s]
   max difference l_infty in the two solution vectors: 4.53216e-05

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605) [1.85269 s] 
   Assembling... [3.36536 s]
   Computing preconditioner... [1.93873 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [3.39201 s]
      Block Schur preconditioner: 13 GMRES iterations [3.77069 s]
   max difference l_infty in the two solution vectors: 0.000291127

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411) [3.85563 s] 
   Assembling... [6.9564 s]
   Computing preconditioner... [4.8595 s]
   Solving...
      Schur complement: 11 outer CG iterations for p  [7.69326 s]
      Block Schur preconditioner: 13 GMRES iterations [8.62034 s]
   max difference l_infty in the two solution vectors: 0.000245764
@endcode

We see that the runtime for solution using the block Schur complement 
preconditioner is higher than the one with the Schur complement directly. The
reason is simple: We used a direct solve as preconditioner here - so there won't
be any gain by avoiding the inner iterations (indeed, we see that slightly more
iterations are needed).

The picture of course changes in 3D:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81) [0.344441 s] 
   Assembling... [1.38754 s]
   Computing preconditioner... [1.29145 s]
   Solving...
      Schur complement: 13 outer CG iterations for p  [0.609626 s]
      Block Schur preconditioner: 24 GMRES iterations [0.107059 s]
   max difference l_infty in the two solution vectors: 1.60165e-05

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261) [2.21296 s] 
   Assembling... [6.28298 s]
   Computing preconditioner... [7.78238 s]
   Solving...
      Schur complement: 14 outer CG iterations for p  [5.98275 s]
      Block Schur preconditioner: 44 GMRES iterations [0.979435 s]
   max difference l_infty in the two solution vectors: 2.00892e-05

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055) [12.2284 s] 
   Assembling... [30.8002 s]
   Computing preconditioner... [42.131 s]
   Solving...
      Schur complement: 14 outer CG iterations for p  [45.1115 s]
      Block Schur preconditioner: 83 GMRES iterations [9.04946 s]
   max difference l_infty in the two solution vectors: 3.06288e-05

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133) [54.9067 s] 
   Assembling... [137.506 s]
   Computing preconditioner... [192.36 s]
   Solving...
      Schur complement: 15 outer CG iterations for p  [371.85 s]
      Block Schur preconditioner: 204 GMRES iterations [110.316 s]
   max difference l_infty in the two solution vectors: 8.92999e-05
@endcode

Here, the block preconditioned solver is clearly superior to the Schur
complement, even though the advantage gets less for more mesh points. The reason
for the decreasing advantage is that the mass matrix is still inverted
iteratively, so that it is inverted more and more times when the block GMRES
solver uses more iterations.

<h4>No block matrices and vectors</h4>
Another possibility that can be taken into account is to not set up a block
system, but rather solve the system of velocity and pressure all at once. The
alternatives are direct solve with UMFPACK (2D) or GMRES with ILU
preconditioning (3D).
