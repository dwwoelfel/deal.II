<a name="Results"></a>
<h1>Results</h1>

<h3>Output of the program and graphical visualization</h3>

<h4>2D calculations</h4>

Running the program with the space dimension set to 2 in
<code>main()</code> yields the following output:
@code
examples/step-31> make run
============================ Remaking Makefile.dep
==============optimized===== step-31.cc
============================ Linking step-31
============================ Running step-31
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411)
   Assembling...
   Computing preconditioner...
   Solving...  11 outer CG Schur complement iterations for pressure
@endcode
The entire computation above takes about 30 seconds on a reasonably
quick (for 2007 standards) machine.

What we see immediately from this is that the number of (outer)
iterations does not increase as we refine the mesh. This confirms the
statement in the introduction that preconditioning the Schur
complement with the mass matrix indeed yields a matrix spectrally
equivalent to the identity matrix (i.e. with eigenvalues bounded above
and below independently of the mesh size or the relative sizes of
cells). In other words, the mass matrix and the Schur complement are
spectrally equivalent.

In the images below, we show the grids for the first six refinement
steps in the program.  Observe how the grid is refined in regions
where the solution rapidly changes: On the upper boundary, we have
Dirichlet boundary conditions that are -1 in the left half of the line
and 1 in the right one, so there is an aprupt change at $x=0$. Likewise,
there are changes from Dirichlet to Neumann data in the two upper
corners, so there is need for refinement there as well:

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.2d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-31.2d.mesh-5.png
    </td>
  </tr>
</table>

Finally, following is a plot of the flow field. It shows fluid
transported along with the moving upper boundary and being replaced by
material coming from below:

@image html step-31.2d.solution.png

This plot uses the capability of VTK-based visualization programs (in
this case of VisIt) to show vector data; this is the result of us
declaring the velocity components of the finite element in use to be a
set of vector components, rather than independent scalar components in
the <code>StokesProblem@<dim@>::output_results</code> function of this
tutorial program.



<h4>3D calculations</h4>

In 3d, the screen output of the program looks like this:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81)
   Assembling...
   Computing preconditioner...
   Solving...  13 outer CG Schur complement iterations for pressure.

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 4
   Number of active cells: 11456
   Number of degrees of freedom: 327808 (313659+14149)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.

Refinement cycle 5
   Number of active cells: 45056
   Number of degrees of freedom: 1254464 (1201371+53093)
   Assembling...
   Computing preconditioner...
   Solving...  14 outer CG Schur complement iterations for pressure.

Refinement cycle 6
   Number of active cells: 170720
   Number of degrees of freedom: 4625150 (4432407+192743)
   Assembling...
   Computing preconditioner...
   Solving...  15 outer CG Schur complement iterations for pressure.
@endcode

Again, we see that the number of outer iterations does not increase as
we refine the mesh. Nevertheless, the compute time increases
significantly: for each of the iterations above separately, it takes a
few seconds, a few seconds, 1min, 5min, 29min, 3h12min, and 21h39min
for the finest level with more than 4.5 million unknowns. This
superlinear (in the number of unknowns) increase is due to first the
superlinear number of operations to compute the ILU decomposition, and
secondly the fact
that our inner solver is not ${\cal O}(N)$: a simple experiment shows
that as we keep refining the mesh, the average number of
ILU-preconditioned CG iterations to invert the velocity-velocity block
$A$ increases.

We will address the question of how possibly to improve our solver <a
href="improved-solver">below</a>.

As for the graphical output, the grids generated during the solution
look as follow: 

<TABLE WIDTH="60%" ALIGN="center">
  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-0.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-1.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-2.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-3.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-4.png
    </td>

    <td ALIGN="center">
      @image html step-31.3d.mesh-5.png
    </td>
  </tr>

  <tr>
    <td ALIGN="center">
      @image html step-31.3d.mesh-6.png
    </td>

    <td ALIGN="center">
    </td>
  </tr>
</table>

Again, they show essentially the location of singularities introduced
by boundary conditions. The vector field computed makes for an
interesting graph:

@image html step-31.3d.solution.png

The isocountours shown here as well are those of the pressure
variable, showing the singularity at the point of discontinuous
velocity boundary conditions.



<h3>Sparsity pattern</h3>

As explained during the generation of the sparsity pattern, it is
important to have the numbering of degrees of freedom in mind when
using preconditioners like incomplete LU decompositions. This is most
conveniently visualized using the distribution of nonzero elements in
the stiffness matrix.

If we don't do anything special to renumber degrees of freedom (i.e.,
without using DoFRenumbering::Cuthill_McKee, but with using
DoFRenumbering::component_wise to ensure that degrees of freedom are
appropriately sorted into their corresponding blocks of the matrix and
vector, then we get the following image after the first adaptive
refinement in two dimensions:

@image html step-31.2d.sparsity-nor.png

In order to generate such a graph, you have to insert a piece of
code like the following to the end of the setup step. 
@code
  {
    std::ofstream out ("sparsity_pattern.gpl");
    sparsity_pattern.print_gnuplot(out);
  }
@endcode



It is clearly visible that the nonzero entries are spread over almost the
whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a
Gaussian elimination (LU decomposition) without fill-in elements, which means
that more tentative fill-ins left out will result in a worse approximation of
the complete decomposition.

In this program, we have thus chosen a more advanced renumbering of
components.  The renumbering with DoFRenumbering::Cuthill_McKee and grouping
the components into velocity and pressure yields the following output:

@image html step-31.2d.sparsity-ren.png

It is apparent that the situation has improved a lot. Most of the elements are
now concentrated around the diagonal in the (0,0) block in the matrix. Similar
effects are also visible for the other blocks. In this case, the ILU
decomposition will be much closer to the full LU decomposition, which improves
the quality of the preconditioner. (It may be interesting to note that the
sparse direct solver UMFPACK does some internal renumbering of the equations
before actually generating a sparse LU decomposition; that procedure leads to
a very similar pattern to the one we got from the Cuthill-McKee algorithm.)

Finally, we want to have a closer
look at a sparsity pattern in 3D. We show only the (0,0) block of the
matrix, again after one adaptive refinement. Apart from the fact that the matrix
size has increased, it is also visible that there are many more entries
in the matrix. Moreover, even for the optimized renumbering, there will be a
considerable amount of tentative fill-in elements. This illustrates why UMFPACK 
is not a good choice in 3D - a full decomposition needs many new entries that
 eventually won't fit into the physical memory (RAM):

@image html step-31.3d.sparsity_uu-ren.png


<h2>Possible Extensions</h2>

<a name="improved-solver">
<h3>Improved linear solver</h3>
</a>

We have seen in the section of computational results that the number of outer
iterations does not depend on the mesh size, which is optimal in a sense of
scalability. This is however only partly true, since we did not look at the
number of iterations for the inner iterations, i.e. generating the inverse of
the matrix $A$ and the mass matrix $M_p$. Of course, this is unproblematic in
the 2D case where we precondition with a direct solver and the
<code>vmult</code> operation of the inverse matrix structure will converge in
one single CG step, but this changes in 3D where we need to apply the ILU
preconditioner.  There, the number of required preconditioned CG steps to
invert $A$ basically increases as the mesh is refined. For 
the 3D results obtained above, each <code>vmult</code> operation involves
on average approximately 14, 23, 36, 59, 72, 101, ... inner CG iterations in
the refinement steps shown above. (On the other hand,
the number of iterations for applying the inverse pressure mass matrix is
always about 10-11.)  To summarize, most work is spent on solving linear
systems with the same
matrix $A$ over and over again. It is a natural question to ask whether we
can do that better.
  
The answer is, of course, that we can do that in a few other (most of the time
better) ways.

The first way would be to choose a preconditioner that makes CG
for the (0,0) matrix converge in a mesh-independent number of iterations, say
10 to 30. We have seen such a canditate in @ref step_16 "step-16": multigrid.

<h4>Block Schur complement preconditioner</h4>
But even in this situation there would still be need for the repeated solution
of the same linear system. The question is how this can be avoided. If we
persist in calculating the Schur complement, there is no other possibility.
The alternative is to attack the block system at one time and use an approximate
Schur complement as an efficient preconditioner. The basic idea is as
follows: If we find a block preconditioner $P$ such that the matrix
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is simple, then an iterative solver with that preconditioner will converge in a
few iterations. Using the Schur complement $S = B A^{-1} B^T$, one finds that
@f{eqnarray*}
  P^{-1}
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
@f}
would appear to be a good choice since
@f{eqnarray*}
  P^{-1}\left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)\cdot \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
  = 
  \left(\begin{array}{cc}
    I & A^{-1} B^T \\ 0 & 0
  \end{array}\right).
@f}
This is the approach taken by the paper by Silvester and Wathen referenced to
in the introduction. In 
this case, a Krylov-based iterative method will converge in two steps, since
there are only two distinct eigenvalues 0 and 1 of this matrix. The resulting
preconditioned matrix can not be solved using CG since it is neither positive
definite not symmetric. On the other hand, the iterative solver BiCGStab, which 
is more expensive per iteration (more inner products are to be calculated) is 
applicable to this sort of matrices. (One could also think of using GMRES, but
that solver has the disadvantage that it needs all Krylov basis vectors in each
iteration - which are many if we need hundreds of iterations. Deal.II does not
use more than 30 by default, so the basis is rebuild after that - which however
leads to slow convergence.)

Since $P$ is aimed to be a preconditioner only, we shall only use
approximations to the Schur complement $S$ and the matrix $A$. So an improved
solver for the Stokes system is going to look like the following: The Schur
complement will be approximated by the pressure mass matrix $M_p$, and we use
a preconditioner to $A$ (without an InverseMatrix class around it) to
approximate $A^{-1}$. The advantage of this approach is however partly
compensated by the fact that we need to perform the solver iterations on the
full block system instead of the smaller pressure space.

An implementation of such a solver could look like this:

First the class for the block Schur complement preconditioner, which implements
a <code>vmult</code> operation of the preconditioner for block vectors. Note
that the preconditioner $P$ described above is implemented by three successive
operations.
@code
template <class PreconditionerA, class PreconditionerMp>
class BlockSchurPreconditioner : public Subscriptor
{
  public:
    BlockSchurPreconditioner (const BlockSparseMatrix<double>         &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp>  &Mpinv,
          const PreconditionerA &Apreconditioner);

  void vmult (BlockVector<double>       &dst,
              const BlockVector<double> &src) const;

  private:
    const SmartPointer<const BlockSparseMatrix<double> > system_matrix;
    const SmartPointer<const InverseMatrix<SparseMatrix<double>, 
                       PreconditionerMp > > m_inverse;
    const PreconditionerA &a_preconditioner;
    
    mutable BlockVector<double> tmp;

};

template <class PreconditionerA, class PreconditionerMp>
BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::BlockSchurPreconditioner(
          const BlockSparseMatrix<double>                            &S,
          const InverseMatrix<SparseMatrix<double>,PreconditionerMp> &Mpinv,
          const PreconditionerA &Apreconditioner
          )
                :
                system_matrix           (&S),
                m_inverse               (&Mpinv),
                a_preconditioner        (Apreconditioner),
                tmp                     (2)
{
        // We have to initialize the <code>BlockVector@</code>
        // tmp to the correct sizes in the respective blocks
    tmp.block(0).reinit(S.block(0,0).m());
    tmp.block(1).reinit(S.block(1,1).m());
    tmp.collect_sizes();
}

        // Now the interesting function, the multiplication of
        // the preconditioner with a BlockVector. 
template <class PreconditionerA, class PreconditionerMp>
void BlockSchurPreconditioner<PreconditionerA, PreconditionerMp>::vmult (
                                     BlockVector<double>       &dst,
                                     const BlockVector<double> &src) const
{
        // Form u_new = A^{-1} u
  a_preconditioner.vmult (dst.block(0), src.block(0));
        // Form tmp.block(1) = - B u_new + p 
        // (<code>SparseMatrix::residual</code>
        // does precisely this)
  system_matrix->block(1,0).residual(tmp.block(1), 
                                     dst.block(0), src.block(1));
        // Change sign in tmp.block(1)
  tmp.block(1) *= -1;
        // Multiply by approximate Schur complement 
        // (i.e. a pressure mass matrix)
  m_inverse->vmult (dst.block(1), tmp.block(1));
}
@endcode

The actual solver call can be realized as follows:

@code
      SparseMatrix<double> pressure_mass_matrix;
      pressure_mass_matrix.reinit(sparsity_pattern.block(1,1));
      pressure_mass_matrix.copy_from(system_matrix.block(1,1));
      system_matrix.block(1,1) = 0;

      SparseILU<double> pmass_preconditioner;
      pmass_preconditioner.initialize (pressure_mass_matrix, 
        SparseILU<double>::AdditionalData());
      
      InverseMatrix<SparseMatrix<double>,SparseILU<double> >
        m_inverse (pressure_mass_matrix, pmass_preconditioner);

      BlockSchurPreconditioner<typename InnerPreconditioner<dim>::type,
                               SparseILU<double> > 
        preconditioner (system_matrix, m_inverse, *A_preconditioner);
      
      SolverControl solver_control (system_matrix.m(),
                                    1e-8*system_rhs.l2_norm());
      
      SolverBicgstab<BlockVector<double> > bicgstab(solver_control);
      
      bicgstab.solve(system_matrix, solution, system_rhs,
                     preconditioner);
      
                         // Produce a constistent solution field
      hanging_node_constraints.distribute (solution);
      
      std::cout << " "
                << solver_control.last_step()
                << " block BiCGStab iterations";
@endcode

Obviously, one needs to add the include file @ref SolverBicgstab 
"<lac/solver_bicgstab.h>" in order to make this run.
We call the solver with a BlockVector template in order to enable
BiCGStab to operate on block vectors and matrices.
Note also that we need to set the (1,1) block in the system
matrix to zero (we saved the pressure mass matrix there which is not part of the
problem) after we copied the information to another matrix. Additionally, we
chose a slightly more stringent tolerance for BiCGStab since we consider the
whole system and not some subblocks.

Using the Timer class, we can collect some statistics that compare the runtime
of the block solver with the one used in the problem implementation above (on
a different machine than the one for which timings were reported
above). Besides the solution of the two systems we also check if the solutions
to the two systems are close to each other, i.e. we calculate the infinity
norm of the vector difference.

Let's first see the results in 2D:
@code
Refinement cycle 0
   Number of active cells: 64
   Number of degrees of freedom: 679 (594+85) [0.013907 s] 
   Assembling... [0.036652 s]
   Computing preconditioner... [0.007464 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [0.012651 s]
      Block Schur preconditioner:    8 BiCGStab iterations [0.013988 s]
   max difference l_infty between solution vectors: 7.90877e-07

Refinement cycle 1
   Number of active cells: 160
   Number of degrees of freedom: 1683 (1482+201) [0.032847 s] 
   Assembling... [0.089153 s]
   Computing preconditioner... [0.020322 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [0.033447 s]
      Block Schur preconditioner:    8 BiCGStab iterations [0.039152 s]
   max difference l_infty between solution vectors: 1.91232e-06

Refinement cycle 2
   Number of active cells: 376
   Number of degrees of freedom: 3813 (3370+443) [0.075853 s] 
   Assembling... [0.207106 s]
   Computing preconditioner... [0.056309 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [0.109884 s]
      Block Schur preconditioner:    8 BiCGStab iterations [0.124498 s]
   max difference l_infty between solution vectors: 6.68936e-06

Refinement cycle 3
   Number of active cells: 880
   Number of degrees of freedom: 8723 (7722+1001) [0.17642 s] 
   Assembling... [0.484385 s]
   Computing preconditioner... [0.152472 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [0.338477 s]
      Block Schur preconditioner:    8 BiCGStab iterations [0.382371 s]
   max difference l_infty between solution vectors: 1.01301e-05

Refinement cycle 4
   Number of active cells: 2008
   Number of degrees of freedom: 19383 (17186+2197) [0.398735 s] 
   Assembling... [1.10472 s]
   Computing preconditioner... [0.420046 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [0.835033 s]
      Block Schur preconditioner:    8 BiCGStab iterations [1.00724 s]
   max difference l_infty between solution vectors: 3.2143e-05

Refinement cycle 5
   Number of active cells: 4288
   Number of degrees of freedom: 40855 (36250+4605) [0.844131 s] 
   Assembling... [2.34257 s]
   Computing preconditioner... [1.00229 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [1.9538 s]
      Block Schur preconditioner:    8 BiCGStab iterations [2.39038 s]
   max difference l_infty between solution vectors: 8.46393e-05

Refinement cycle 6
   Number of active cells: 8896
   Number of degrees of freedom: 83885 (74474+9411) [1.76458 s] 
   Assembling... [4.88486 s]
   Computing preconditioner... [2.32077 s]
   Solving...
      Schur complement:  11 outer CG iterations for p      [4.2994 s]
      Block Schur preconditioner:    7 BiCGStab iterations [4.48932 s]
   max difference l_infty between solution vectors: 0.000244068
@endcode

We see that there is no huge difference in the solution time between
the block Schur complement preconditioner solver and 
the actual Schur complement. The
reason is simple: we used a direct solve as preconditioner for the latter - so
there won't be any gain by avoiding the inner iterations. We see that the number
of iterations has decreased a bit for BiCGStab, but one step is more expensive
here and so there is no gain.


The picture of course changes in 3D:

@code
Refinement cycle 0
   Number of active cells: 32
   Number of degrees of freedom: 1356 (1275+81) [0.162387 s] 
   Assembling... [0.867126 s]
   Computing preconditioner... [0.599154 s]
   Solving...
      Schur complement:  13 outer CG iterations for p      [0.269857 s]
      Block Schur preconditioner:   16 BiCGStab iterations [0.059217 s]
   max difference l_infty between solution vectors: 1.10398e-05

Refinement cycle 1
   Number of active cells: 144
   Number of degrees of freedom: 5088 (4827+261) [1.01979 s] 
   Assembling... [3.88896 s]
   Computing preconditioner... [3.56172 s]
   Solving...
      Schur complement:  14 outer CG iterations for p      [5.37291 s]
      Block Schur preconditioner:   28 BiCGStab iterations [1.07732 s]
   max difference l_infty between solution vectors: 2.55495e-05

Refinement cycle 2
   Number of active cells: 704
   Number of degrees of freedom: 22406 (21351+1055) [5.64807 s] 
   Assembling... [19.0596 s]
   Computing preconditioner... [18.7171 s]
   Solving...
      Schur complement:  14 outer CG iterations for p      [43.0203 s]
      Block Schur preconditioner:   53 BiCGStab iterations [10.3121 s]
   max difference l_infty between solution vectors: 4.11953e-05

Refinement cycle 3
   Number of active cells: 3168
   Number of degrees of freedom: 93176 (89043+4133) [25.135 s] 
   Assembling... [85.175 s]
   Computing preconditioner... [87.0619 s]
   Solving...
      Schur complement:  15 outer CG iterations for p      [319.224 s]
      Block Schur preconditioner:  118 BiCGStab iterations [104.231 s]
   max difference l_infty between solution vectors: 7.74303e-05
@endcode

Here, the block preconditioned solver is clearly superior to the Schur
complement, even though the advantage gets less for more mesh points. There are 
two reason for that. The first one is that it is still necessary to invert the
mass matrix iteratively, which means more work if we need more (outer)
iterations. The second reason is related to the solver: BiCGStab scales slightly
worse with the size of the problem than the iterator for the CG solver build
into the Schur complement. Nonetheless, the improvement by a factor of 3-4 is
quite impressive.

<h4>No block matrices and vectors</h4>
Another possibility that can be taken into account is to not set up a block
system, but rather solve the system of velocity and pressure all at once. The
alternatives are direct solve with UMFPACK (2D) or GMRES with ILU
preconditioning (3D). It should be straightforward to try that.
