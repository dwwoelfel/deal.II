<br>

<i>This program was contributed by Martin Kronbichler and Wolfgang
Bangerth. 
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of 
Technology. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation or of The
California Institute of Technology.
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

This program does pretty much exactly what @ref step_31 "step-31" already
does: it solves the Boussinesq equations that describe the motion of a fluid
whose temperature is not in equilibrium. As such, all the equations we have
described in @ref step_31 "step-31" still hold: we solve the same partial
differential equation, using the same finite element scheme, the same time
stepping algorithm, and the same stabilization method for the temperature
advection-diffusion equation. As a consequence, you may first want to
understand that program before you work on the current one.

The difference between @ref step_31 "step-31" and the current program is that
here we want to do things in %parallel, using both the availability of many
machines in a cluster (with parallelization based on MPI) as well as many
processor cores within a single machine (with parallelization based on
threads). This program's main job is therefore to introduce the changes that are
necessary to utilize the availability of these %parallel compute resources.

In addition to these changes, we also use a slightly different preconditioner,
which we will discuss first.


<h3> Changes to the Stokes preconditioner </h3>

In this tutorial program, we apply a variant of the preconditioner used in
@ref step_31 "step-31". That preconditioner was built to operate on the
system matrix <i>M</i> in block form such that the product matrix
@f{eqnarray*}
  P^{-1} M
  = 
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right) 
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right) 
@f}
is of a form that Krylov-based iterative solvers like GMRES can solve in a
few iterations. We then replaced the exact inverse of <i>A</i> by the action
of an AMG preconditioner $\tilde{A}$ based on a vector Laplace matrix,
approximated the Schur complement $S = B A^{-1} B^T$ by a mass matrix $M_p$
on the pressure space and wrote an <tt>InverseMatrix</tt> class for
implementing the action of $M_p^{-1}\approx S^{-1}$ on vectors. In the
InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC)
preconditioner for performing the inner solves.

An observation one can make is that we use just the action of a
preconditioner for approximating the velocity inverse $A^{-1}$ (and the
outer GMRES iteration takes care of the approximate character of the
inverse), whereas we use a more or less <i>exact</i> inverse for $M_p^{-1}$,
realized by a fully converged CG solve. What we change here is to skip that
<i>exact</i> inverse matrix and replace it &ndash; as usual &ndash; by the
action of a preconditioner only. This works, as we will demonstrate
below. For efficiency reasons, we want to avoid increasing the number of
iterations for the block solve. Keep in mind that most of the time in the
solution of the matrix system is the application of the AMG preconditioner
(about half the time of the total solve), and the application of matrix
<i>A</i> (about one third of the total solve time). This means that we
really do not want to do those operations more often when we remove the
inner solve on the Schur complement approximation. It turns out that the
Trilinos IC preconditioner would not fulfill this requirement, however, the
Trilinos ILU preconditioner does. It does even better than so &mdash; it
decreases the iteration count for large 3D problems. The reason for that
decrease is that we avoid some errors that CG introduces: Even a converged
solve has still some residual. That is a problem because that small error
interferes with the outer iterative solver, probably because a CG solver
does some nonlinear operations by weighting vectors by some inner products.

Except the simplification in the preconditioner, we replaced the GMRES
solver by BiCGStab. This is merely to demonstrate that GMRES is not the only
possible option for a saddle point system like the Stokes
equations. BiCGStab harmonizes nicely with the ILU preconditioner on a
pressure mass matrix as approximation for $S^{-1}$, so it is at least as
good as GMRES in this example. Keep in mind the discussion in the results
section of the @ref step_22 "step-22" tutorial program, where we observed
that BiCGStab does <i>not</i> like inner solves with CG, which made us
choose GMRES in step-31

<h3> Parallelization on clusters </h3>

Parallelization of scientific codes across multiple machines in a cluster of
computers is almost always done using the Message Passing Interface
(MPI). This program is no exception to that, and it follows the
@ref step_17 "step-17" and @ref step_18 "step-18" programs in this. 

MPI is a rather awkward interface to program with, and so we usually try to
not use it directly but through an interface layer that abstracts most of the
MPI operations into a friendlier interface. In the two programs mentioned
above, this was achieved by using the PETSc library that provides support for
%parallel linear algebra in a way that almost completely hides the MPI layer
under it. PETSc is powerful, providing a large number of functions that deal
with matrices, vectors, and iterative solvers and preconditioners, along with
lots of other stuff, most of which runs quite well in %parallel. It is,
however, a few years old already, written in C, and generally not quite as
easy to use as some other libraries. As a consequence, deal.II also has
interfaces to Trilinos, a library similar to PETSc in its aims and with a lot
of the same functionality. It is, however, a project that is several years
younger, is written in C++ and by people who generally have put a significant
emphasis on software design. We have already used Trilinos in 
@ref step_31 "step-31", and will do so again here, with the difference that we
will use its %parallel capabilities.

deal.II's Trilinos interfaces encapsulate pretty much everything Trilinos
provides into wrapper classes (in namespace TrilinosWrappers) that make the
Trilinos matrix, vector, solver and preconditioner classes look very much the
same as deal.II's own implementations of this functionality. However, as
opposed to deal.II's classes, they can be used in %parallel if we give them the
necessary information. As a consequence, there are two Trilinos classes that
we have to deal with directly (rather than through wrappers), both of which
are part of Trilinos' Epetra library of basic linear algebra and tool classes: 
<ul>
<li> The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.
  it describes how many and which machines can communicate with each other.
  Each distributed object, such as a sparse matrix or a vector for which we
  may want to store parts on different machines, needs to have a communicator
  object to know how many parts there are, where they can be found, and how
  they can be accessed.

  In this program, we only really use one communicator object -- based on the
  MPI variable <code>MPI_COMM_WORLD</code> -- that encompasses <i>all</i>
  processes that work together. It would be perfectly legitimate to start a
  process on $N$ machines but only store vectors on a subset of these by
  producing a communicator object that only encompasses this subset of
  machines; there is really no compelling reason to do so here, however. As a
  second note, while we use <code>MPI_COMM_WORLD</code> as communicator in the
  program's source code, every time we create a Trilinos object in the wrapper
  classes in namespace TrilinosWrappers, we don't use the given communicator
  but instead create a new and unique communicator that happens to have the
  same machines but has a distinct communicator ID. This way, we can ensure
  that all communications that have to do with this, say, sparse matrix really
  only occur on a channel associated with only this one object, while all
  other objects communicate on other channels of their own. This helps in
  debugging, and may also allow some communications to be reordered for better
  %parallel performance because they can be told apart by their communicator
  number, not just their relative timing.

<li> The Epetra_Map class is used to describe which elements of a vector or which
  rows of a matrix reside on a given machine that is part of a
  communicator. To create such an object, you need to know (i) the total
  number of elements or rows, (ii) the number of elements or rows you want to
  store on the current machine, and (iii) which communicator enumerates the
  machines that we want this matrix or vector be stored on. We will set up
  these maps (which we call <code>partitioners</code> in our code, since we
  believe this is a better word) in the
  <code>BoussinesqFlowProblem::setup_dofs</code> function below and then hand
  it to every %parallel object we create.
</ul>

The only other things specific to programming using MPI that we will use in
this program are the following facilities deal.II provides:
<ul>
<li> We need to subdivide the domain into subdomains, each of which will
  represent the cells that one of the processors coupled by MPI shall consider
  its own and work on. This is done using the
  GridTools::partition_triangulation function.
<li> In order to know which elements of a vector or rows of a matrix shall be
  stored on each of the processors, we need to know how many degrees of
  freedom each of the owners of certain subdomains call their own. This is
  conveniently returned by the DoFTools::count_dofs_with_subdomain_association
  function.
</ul>
The rest of the program is almost completely agnostic about the fact that we
don't store all objects completely locally. There will be a few points where
we can not use certain programming techniques (though without making explicit
reference to MPI or parallelization) or where we need access to <i>all</i>
elements of a vector and therefore need to <i>localize</i> its elements
(i.e. create a vector that has all its elements stored on the current
machine), but we will comment on these locations as we get to them in the
program code.


<h3> Parallelization within individual nodes of a cluster </h3>

The second strategy to parallelize a program is to make use of the fact that
most computers today have more than one processor that all have access to the
same memory. In other words, in this model, we don't explicitly have to say
which pieces of data reside where -- all of the data we need is directly
accessible and all we have to do is split <i>processing</i> this data between
the available processors. We will then couple this with the MPI
parallelization outlined above, i.e. we will have all the processors on a
machine work together to, for example, assemble the local contributions to the
global matrix for the cells that this machine actually "owns" but not for
those cells that are owned by other machines. We will use this strategy for
four kinds of operations we frequently do in this program: assembly of the
Stokes and temperature matrices, assembly of the matrix that forms the Stokes
preconditioner, and assembly of the right hand side of the temperature system.

All of these operations essentially look as follows: we need to loop over all
cells for which <code>cell-@>subdomain_id()</code> equals the index our
machine has within the communicator object used for all communication
(i.e. essentially <code>MPI_COMM_WORLD</code>, as explained above), on each
cell we need to assemble the local contributions to the global matrix or
vector, and then we have to copy each cell's contribution into the global
matrix or vector. Note that the first part of this (the loop) defines a range
of iterators on which something has to happen. The second part, assembly of
local contributions is something that takes the majority of CPU time in this
sequence of steps, is a typical example of things that can be done in
%parallel: each cell's contribution is entirely independent of all other cells'
contributions. The third part, copying into the global matrix, must not happen
in %parallel since we are modifying one object and so several threads can not
at the same time read an existing matrix element, add their contribution, and
write the sum back into memory without danger of producing a <a
href="http://en.wikipedia.org/wiki/Race_condition">race condition</a>. 

deal.II has a class that is made for exactly this workflow: WorkStream. Its
use is extensively documented in the module on @ref threads (in the section
on @ref MTWorkStream "the WorkStream class") and we won't repeat here the
rationale and detailed instructions laid out there, though you will want to
read through this module to understand the distinction between scratch space
and per-cell data. Suffice it to mention that we need the following:

- An iterator range for those cells on which we are supposed to work. This is
  provided by the FilteredIterator class which acts just like every other cell
  iterator in deal.II with the exception that it skips all cells that do not
  satisfy a particular predicate (i.e. a criterion that evaluates to true or
  false). In our case, the predicate is whether a cell has the correct
  subdomain id.

- A function that does the work on each cell for each of the tasks identified
  above, i.e. functions that assemble the local contributions to Stokes matrix
  and preconditioner, temperature matrix, and temperature right hand
  side. These are the
  <code>BoussinesqFlowProblem::local_assemble_stokes_system</code>,
  <code>BoussinesqFlowProblem::local_assemble_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::local_assemble_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::local_assemble_temperature_rhs</code> functions in
  the code below. These four functions can all have several instances of each
  running in %parallel.

- %Functions that copy the result of the previous ones into the global object
  and that run sequentially to avoid race conditions. These are the
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_system</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_rhs</code>
  functions.

We will comment on a few more points in the actual code, but in general
their structure should be clear from the discussion in @ref threads.

The underlying technology for WorkStream identifies "tasks" that need to be
worked on (e.g. assembling local contributions on a cell) and schedules these
tasks automatically to available processors. WorkStream creates these tasks
automatically, by splitting the iterator range into suitable chunks, but as
outlined in @ref threads, one can also create tasks explicitly. We use this in
one place in the program, namely where we set up the Stokes system and
preconditioner matrices as well as the temperature matrix. These are
independent operations that, if enough processors are available, can be worked
on in parallel (if not enough processors are available -- because the system
has only one, or because the others are working on something else for us --
then these tasks will be worked on sequentially). Consequently, 
<code>BoussinesqFlowProblem::setup_dofs</code> creates tasks for the three calls
to <code>BoussinesqFlowProblem::setup_stokes_matrix</code>,
<code>BoussinesqFlowProblem::setup_stokes_preconditioner</code>, and
<code>BoussinesqFlowProblem::setup_temperature_matrices</code> that are then
scheduled to available resources. There is one problem with this, however: if
we have more than MPI process running in parallel, then all of these processes
need to communicate in a certain order and that requires that the various
<code>setup_*</code> functions can't run in parallel. For these setup
operations, we will therefore only be able to make use of multiple processor
cores within the same machine if there is only a single MPI process in use.



<h3> The testcase </h3>

TODO: WOLFGANG
