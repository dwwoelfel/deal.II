<br>

<i>This program was contributed by Martin Kronbichler and Wolfgang
Bangerth.
<br>
This material is based upon work partly supported by the National
Science Foundation under Award No. EAR-0426271 and The California Institute of
Technology. Any opinions, findings, and conclusions or recommendations
expressed in this publication are those of the author and do not
necessarily reflect the views of the National Science Foundation or of The
California Institute of Technology.
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

This program does pretty much exactly what @ref step_31 "step-31" already
does: it solves the Boussinesq equations that describe the motion of a fluid
whose temperature is not in equilibrium. As such, all the equations we have
described in @ref step_31 "step-31" still hold: we solve the same partial
differential equation, using the same finite element scheme, the same time
stepping algorithm, and the same stabilization method for the temperature
advection-diffusion equation. As a consequence, you may first want to
understand that program before you work on the current one.

The difference between @ref step_31 "step-31" and the current program is that
here we want to do things in %parallel, using both the availability of many
machines in a cluster (with parallelization based on MPI) as well as many
processor cores within a single machine (with parallelization based on
threads). This program's main job is therefore to introduce the changes that are
necessary to utilize the availability of these %parallel compute resources.

In addition to these changes, we also use a slightly different preconditioner,
which we will discuss first.


<h3> Changes to the Stokes preconditioner </h3>

In this tutorial program, we apply a variant of the preconditioner used in
@ref step_31 "step-31". That preconditioner was built to operate on the
system matrix <i>M</i> in block form such that the product matrix
@f{eqnarray*}
  P^{-1} M
  =
  \left(\begin{array}{cc}
    A^{-1} & 0 \\ S^{-1} B A^{-1} & -S^{-1}
  \end{array}\right)
  \left(\begin{array}{cc}
    A & B^T \\ B & 0
  \end{array}\right)
@f}
is of a form that Krylov-based iterative solvers like GMRES can solve in a
few iterations. We then replaced the exact inverse of <i>A</i> by the action
of an AMG preconditioner $\tilde{A}$ based on a vector Laplace matrix,
approximated the Schur complement $S = B A^{-1} B^T$ by a mass matrix $M_p$
on the pressure space and wrote an <tt>InverseMatrix</tt> class for
implementing the action of $M_p^{-1}\approx S^{-1}$ on vectors. In the
InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC)
preconditioner for performing the inner solves.

An observation one can make is that we use just the action of a
preconditioner for approximating the velocity inverse $A^{-1}$ (and the
outer GMRES iteration takes care of the approximate character of the
inverse), whereas we use a more or less <i>exact</i> inverse for $M_p^{-1}$,
realized by a fully converged CG solve. What we change here is to skip that
<i>exact</i> inverse matrix and replace it &ndash; as usual &ndash; by the
action of a preconditioner only. This works, as we will demonstrate
below. For efficiency reasons, we want to avoid increasing the number of
iterations for the block solve. Keep in mind that most of the time in the
solution of the matrix system is the application of the AMG preconditioner
(about half the time of the total solve), and the application of matrix
<i>A</i> (about one third of the total solve time). This means that we
really do not want to do those operations more often when we remove the
inner solve on the Schur complement approximation. It turns out that the
Trilinos IC preconditioner would not fulfill this requirement, however, the
Trilinos ILU preconditioner does. It does even better than so &mdash; it
decreases the iteration count for large 3D problems. The reason for that
decrease is that we avoid some errors that CG introduces: Even a converged
solve has still some residual. That is a problem because that small error
interferes with the outer iterative solver, probably because a CG solver
does some nonlinear operations by weighting vectors by some inner products.

Except the simplification in the preconditioner, we replaced the GMRES
solver by BiCGStab. This is merely to demonstrate that GMRES is not the only
possible option for a saddle point system like the Stokes
equations. BiCGStab harmonizes nicely with the ILU preconditioner on a
pressure mass matrix as approximation for $S^{-1}$, so it is at least as
good as GMRES in this example. Keep in mind the discussion in the results
section of the @ref step_22 "step-22" tutorial program, where we observed
that BiCGStab does <i>not</i> like inner solves with CG, which made us
prefer GMRES in step-31.

<h3> Parallelization on clusters </h3>

Parallelization of scientific codes across multiple machines in a cluster of
computers is almost always done using the Message Passing Interface
(MPI). This program is no exception to that, and it follows the
@ref step_17 "step-17" and @ref step_18 "step-18" programs in this.

MPI is a rather awkward interface to program with, and so we usually try to
not use it directly but through an interface layer that abstracts most of the
MPI operations into a friendlier interface. In the two programs mentioned
above, this was achieved by using the PETSc library that provides support for
%parallel linear algebra in a way that almost completely hides the MPI layer
under it. PETSc is powerful, providing a large number of functions that deal
with matrices, vectors, and iterative solvers and preconditioners, along with
lots of other stuff, most of which runs quite well in %parallel. It is,
however, a few years old already, written in C, and generally not quite as
easy to use as some other libraries. As a consequence, deal.II also has
interfaces to Trilinos, a library similar to PETSc in its aims and with a lot
of the same functionality. It is, however, a project that is several years
younger, is written in C++ and by people who generally have put a significant
emphasis on software design. We have already used Trilinos in
@ref step_31 "step-31", and will do so again here, with the difference that we
will use its %parallel capabilities.

deal.II's Trilinos interfaces encapsulate pretty much everything Trilinos
provides into wrapper classes (in namespace TrilinosWrappers) that make the
Trilinos matrix, vector, solver and preconditioner classes look very much the
same as deal.II's own implementations of this functionality. However, as
opposed to deal.II's classes, they can be used in %parallel if we give them the
necessary information. As a consequence, there are two Trilinos classes that
we have to deal with directly (rather than through wrappers), both of which
are part of Trilinos' Epetra library of basic linear algebra and tool classes:
<ul>
<li> The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.
  it describes how many and which machines can communicate with each other.
  Each distributed object, such as a sparse matrix or a vector for which we
  may want to store parts on different machines, needs to have a communicator
  object to know how many parts there are, where they can be found, and how
  they can be accessed.

  In this program, we only really use one communicator object -- based on the
  MPI variable <code>MPI_COMM_WORLD</code> -- that encompasses <i>all</i>
  processes that work together. It would be perfectly legitimate to start a
  process on $N$ machines but only store vectors on a subset of these by
  producing a communicator object that only encompasses this subset of
  machines; there is really no compelling reason to do so here, however. As a
  second note, while we use <code>MPI_COMM_WORLD</code> as communicator in the
  program's source code, every time we create a Trilinos object in the wrapper
  classes in namespace TrilinosWrappers, we don't use the given communicator
  but instead create a new and unique communicator that happens to have the
  same machines but has a distinct communicator ID. This way, we can ensure
  that all communications that have to do with this, say, sparse matrix really
  only occur on a channel associated with only this one object, while all
  other objects communicate on other channels of their own. This helps in
  debugging, and may also allow some communications to be reordered for better
  %parallel performance because they can be told apart by their communicator
  number, not just their relative timing.

<li> The Epetra_Map class is used to describe which elements of a vector or which
  rows of a matrix reside on a given machine that is part of a
  communicator. To create such an object, you need to know (i) the total
  number of elements or rows, (ii) the number of elements or rows you want to
  store on the current machine, and (iii) which communicator enumerates the
  machines that we want this matrix or vector be stored on. We will set up
  these maps (which we call <code>partitioners</code> in our code, since we
  believe this is a better word) in the
  <code>BoussinesqFlowProblem::setup_dofs</code> function below and then hand
  it to every %parallel object we create.
</ul>

The only other things specific to programming using MPI that we will use in
this program are the following facilities deal.II provides:
<ul>
<li> We need to subdivide the domain into subdomains, each of which will
  represent the cells that one of the processors coupled by MPI shall consider
  its own and work on. This is done using the
  GridTools::partition_triangulation function.
<li> In order to know which elements of a vector or rows of a matrix shall be
  stored on each of the processors, we need to know how many degrees of
  freedom each of the owners of certain subdomains call their own. This is
  conveniently returned by the DoFTools::count_dofs_with_subdomain_association
  function.
</ul>
The rest of the program is almost completely agnostic about the fact that we
don't store all objects completely locally. There will be a few points where
we can not use certain programming techniques (though without making explicit
reference to MPI or parallelization) or where we need access to <i>all</i>
elements of a vector and therefore need to <i>localize</i> its elements
(i.e. create a vector that has all its elements stored on the current
machine), but we will comment on these locations as we get to them in the
program code.


<h3> Parallelization within individual nodes of a cluster </h3>

The second strategy to parallelize a program is to make use of the fact that
most computers today have more than one processor that all have access to the
same memory. In other words, in this model, we don't explicitly have to say
which pieces of data reside where -- all of the data we need is directly
accessible and all we have to do is split <i>processing</i> this data between
the available processors. We will then couple this with the MPI
parallelization outlined above, i.e. we will have all the processors on a
machine work together to, for example, assemble the local contributions to the
global matrix for the cells that this machine actually "owns" but not for
those cells that are owned by other machines. We will use this strategy for
four kinds of operations we frequently do in this program: assembly of the
Stokes and temperature matrices, assembly of the matrix that forms the Stokes
preconditioner, and assembly of the right hand side of the temperature system.

All of these operations essentially look as follows: we need to loop over all
cells for which <code>cell-@>subdomain_id()</code> equals the index our
machine has within the communicator object used for all communication
(i.e. essentially <code>MPI_COMM_WORLD</code>, as explained above), on each
cell we need to assemble the local contributions to the global matrix or
vector, and then we have to copy each cell's contribution into the global
matrix or vector. Note that the first part of this (the loop) defines a range
of iterators on which something has to happen. The second part, assembly of
local contributions is something that takes the majority of CPU time in this
sequence of steps, is a typical example of things that can be done in
%parallel: each cell's contribution is entirely independent of all other cells'
contributions. The third part, copying into the global matrix, must not happen
in %parallel since we are modifying one object and so several threads can not
at the same time read an existing matrix element, add their contribution, and
write the sum back into memory without danger of producing a <a
href="http://en.wikipedia.org/wiki/Race_condition">race condition</a>.

deal.II has a class that is made for exactly this workflow: WorkStream. Its
use is extensively documented in the module on @ref threads (in the section
on @ref MTWorkStream "the WorkStream class") and we won't repeat here the
rationale and detailed instructions laid out there, though you will want to
read through this module to understand the distinction between scratch space
and per-cell data. Suffice it to mention that we need the following:

- An iterator range for those cells on which we are supposed to work. This is
  provided by the FilteredIterator class which acts just like every other cell
  iterator in deal.II with the exception that it skips all cells that do not
  satisfy a particular predicate (i.e. a criterion that evaluates to true or
  false). In our case, the predicate is whether a cell has the correct
  subdomain id.

- A function that does the work on each cell for each of the tasks identified
  above, i.e. functions that assemble the local contributions to Stokes matrix
  and preconditioner, temperature matrix, and temperature right hand
  side. These are the
  <code>BoussinesqFlowProblem::local_assemble_stokes_system</code>,
  <code>BoussinesqFlowProblem::local_assemble_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::local_assemble_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::local_assemble_temperature_rhs</code> functions in
  the code below. These four functions can all have several instances of each
  running in %parallel.

- %Functions that copy the result of the previous ones into the global object
  and that run sequentially to avoid race conditions. These are the
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_system</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_stokes_preconditioner</code>,
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_matrix</code>, and
  <code>BoussinesqFlowProblem::copy_local_to_global_temperature_rhs</code>
  functions.

We will comment on a few more points in the actual code, but in general
their structure should be clear from the discussion in @ref threads.

The underlying technology for WorkStream identifies "tasks" that need to be
worked on (e.g. assembling local contributions on a cell) and schedules
these tasks automatically to available processors. WorkStream creates these
tasks automatically, by splitting the iterator range into suitable chunks,
but as outlined in @ref threads, one can also create tasks explicitly. We
use this in one place in the program, namely where we set up the Stokes
system and preconditioner matrices as well as the temperature matrix. These
are independent operations that, if enough processors are available, can be
worked on in parallel (if not enough processors are available -- because the
system has only one, or because the others are working on something else for
us -- then these tasks will be worked on sequentially). Consequently,
<code>BoussinesqFlowProblem::setup_dofs</code> creates tasks for the three
calls to <code>BoussinesqFlowProblem::setup_stokes_matrix</code>,
<code>BoussinesqFlowProblem::setup_stokes_preconditioner</code>, and
<code>BoussinesqFlowProblem::setup_temperature_matrices</code> that are then
scheduled to available resources. There is one problem with this, however:
if we have more than one MPI process running in parallel, then all of these
processes need to communicate in a certain order and that requires that the
various <code>setup_*</code> functions can't run in parallel. To make
things worse, even if there is only a single MPI process, we still
have to make a few calls to the MPI runtime (for example to set up
communicators for the various matrices and vectors; all of these
communicators contain only a single processor, but we still have to
call MPI functions for this). Unfortunately, most MPI implementations
are not thread-safe, and we can't call MPI functions from multiple
threads at once. For these setup operations, we will therefore only be
able to make use of multiple processor cores within the same machine
if we are not running under the control of MPI -- a condition we can
check using the Utilities::System::job_supports_mpi() function.


<h3> Implementation details </h3>

One detail worth discussing before we show the actual implementation
is how to deal with distributed vs. localized vectors. When we build
the linear systems, the various matrices and right hand side vectors
are always built distributed, i.e. each processor builds contributions
for a subset of cells and adds them to the global matrix objects;
Trilinos then makes sure that the right entries end up on the right
processors so that each processor stores a part of these distributed
objects.

When we then come around to solving linear systems, the solution
vectors will also be distributed. On the other hand, in later steps,
processors will need to be able to access random elements for various
tasks. For example, processor zero needs to have access to all
elements in order to produce output; all processors need to have
access to temperature and Stokes solution data on the cells they own
to build right hand side vectors for the next time step; and all
processors need to access data on the cells they own as well as their
neighbors (whether these neighbors are owned by the same process or
another one) in order to compute the jump of the gradient across cell
interfaces in the Kelly error estimator when computing refinement
information.

In other words, for some operations we will have to exchange
information between processors. We could try to be smart and really
only exchange that data that we really need. This would probably be
important if we were to run this program on thousands of processors,
as then storing <i>all</i> elements of a solution vector may become
impractical. On the other hand, deal.II currently has a number of
other bottlenecks that make sure that thousands of processors is not
possible anyway. Consequently, here we opt for the simplest choice:
solve linear systems using distributed vectors, and immediately
afterwards each processor requests a localized copy of the solution
vector containing all elements that we will use henceforth for all
operations. The distributed vector is no longer necessary at this
point and is, in fact, deallocated again immediately.


<h3> The testcase </h3>

The setup for this program is mildly reminiscent of the problem we wanted to
solve in the first place (see the introduction of @ref step_31 "step-31"):
convection in the earth mantle. As a consequence, we choose the following
data, all of which appears in the program in units of meters and seconds (the
SI system) even if we list them here in other units.

As a reminder, the equations we want to solve are these:
@f{eqnarray*}
  -\nabla \cdot (2 \eta \varepsilon ({\mathbf u})) + \nabla p &=& 
  -\rho \; \beta \; T \mathbf{g},
  \\
  \nabla \cdot {\mathbf u} &=& 0,
  \\
  \frac{\partial T}{\partial t}
  +
  {\mathbf u} \cdot \nabla T
  -
  \nabla \cdot \kappa \nabla T &=& \gamma,
@f}
augmented by boundary and initial conditions. We then have to choose data for
the following quantities:
<ul>
  <li>The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner
  and outer radii that match that of the earth: the total radius of the earth
  is 6371km, with the mantle starting at a depth of around 35km (just under
  the solid earth <a target="_top"
  href="http://en.wikipedia.org/wiki/Crust_(geology)">crust</a> composed of
  <a target="_top"
  href="http://en.wikipedia.org/wiki/Continental_crust">continental</a> and <a
  target="_top" href="http://en.wikipedia.org/wiki/Oceanic_crust">oceanic
  plates</a>) to a depth of 2890km (where the 
  <a target="_top" href="http://en.wikipedia.org/wiki/Outer_core">outer earth
  core</a> starts). The radii are therefore $R_0=(6371-2890)\text{km},
  R_1=(6371-35)\text{km}$. This domain is conveniently generated using the
  GridGenerator::hyper_shell() function, and we use a HyperShellBoundary
  objects for the inner and outer boundary.

  <li>At the interface between crust and mantle, the temperature is between
  500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees
  Celsius (see, for example, <a target="_top"
  href="http://en.wikipedia.org/wiki/Mantle_(geology)">this Wikipedia
  entry</a>). In Kelvin, we therefore choose $T_0=(4000+273)\text{K},
  T_1=(500+273)\text{K}$ as boundary conditions at the inner and outer edge.

  In addition to this, we also have to specifiy some initial conditions for
  the temperature field. The real temperature field of the earth is quite
  complicated as a consequence of the convection that has been going on for
  more than four billion years -- in fact, it is the properties of this
  temperature distribution that we want to explore with programs like this
  (or, to be honest, more sophisticated versions of it). As a consequence, we
  don't really have anything useful to offer here, but we can hope that if we
  start with something and let things run for a while that the exact initial
  conditions don't matter that much any more. The initial temperature field we
  use here is given in terms of the radius by
  @f{align*}
    \rho &= \frac{r-R_0}{R_1-R_0}, \\
    T(r) &= T_0(1-\rho)^2 + T_1[1-(1-\rho)^2].
  @f}
  This profile is quadratic and matches the boundary conditions at the inner
  and outer radii.

  <li>The right hand side of the temperature equation contains the rate of
  internal heating $\gamma$. The earth does heat naturally through three mechanisms:
  radioactive decay, chemical separation (heavier elements sink to the bottom,
  lighter ones rise to the top; the countercurrents dissipate emergy equal to
  the loss of potential energy by this separation process), and heat release
  by crystallization of liquid metal as the solid inner core of the earth
  grows. None of these processes are overly significant the earth mantle, and
  so we assume that the internal heating can be set to zero.

  <li>For the velocity we choose as boundary conditions $\mathbf{v}=0$ at the
  inner radius (i.e. the fluid sticks to the earth core) and
  $\mathbf{n}\cdot\mathbf{v}=0$ at the outer radius (i.e. the fluid flows
  tangentially along the bottom of the earth crust). Neither of these is
  physically overly correct: certainly, on both boundaries, fluids can flow
  tangentially, but they will incur a shear stress through friction against
  the medium at the other side of the interface (the metallic core and the
  crust, respectively). Such a situation could be modeled by a Robin-type
  boundary condition. To make things worse, the medium on the other side is
  in motion as well, so the shear stress would, in the simplest case, be
  proportional to the <i>velocity difference</i>, leading to a boundary
  condition of the form
  @f{align*}
    \mathbf{n}\cdot [2\eta \varepsilon(\mathbf v)] 
    &= 
    s \mathbf{n} \times [\mathbf v - \mathbf v_0],
    \\
    \mathbf{n} \cdot \mathbf v &= 0,
  @f}
  with a proportionality constant $s$. Rather than going down this route,
  however, we rather stick with the choice of zero (stick) and tangential
  flow boundary conditions.

  As a side note of interest, we may also have chosen tangential flow
  conditions on both inner and outer boundary. That has a significant
  drawback, however: it leaves the velocity not uniquely defined. The reason
  is that all velocity fields $\hat{\mathbf v}$ that correspond to a solid
  body rotation around the center of the domain satisfy $\mathrm{div}
  \varepsilon(\hat{\mathbf v})=0, \mathrm{div} \hat{\mathbf v} = 0$, and
  $\mathbf{n} \cdot \hat{\mathbf v} = 0$. As a consequence, if $\mathbf v$
  satisfies equations and boundary conditions, then so does $\mathbf v +
  \hat{\mathbf v}$. That's certainly not a good situation that we would like
  to avoid. The traditional way to work around this is to pick an arbitrary
  point on the boundary and call this your fixed point by choosing the
  velocity to be zero in all components there. 

  <li>To first order, the gravity vector always points downward. The question for
  a body as big as the earth is just: where is "up". The answer of course is
  "radially inward, towards the center of the earth". So at the surface of the
  earth, we have
  @f[
    \mathbf g 
    = 
    -9.81 \frac{\text{m}}{\text{s}^2} \frac{\mathbf x}{\|\mathbf x\|},
  @f]
  where $9.81 \frac{\text{m}}{\text{s}^2}$ happens to be the average gravity
  acceleration at the earth surface. But in the earth interior, the question
  becomes a bit more complicated: at the center of the earth, for example, you
  have matter pulling equally hard in all directions, and so $\mathbf g=0$. In
  between, the net force is described as follows: let us define the 
  <a target="_top"
  href="http://en.wikipedia.org/wiki/Potential_energy#Gravitational_potential_energy">gravity
  potential</a> by
  @f[
    \varphi(\mathbf x)
    =
    \int_{\text{earth}} 
    -G \frac{\rho(\mathbf y)}{\|\mathbf x-\mathbf y\|} 
    \ \text{d}y,
  @f]
  then $\mathbf g(\mathbf x) = -\nabla \varphi(\mathbf x)$. If we assume that
  the density $\rho$ is constant throughout the earth, we can produce an
  analytical expression for the gravity vector (don't try to integrate above
  equation somehow -- it leads to elliptic integrals; the simpler way is to
  notice that $-\Delta\varphi(\mathbf x) = 4\pi G \rho
  \chi_{\text{earth}}(\mathbf x)$ and solving this
  partial differential equation in all of ${\mathbb R}^3$ exploiting the
  radial symmetry):
  @f[
    \mathbf g(\mathbf x) = 
    \left\{
      \begin{array}{ll}
        -\frac{4}{3}\pi G \rho \|\mathbf x\| \frac{\mathbf x}{\|\mathbf x\|}
	& \text{for} \ \|\mathbf x\|<R_1, \\
        -\frac{4}{3}\pi G \rho R^3 \frac{1}{\|\mathbf x\|^2} 
        \frac{\mathbf x}{\|\mathbf x\|}
	& \text{for} \ \|\mathbf x\|\ge R_1
      \end{array}
    \right.
  @f]
  The factor $-\frac{\mathbf x}{\|\mathbf x\|}$ is the unit vector pointing
  radially inward. Of course, within this problem, we are only interested in
  the branch that pertains to within the earth, i.e. $\|\mathbf x\|<R_1$. In
  the program, we therefore only consider the expression
  @f[
    \mathbf g(\mathbf x) = 
        -\frac{4}{3}\pi G \rho \|\mathbf x\| \frac{\mathbf x}{\|\mathbf x\|}
        =
        -\frac{4}{3}\pi G \rho \mathbf x
	=
	- 9.81 \frac{\mathbf x}{R_1} \frac{\text{m}}{\text{s}^2},
  @f]
  where we can infer the last expression because we know Earth's gravity at
  the surface (where $\|x\|=R_1$).

  There are two problems with this, however: (i) The Earth is not homogenous,
  i.e. the density $\rho$ depends on $\mathbf x$; in fact it is not even a
  function that only depends on the radius $r$. In reality, gravity therefore
  does not always decrease as we get deeper: because the earth core is so much
  denser than the mantle, gravity actually peaks at around $10.7
  \frac{\text{m}}{\text{s}^2}$ at the core mantle boundary (see <a
  target="_top" href="http://en.wikipedia.org/wiki/Earth's_gravity">this
  article</a>). (ii) The density, and by
  consequence the gravity vector, is not even constant in time: after all, the
  problem we want to solve is the time dependent upwelling of hot, less dense
  material and the downwelling of cold dense material. This leads to a gravity
  vector that varies with space and time, and does not always point straight
  down.

  In order to not make the situation more complicated than necessary, we'll
  here just go with the constant density model above.

  <li>The density of the earth mantle varies spatially, but not by very
  much. $\rho=3300 \frac{\text{kg}}{\text{m}^3}$ is a relatively good average
  value.

  <li>The thermal expansion coefficient $\beta$ also varies with depth
  (through its dependence on temperature and pressure). Close to the surface,
  it appears to be on the order of $\beta=45\cdot 10^{-6} \frac 1{\text{K}}$,
  whereas at the core mantle boundary, it may be closer to $\beta=10\cdot
  10^{-6} \frac 1{\text{K}}$. As a reasonable value, let us choose 
  $\beta=2\cdot 10^{-5} \frac 1{\text{K}}$.

  <li>The second to last parameter we need to specify is the viscosity
  $\eta$. This is a tough one, because rocks at the temperatures and pressure
  typical for the earth mantle flow so slowly that the viscosity can not be
  determined accurately in the laboratory. So how do we know about the
  viscosity of the mantle? The most commonly used route is to consider that
  during and after ice ages, ice shields form and disappear on time scales
  that are shorter than the time scale of flow in the mantle. As a
  consequence, continents slowly sink into the earth mantle under the added
  weight of an ice shield, and they rise up again slowly after the ice shield
  has disappeared again (this is called <a target="_top"
  href="http://en.wikipedia.org/wiki/Postglacial_rebound"><i>postglacial
  rebound</i></a>). By measuring the speed of this rebound, we can infer the
  viscosity of the material that flows into the area vacated under the
  rebounding continental plates.

  Using this technique, values around $\eta=10^{21} \text{Pa\; s} 
  = 10^{21} \frac{\text{N\; s}}{\text{m}^2}
  = 10^{21} \frac{\text{kg}}{\text{m\; s}}$ have been found as the most
  likely, though the error bar on this is at least one order of magnitude. 

  While we will use this value, we again have to caution that there are many
  physical reasons to assume that this is not the correct value. First, it
  should really be made dependent on temperature: hotter material is most
  likely to be less viscous than colder material. In reality, however, the
  situation is even more complex. Most rocks in the mantle undergo phase
  changes as temperature and pressure change: depending on temperature and
  pressure, different crystal configurations are thermodynamically favored
  over others, even if the chemical composition of the mantle were
  homogenous. For example, the common mantle material MgSiO<sub>3</sub> exists
  in its <a target="_top"
  href="http://en.wikipedia.org/wiki/Perovskite_(structure)">perovskite
  structure</a> throughout most of the mantle, but in the lower mantle the
  same substance is stable only as <a targe="_top"
  href="http://en.wikipedia.org/wiki/Postperovskite">post-perovskite</a>. Clearly,
  to compute realistic viscosities, we would not only need to know the exact
  chemical composition of the mantle and the viscosities of all materials, but
  we would also have to compute the thermodynamically most stable
  configurations for all materials at each quadrature point. This is at the
  time of writing this program not a feasible suggestion.

  <li>Our last material parameter is the thermal diffusivity $\kappa$, which
  is defined as $\kappa=\frac{k}{\rho c_p}$ where $k$ is the thermal
  conductivity, $\rho$ the density, and $c_p$ the specific heat. For
  this, the literature indicates that it increases from around $0.7$ in the
  upper mantle to around $1.7 \frac{\text{mm}^2}{\text{s}}$ in the lower
  mantle, though the exact value 
  is not really all that important: heat transport through convection is
  several orders of magnitude more important than through thermal
  conduction. It may be of interest to know that perovskite, the most abundant
  material in the earth mantle, appears to become transparent at pressures
  above around 120 GPa (see, for example, J. Badro et al., Science 305,
  383-386 (2004)); in the lower mantle, it may therefore be that heat
  transport through radiative transfer is more efficient than through thermal
  conduction.

  In view of these considerations, let us choose 
  $\kappa=1 \frac{\text{mm}^2}{\text{s}} =10^{-6} \frac{\text{m}^2}{\text{s}}$
  for the purpose of this program.
</ul>

All of these pieces of equation data are defined in the program in the
<code>EquationData</code> namespace.
