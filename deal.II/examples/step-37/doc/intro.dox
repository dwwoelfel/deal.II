<br>

<i>
This program was contributed by Katharina Kormann and Martin
Kronbichler.

This program is currently under construction.

The algorithm for the matrix-vector product is built upon the report &quot;MPI
parallelization of a cell-based matrix-vector product for finite elements. An
application from quantum dynamics&quot; by Katharina Kormann, Uppsala
University, June 2009.  
</i>


<a name="Intro"></a>
<h1>Introduction</h1>

This example shows how to implement a matrix-free method, that is, a method
that does not explicitly store the matrix elements, for a
second-order Poisson equation with variable coefficients on a fairly 
unstructured mesh representing a circle.

<h3>Matrix-vector product implementation</h3>

In order to find out how we can write a code that performs a matrix-vector
product, but does not need to store the matrix elements, let us start at
looking how some finite-element related matrix <i>A</i> is assembled:
@f{eqnarray*}
A = \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T A_\mathrm{cell}
P_\mathrm{cell,{loc-glob}}.
@f}
In this formula, the matrix <i>P</i><sub>cell,loc-glob</sub> is a rectangular
matrix that defines the index mapping from local degrees of freedom in the
current cell to the global degrees of freedom. The information from which this
operator can be built is usually encoded in the <code>local_dof_indices</code>
variable we have always used in the assembly of matrices.

If we are to perform a matrix-vector product, we can hence use that
@f{eqnarray*}
y &=& A\cdot x = \left(\sum_{\text{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} P_\mathrm{cell,{loc-glob}}\right) \cdot x
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} x_\mathrm{cell}
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
y_\mathrm{cell},
@f}
where <i>x</i><sub>cell</sub> are the values of <i>x</i> at the degrees of freedom
of the respective cell, and  <i>x</i><sub>cell</sub> correspondingly for the result.
A naive attempt to implement the local action of the Laplacian would hence be
to use the following code:
@code
MatrixFree::vmult (Vector<double>       &dst,
		   const Vector<double> &src) const
{
  dst = 0;

  QGauss<dim>  quadrature_formula(fe.degree+1);
  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_gradients | update_JxW_values);

  const unsigned int   dofs_per_cell = fe.dofs_per_cell;
  const unsigned int   n_q_points    = quadrature_formula.size();

  FullMatrix<double>   cell_matrix (dofs_per_cell, dofs_per_cell);
  Vector<double>       cell_src (dofs_per_cell),
   		       cell_dst (dofs_per_cell);

  std::vector<unsigned int> local_dof_indices (dofs_per_cell);

  typename DoFHandler<dim>::active_cell_iterator
    cell = dof_handler.begin_active(),
    endc = dof_handler.end();
  for (; cell!=endc; ++cell)
    {
      cell_matrix = 0;
      fe_values.reinit (cell);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int i=0; i<dofs_per_cell; ++i)
	  for (unsigned int j=0; j<dofs_per_cell; ++j)
            cell_matrix(i,j) += (fe_values.shape_grad(i,q_point) *
                                 fe_values.shape_grad(j,q_point) *
                                 fe_values.JxW(q_point));

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      cell_matrix.vmult (cell_dst, cell_src);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst;
    }
}
@endcode

Here we neglected boundary conditions as well as any hanging nodes we may
have, though neither would be very difficult to include using the
ConstraintMatrix class. Note how we first generate the local matrix in the
usual way. To form the actual product as expressed in the above formula, we
read in the values of <code>src</code> of the cell-related degrees of freedom
(the action of <i>P</i><sub>cell,loc-glob</sub>), multiply by the local matrix
(the action of <i>A</i><sub>cell</sub>), and finally add the result to the
destination vector <code>dst</code> (the action of
<i>P</i><sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). It
is not more difficult than that, in principle.

While this code is completely correct, it is very slow. For every cell, we
generate a local matrix, which takes three nested loops with as many
elements as there are degrees of freedom on the actual cell to compute. The
multiplication itself is then done by two nested loops, which means that it
is much cheaper.

One way to improve this is to realize that conceptually the local
matrix can be thought of as the product of three matrices,
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D_\mathrm{cell} B_\mathrm{cell},
@f}
where for the example of the Laplace operator the (<i>q</i>*dim+<i>d,i</i>)-th
element of <i>B</i><sub>cell</sub> is given by
<code>fe_values.shape_grad(i,q)[d]</code>. The matrix consists of
<code>dim*n_q_points</code> rows and @p dofs_per_cell columns). The matrix
<i>D</i><sub>cell</sub> is diagonal and contains the values
<code>fe_values.JxW(q)</code> (or, rather, @p dim copies of it).

Every numerical analyst learns in one of her first classes that for
forming a product of the form
@f{eqnarray*}
A_\mathrm{cell}\cdot x_\mathrm{cell} = B_\mathrm{cell} D_\mathrm{cell} 
		     B_\mathrm{cell}^T \cdot x_\mathrm{cell},
@f}
one should never form the matrix-matrix products, but rather multiply with the
vector from right to left so that only three successive matrix-vector products
are formed. To put this into code, we can write:
@code
...
  for (; cell!=endc; ++cell)
    {
      fe_values.reinit (cell);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      temp_vector = 0;
      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
          for (unsigned int i=0; i<dofs_per_cell; ++i)
	    temp_vector(q_point*dim+d) += fe_values.shape_grad(i,q_point)[d] *
	    			          cell_src(i);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
	  temp_vector(q_point*dim+d) *= fe_values.JxW(q_point);

      cell_dst = 0;
      for (unsigned int i=0; i<dofs_per_cell; ++i)
        for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
          for (unsigned int d=0; d<dim; ++d)
	    cell_dst(i) += fe_values.shape_grad(i,q_point)[d] *
	         	   temp_vector(q_point*dim+d);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst(i);
    }
}
@endcode

This removed the three nested loops in the calculation of the local matrix
(here the loop over <i>d</i> is a not really a loop, rather two or three
operations). What happens is as follows: We first transform the vector of
values on the local dofs to a vector of gradients on the quadrature
points. In the second loop, we multiply these gradients by the integration
weight. The third loop applies the second gradient (in transposed form), so
that we get back to a vector of (Laplacian) values on the cell dofs.

This improves the situation a lot and reduced the complexity of the product
from something like $\mathcal {O}(\mathrm{dofs\_per\_cell}^3)$ to $\mathcal
{O}(\mathrm{dofs\_per\_cell}^2)$. In fact, all the remainder is just to make
a slightly more clever use of data in order to gain some extra speed. It does
not change the code structure, though.

The bottleneck in the above code is the operations done by the call
<code>fe_values.reinit(cell)</code>, which take about as much time as the
other steps together (at least if the mesh is unstructured; deal.II can
recognize that the gradients are often unchanged on structured meshes). That
is certainly not ideal and we would like to do better than this. What the
reinit function does is to calculate the gradient in real space by
transforming the gradient on the reference cell using the Jacobian of the
transformation from real to reference cell. This is done for each basis
function on the cell, for each quadrature point. The Jacobian does not depend on
the basis function, but it is different on different quadrature points in
general. The trick is now to factor out the Jacobian transformation and first
apply the operation that leads us to <code>temp_vector</code> only with the
gradient on the reference cell. That transforms the vector of values on the
local dofs to a vector of gradients on the quadrature points. There, we first
apply the Jacobian that we factored out from the gradient, then we apply the
weights of the quadrature, and we apply with the transposed Jacobian for
preparing the third loop which again uses the gradients on the unit cell.

Let us again write this in terms of matrices. Let the matrix
<i>B</i><sub>cell</sub> denote the cell-related gradient matrix, with each row
containing the values of the quadrature points. It is constructed by a
matrix-matrix product as
@f{eqnarray*}
B_\mathrm{cell} = J_\mathrm{cell} B_\mathrm{ref\_cell},
@f}
where <i>B</i><sub>ref_cell</sub> denotes the gradient on the reference cell
and <i>J</i><sub>cell</sub> denotes the Jacobian
transformation. <i>J</i><sub>cell</sub> is block-diagonal, and the blocks size
is equal to the dimension of the problem. Each diagonal block is the Jacobian
transformation that goes from the reference cell to the real cell.

Putting things together, we find that
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D B_\mathrm{cell} 
		= B_\mathrm{ref\_cell}^T J_\mathrm{cell}^T 
		  D_\mathrm{cell} 
		  J_\mathrm{cell} B_\mathrm{ref\_cell},
@f}
so we calculate the product (starting the local product from the right)
@f{eqnarray*}
y_\mathrm{cell} = B_\mathrm{ref\_cell}^T J_\mathrm{cell}^T D J_\mathrm{cell} 
B_\mathrm{ref\_cell} x_\mathrm{cell}, \quad
y = \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
y_\mathrm{cell}.
@f}
@code
...
  FEValues<dim> fe_values_reference (fe, quadrature_formula,
                           	     update_gradients);
  Triangulation<dim> reference_cell;
  GridGenerator::hyper_cube(reference_cell, 0., 1.);
  fe_values_reference.reinit (reference_cell.begin());

  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_inverse_jacobians | update_JxW_values);

  for (; cell!=endc; ++cell)
    {
      fe_values.reinit (cell);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      temp_vector = 0;
      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
          for (unsigned int i=0; i<dofs_per_cell; ++i)
	    temp_vector(q_point*dim+d) +=
	      fe_values_reference.shape_grad(i,q_point)[d] * cell_src(i);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        {
          // apply the Jacobian of the mapping from unit to real cell
	  Tensor<1,dim> temp;
	  for (unsigned int d=0; d<dim; ++d)
	    temp[d] = temp_vector(q_point*dim+d);
	  for (unsigned int d=0; d<dim; ++d)
	    {
	      double sum = 0;
	      for (unsigned int e=0; e<dim; ++e)
	      	sum += fe_values.inverse_jacobian(q_point)[d][e] *
	       	       temp[e];
	      temp_vector(q_point*dim+d) = sum;
	    }

          // multiply with integration weight
	  for (unsigned int d=0; d<dim; ++d)
	    temp_vector(q_point*dim+d) *= fe_values.JxW(q_point);

          // apply the transpose of the Jacobian of the mapping from unit
	  // to real cell
	  for (unsigned int d=0; d<dim; ++d)
	    temp[d] = temp_vector(q_point*dim+d);
	  for (unsigned int d=0; d<dim; ++d)
	    {
	      double sum = 0;
	      for (unsigned int e=0; e<dim; ++e)
	      	sum += fe_values.inverse_jacobian(q_point)[e][d] *
	    	       temp[e];
	      temp_vector(q_point*dim+d) = sum;
	    }
        }

      cell_dst = 0;
      for (unsigned int i=0; i<dofs_per_cell; ++i)
        for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
          for (unsigned int d=0; d<dim; ++d)
	    cell_dst(i) += fe_values_reference.shape_grad(i,q_point)[d] *
	               	   temp_vector(q_point*dim+d);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst(i);
    }
}
@endcode

Note how we create an additional FEValues object for the reference cell
gradients and how we initialize it to the reference cell. The actual
derivative data is then applied by the Jacobians (deal.II calls the Jacobian
matrix from unit to real cell inverse_jacobian, because the transformation
direction in deal.II is from real to unit cell).

To sum up, we want to look at the additional costs introduced by this
written-out matrix-matrix-vector product compared to a sparse matrix. To first
approximation, we have increased the number of operations for the local
matrix-vector product by a factor of 4 in 2D and 6 in 3D, i.e., two
matrix-vector products with matrices that have <code>dim</code> as many
columns than before (here we assume that the number of quadrature points is
the same as the number of degrees of freedom per cell, which is usual for
scalar problems). Then, we also need to keep in mind that we touch some
degrees of freedom several times because they belong to several cells. This
also increases computational costs. A realistic value compared to a sparse
matrix is that we now have to perform about 10 times as many operations (a bit
less in 2D, a bit more in 3D).

The above is, in essence, what happens in the code below and if you have
difficulties in understanding the implementation, you should try to first
understand what happens in the code above. In the actual implementation
there are a few more points done to be even more efficient, namely:
<ul>
  <li>We pre-compute the inverse of the Jacobian of the transformation and
      store it in an extra array. This allows us to fuse the three
      operations <i>J</i><sub>cell</sub><sup>T</sup><i> D</i><sub>cell</sub>
      <i>J</i><sub>cell</sub> (apply Jacobian, multiply by 
      weights, apply transposed
      Jacobian) into one second-rank tensor that is also symmetric (so we
      only need to store half the tensor).
  <li>We work on several cells at once when we apply the gradients of the
      unit cell (it is always the same matrix with the reference cell
      data). This allows us to replace the matrix-vector product by a
      matrix-matrix product (several vectors of cell-data form a matrix),
      which enables a faster implementation. Obviously, we need some adapted
      data structures for that, but it isn't too hard to provide that. What
      is nice is that dense matrix-matrix products are close to today's
      processors' peak performance if the matrices are neither too small nor
      too large &mdash; and these operations are the most expensive part in
      the implementation shown here. 
</ul>

The implementation of the matrix-free matrix-vector product shown in this
tutorial is slower than a matrix-vector product using a sparse matrix for
linear and quadratic elements, but on par with third order elements and faster
for even higher order elements. An additional gain with this implementation is
that we do not have to build the sparse matrix itself, which can also be quite
expensive depending on the underlying differential equation.


<h3>Combination with multigrid</h3>

Above, we have gone to significant lengths to implement a matrix-vector
product that does not actually store the matrix elements. In many user codes,
however, one wants more than just performing some uncertain number of
matrix-vector products &mdash; one wants to do as little of these operations
as possible when solving linear equation systems. In theory, we could use the
CG method without preconditioning; however, that would not be very
efficient. Rather, one uses preconditioners for improving speed. On the other
hand, most of the more frequently used preconditioners such as SSOR, ILU or
algebraic multigrid (AMG) can now no longer be used here because their
implementation requires knowledge of the elements of the system matrix.

One solution is to use multigrid methods as shown in
step-16. They are known to be very fast, and they are suitable for our
purpose since they can be designed based purely on matrix-vector products. All
one needs to do is to find a smoother that works with matrix-vector products
only (our choice requires knowledge of the diagonal entries of the matrix,
though). One such candidate would be a damped Jacobi iteration, but that is
often not sufficiently good in damping high-frequency errors.
A Chebyshev preconditioner, eventually, is what we use here. It can be
seen as an extension of the Jacobi method by using Chebyshev polynomials. With
degree zero, the Jacobi method with optimal damping parameter is retrieved,
whereas higher order corrections improve the smoothing properties if some
parameters are suitably chosen. The effectiveness of Chebyshev smoothing in
multigrid has been demonstrated, e.g., in the article <i>M. Adams, M. Brezina,
J. Hu, R. Tuminaro. Parallel multigrid smoothers: polynomial versus
Gauss&ndash;Seidel, J. Comput. Phys. 188:593&ndash;610, 2003</i>. This
publication also identifies one more advantage of Chebyshev smoothers that we
exploit here, namely that they are easy to parallelize, whereas
SOR/Gauss&ndash;Seidel smoothing relies on substitutions, which can often only
be parallelized by working on diagonal sub-blocks of the matrix, which
decreases efficiency.

The implementation into the multigrid framework is then straightforward. This
program is based on an earlier version of step-16 that demonstrated multigrid
on uniformly refined grids. However, the present matrix-free techniques would
obviously also apply to the adaptive meshes the current step-16 uses.


<h3>The test case</h3>

In order to demonstrate the capabilities of the method, we work on a rather
general Poisson problem, based on a more or less unstructured mesh (where
the Jacobians are different from cell to cell), higher order mappings to a
curved boundary, and a non-constant coefficient in the equation. If we
worked on a constant-coefficient case with structured mesh, we could
decrease the operation count by a factor of 4 in 2D and 6 in 3D by building
a local matrix (which is then the same for all cells), and doing the
products as in the first developing step of the above code pieces.
