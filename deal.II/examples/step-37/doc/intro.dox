<br>

<i>This program was contributed by Martin Kronbichler.</i>


<a name="Intro"></a>
<h1>Introduction</h1>

This example shows how to implement a matrix-free method, that is, a method
that does not explicitly store the matrix elements, for a
second-order Poisson equation with variable coefficients on a fairly 
unstructured mesh representing a circle.

<h3>Matrix-vector product implementation</h3>

<h4>Philosophical view on usual matrix-vector products</h4>

In most deal.II tutorial programs the code is built around assembling
some sparse matrix and solving a linear system of equations based on that
matrix. The run time of such programs is mostly spent in the construction of
the sparse matrix (assembling) and in performing some matrix-vector products
(possibly together with some substitutions like in SSOR preconditioners) in
an iterative Krylov method. This is a general concept in finite element
programs. Depending on the quality of the linear solver and the complexity
of the equation to be solved, between 40 and 95 per cent of the computational
time is spent in performing sparse matrix-vector products.

Let us briefly look at a simplified version of code for the matrix-vector
product when the matrix is stored in the usual sparse compressed row storage
&mdash; in short CRS &mdash; format implemented by the SparsityPattern and
SparseMatrix classes (the actual implementation in deal.II uses a slightly
different structure for the innermost loop, thereby avoiding the counter
variable):
@code
// y = A * x
std::size_t element_index = A_row_indices[0];
for (unsigned int row=0; row<n_rows; ++row)
  {
    const std::size_t row_ends = A_row_indices[row+1];
    double sum = 0;
    while (element_index != row_ends)
      {
        sum += A_values[element_index] *
               x[A_column_indices[element_index]];
        element_index++;
      }
    y[row] = sum;
  }
@endcode

Looking into what actually happens in the computer when forming a
matrix-vector product, one observes that the matrix data
<code>A_values</code> is continuously traveling from main memory into the
CPU in order to be multiplied with some vector element determined by the other
array <code>A_column_indices</code> and add this product to a sum that
eventually is written into the output vector. Let us assume for simplicity
that all the vector elements are sitting in the CPU (or some fast memory like
caches) and that we use a compressed storage scheme for the sparse matrix,
i.e., the format deal.II matrices (as well as PETSc and Trilinos matrices)
use. Then each matrix element corresponds to 12 bytes of data, 8 bytes for the
respective element in <code>A_values</code>, and 4 bytes for the unsigned
integer position <code>A_column_indices</code> that indicates which vector element
we actually use for multiplication. Here we neglect the additional array that
tells us the ranges of individual rows in the matrix. With those 12 bytes of
data, we perform two floating point operations, a multiplication and an
addition. If our matrix has one billion entries, a matrix-vector product
consists of two billion floating point operations, 2 GFLOP. One core of a
processor of 2009's standard (Intel's 'Nehalem' processor, 3 GHz) has a peak
performance of about 12 billion operations per second, 12 GFLOP/s. Now we
might be tempted to hope for getting a matrix-vector product done in about one
sixth of a second on such a machine. Remember, though, that we have to get 12
GB of data through the processor in order to form the matrix-vector
product. Looking again at which hardware is available in 2009, we will hardly
get more than 10 GB/s of data read. This means that the matrix-vector product
will take more than one second to complete, giving a rate of 1.7 GFLOP/s at
the best. This is quite far away from the theoretical peak performance of 12
GFLOP/s. In practice, the rate is often considerably lower because, for
example, the vectors do not fit into cache. A usual value on a 2009's machine
is 0.5 to 1.1 GFLOP/s.

What makes things worse is that today's processors have multiple cores, and
multiple cores have to compete for memory bandwidth. Imagine we have 8 cores
available with a theoretical peak performance of 96 GFLOP/s. However, these
cores sit on a machine with about 35 GB/s of memory bandwidth. For our
matrix-vector product, we would get a performance of about 6 GFLOP/s, which is
a nightmarish 6 per cent of the system's peak performance. And this is the
theoretical maximum we can get!

Things won't get better in the future, rather worse: Memory bandwidth will
most likely continue to grow more slowly than the number of cores (i.e., the
computing power). Consequently, we will probably not see that much of an
improvement in the speed of our programs even though computers do become
faster if we accumulate their speed over all of their cores.

In essence, one may ask how this can be avoided. The basic fact that
precipitates this is that matrices just consume a lot of memory &mdash;
sometimes too much, if it limits the number of computations that can
be done with them. A billion matrix entries might seem like an enormous
problem, but in fact, already a Laplace problem in 3D with cubic elements
and 6 million unknowns results in a matrix of that size, or even at 2
million unknowns for sixth order polynomial interpolation.

This tutorial shows an alternative that is less memory demanding. This comes
at the cost of more operations to be performed in an actual matrix-vector
product. However, one can hope that because the speed with which
computations can be done increases faster than the speed with which memory
can be streamed into a processor, this trade-off will be worthwhile.

<h4>Avoid forming the matrix explicitly</h4>

In order to find out how we can write a code that performs a matrix-vector
product, but does not need to store the matrix elements, let us start at
looking how some finite-element related matrix $A$ is assembled:
@f{eqnarray*}
A = \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T A_\mathrm{cell}
P_\mathrm{cell,{loc-glob}}.
@f}
In this formula, the matrix $P_\mathrm{cell,{loc-glob}}$ is a rectangular
matrix that defines the index mapping from local degrees of freedom in the
current cell
to the global degrees of freedom. The information from which this
operator can be built is usually encoded in the
<code>local_dof_indices</code> variable we have always used in the
assembly of matrices.

If we are to perform a matrix-vector product, we can hence use that
@f{eqnarray*}
y &=& A\cdot x = \left(\sum_{\text{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} P_\mathrm{cell,{loc-glob}}\right) \cdot x
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
A_\mathrm{cell} x_\mathrm{cell}
\\
&=& \sum_{\mathrm{cell}=1}^{\mathrm{n\_cells}} P_\mathrm{cell,{loc-glob}}^T
y_\mathrm{cell},
@f}
where $x_\mathrm{cell}$ are the values of <i>x</i> at the degrees of freedom
of the respective cell, and $y_\mathrm{cell}$ correspondingly for the result.
A naive attempt to implement the local action of the Laplacian would hence be
to use the following code:
@code
MatrixFree::vmult (Vector<double>       &dst,
		   const Vector<double> &src) const
{
  dst = 0;

  QGauss<dim>  quadrature_formula(fe.degree+1);
  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_gradients | update_JxW_values);

  const unsigned int   dofs_per_cell = fe.dofs_per_cell;
  const unsigned int   n_q_points    = quadrature_formula.size();

  FullMatrix<double>   cell_matrix (dofs_per_cell, dofs_per_cell);
  Vector<double>       cell_src (dofs_per_cell),
   		       cell_dst (dofs_per_cell);

  std::vector<unsigned int> local_dof_indices (dofs_per_cell);

  typename DoFHandler<dim>::active_cell_iterator
    cell = dof_handler.begin_active(),
    endc = dof_handler.end();
  for (; cell!=endc; ++cell)
    {
      cell_matrix = 0;
      fe_values.reinit (cell);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int i=0; i<dofs_per_cell; ++i)
	  for (unsigned int j=0; j<dofs_per_cell; ++j)
            cell_matrix(i,j) += (fe_values.shape_grad(i,q_point) *
                                 fe_values.shape_grad(j,q_point) *
                                 fe_values.JxW(q_point));

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      cell_matrix.vmult (cell_dst, cell_src);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst;
    }
}
@endcode

Here we neglected boundary conditions as well as any hanging nodes we
may have, though neither would be very difficult to
include using the ConstraintMatrix class. Note how we first generate the local
matrix in the usual way. To form the actual product as expressed in the
above formula, we read in the values of <code>src</code> of the cell-related
degrees of freedom, multiply by the local matrix, and finally add the result
to the destination vector <code>dst</code>. It is not more difficult than
that, in principle.

While this code is completely correct, it is very slow. For every cell, we
generate a local matrix, which takes three nested loops with as many
elements as there are degrees of freedom on the actual cell to compute. The
multiplication itself is then done by two nested loops, which means that it
is much cheaper.

One way to improve this is to realize that conceptually the local
matrix can be thought of as the product of three matrices,
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D_\mathrm{cell} B_\mathrm{cell},
@f}
where for the example of the Laplace operator 
the $(q*\mathrm{dim}+d,i)$-th element of <i>B</i><sub>cell</sub> is given by
<code>fe_values.shape_grad(i,q)[d]</code>. The matrix consists of
<code>dim*n_q_pointsdofs_per_cell</code> rows and <code>dofs_per_cell</code>
columns). The matrix <i>D</i><sub>cell</sub> is diagonal and contains the values
<code>fe_values.JxW(q)</code> (or, rather, <code>dim</code> copies of it).

Every numerical analyst learns in one of her first classes that for
forming a product of the form
@f{eqnarray*}
A_\mathrm{cell}\cdot x_\mathrm{cell} = B_\mathrm{cell} D_\mathrm{cell} 
		     B_\mathrm{cell}^T \cdot x_\mathrm{cell},
@f}
one should never form the matrix-matrix products, but rather multiply
with the vector from right to left so that only matrix-vector products are
formed. To put this into code, we can write:
@code
...
  for (; cell!=endc; ++cell)
    {
      fe_values.reinit (cell);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      temp_vector = 0;
      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
          for (unsigned int i=0; i<dofs_per_cell; ++i)
	    temp_vector(q_point*dim+d) += fe_values.shape_grad(i,q_point)[d] *
	    			          cell_src(i);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
	  temp_vector(q_point*dim+d) *= fe_values.JxW(q_point);

      cell_dst = 0;
      for (unsigned int i=0; i<dofs_per_cell; ++i)
        for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
          for (unsigned int d=0; d<dim; ++d)
	    cell_dst(i) += fe_values.shape_grad(i,q_point)[d] *
	         	   temp_vector(q_point*dim+d);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst(i);
    }
}
@endcode

This removed the three nested loops in the calculation of the local matrix
(here the loop over <i>d</i> is a not really a loop, rather two or three
operations). What happens is as follows: We first transform the vector of
values on the local dofs to a vector of gradients on the quadrature
points. In the second loop, we multiply these gradients by the integration
weight. The third loop applies the second gradient (in transposed form), so
that we get back to a vector of (Laplacian) values on the cell dofs.

This improves the situation a lot and reduced the complexity of the product
from something like $\mathcal {O}(\mathrm{dofs\_per\_cell}^3)$ to $\mathcal
{O}(\mathrm{dofs\_per\_cell}^2)$. In fact, all the remainder is just to make
a slightly more clever use of data in order to gain some extra speed. It does
not change the code structure, though.

If one would implement the code above, one would soon realize that the
operations done by the call <code>fe_values.reinit(cell)</code> take about
as much time as the other steps together (at least if the mesh is
unstructured; deal.II can recognize that the gradients are often unchanged
on structured meshes). That is certainly not ideal and we would like to do
better than this. What the reinit function does is to calculate the
gradient in real space by transforming the gradient on the reference cell
using the Jacobian of the transformation from real to reference cell. This is
done for each basis function on the cell on each quadrature point. The
Jacobian does not depend on the basis function, but it is different on
different quadrature points in general. The trick is now to factor out the
Jacobian transformation and first apply the operation that leads us to
<code>temp_vector</code> only with the gradient on the reference cell. That
transforms the vector of values on the local dofs to a vector of gradients
on the quadrature points. There, we first apply the Jacobian that we
factored out from the gradient, then we apply the weights of the quadrature,
and we apply with the transposed Jacobian for preparing the third loop which
again uses the gradients on the unit cell.

Let us again write this in terms of matrices. Let the matrix
<i>B</i><sub>cell</sub> denote the cell-related gradient matrix, with each row
containing the values of the quadrature points. It is constructed by a
matrix-matrix product as
@f{eqnarray*}
B_\mathrm{cell} = J_\mathrm{cell} B_\mathrm{ref\_cell},
@f}
where <i>B</i><sub>ref_cell</sub> denotes the gradient on the reference cell
and <i>J</i><sub>cell</sub><sup>T</sup> denotes the
Jacobian. <i>J</i><sub>cell</sub><sup>T</sup> is block-diagonal, and the
blocks size is equal to the dimension of the problem. Each diagonal block is
the Jacobian transformation that goes from the reference cell to the real 
cell.

Putting things together, we find that
@f{eqnarray*}
A_\mathrm{cell} = B_\mathrm{cell}^T D B_\mathrm{cell} 
		= B_\mathrm{ref\_cell}^T J_\mathrm{cell}^T 
		  D_\mathrm{cell} 
		  J_\mathrm{cell} B_\mathrm{ref\_cell},
@f}
so we calculate the product (starting from the right)
@f{eqnarray*}
B_\mathrm{ref\_cell}^T J_\mathrm{cell}^T D J_\mathrm{cell} 
B_\mathrm{ref\_cell}\cdot x_\mathrm{cell}.
@f}
@code
...
  FEValues<dim> fe_values_reference (fe, quadrature_formula,
                           	     update_gradients);
  Triangulation<dim> reference_cell;
  GridGenerator::hyper_cube(reference_cell, 0., 1.);
  fe_values_reference.reinit (reference_cell.begin());

  FEValues<dim> fe_values (fe, quadrature_formula,
                           update_inverse_jacobians | update_JxW_values);

  for (; cell!=endc; ++cell)
    {
      fe_values.reinit (cell);

      cell->get_dof_indices (local_dof_indices);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        cell_src(i) = src(local_dof_indices(i));

      temp_vector = 0;
      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        for (unsigned int d=0; d<dim; ++d)
          for (unsigned int i=0; i<dofs_per_cell; ++i)
	    temp_vector(q_point*dim+d) +=
	      fe_values_reference.shape_grad(i,q_point)[d] * cell_src(i);

      for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
        {
          // apply the Jacobian of the mapping from unit to real cell
	  Tensor<1,dim> temp;
	  for (unsigned int d=0; d<dim; ++d)
	    temp[d] = temp_vector(q_point*dim+d);
	  for (unsigned int d=0; d<dim; ++d)
	    {
	      double sum = 0;
	      for (unsigned int e=0; e<dim; ++e)
	      	sum += fe_values.inverse_jacobian(q_point)[d][e] *
	       	       temp[e];
	      temp_vector(q_point*dim+d) = sum;
	    }

          // multiply with integration weight
	  for (unsigned int d=0; d<dim; ++d)
	    temp_vector(q_point*dim+d) *= fe_values.JxW(q_point);

          // apply the transpose of the Jacobian of the mapping from unit
	  // to real cell
	  for (unsigned int d=0; d<dim; ++d)
	    temp[d] = temp_vector(q_point*dim+d);
	  for (unsigned int d=0; d<dim; ++d)
	    {
	      double sum = 0;
	      for (unsigned int e=0; e<dim; ++e)
	      	sum += fe_values.inverse_jacobian(q_point)[e][d] *
	    	       temp[e];
	      temp_vector(q_point*dim+d) = sum;
	    }
        }

      cell_dst = 0;
      for (unsigned int i=0; i<dofs_per_cell; ++i)
        for (unsigned int q_point=0; q_point<n_q_points; ++q_point)
          for (unsigned int d=0; d<dim; ++d)
	    cell_dst(i) += fe_values_reference.shape_grad(i,q_point)[d] *
	               	   temp_vector(q_point*dim+d);

      for (unsigned int i=0; i<dofs_per_cell; ++i)
        dst(local_dof_indices(i)) += cell_dst(i);
    }
}
@endcode

Note how we create an additional FEValues object for the reference cell
gradients and how we initialize it to the reference cell. The actual
derivative data is then applied by the Jacobians (deal.II calls the Jacobian
matrix from unit to real cell inverse_jacobian, because the transformation
direction in deal.II is from real to unit cell).

To sum up, we want to look at the additional costs introduced by this
written-out matrix-matrix-vector product compared to a sparse matrix. To first
approximation, we have increased the number of operations for the local
matrix-vector product by a factor of 4 in 2D and 6 in 3D, i.e., two
matrix-vector products with matrices that have <code>dim</code> as many
columns than before (here we assume that the number of quadrature points is
the same as the number of degrees of freedom per cell, which is usual for
scalar problems). Then, we also need to keep in mind that we touch some
degrees of freedom several times because they belong to several cells. This
also increases computational costs. A realistic value compared to a sparse
matrix is that we now have to perform about 10 times as many operations (a bit
less in 2D, a bit more in 3D).

The above is, in essence, what happens in the code below and if you have
difficulties in understanding the implementation, you should try to first
understand what happens in the code above. In the actual implementation
there are a few more points done to be even more efficient, namely:
<ul>
  <li>We pre-compute the inverse of the Jacobian of the transformation and
      store it in an extra array. This allows us to fuse the three
      operations <i>J</i><sub>cell</sub><sup>T</sup><i> D </i>
      <i>J</i><sub>cell</sub> (apply Jacobian, multiply by 
      weights, apply transposed
      Jacobian) into one second-rank tensor that is also symmetric (so we
      can save only half the tensor).
  <li>We work on several cells at once when we apply the gradients of the
      unit cell (it is always the same matrix with the reference cell
      data). This allows us to replace the matrix-vector product by a
      matrix-matrix product (several vectors of cell-data form a matrix),
      which enables a faster implementation. Obviously, we need some adapted
      data structures for that, but it isn't too hard to provide that. What
      is nice is that dense matrix-matrix products are close to today's
      processors' peak performance if the matrices are neither too small nor
      too large &mdash; and these operations are the most expensive part in
      the implementation shown here. 
</ul>

The implementation of the matrix-free matrix-vector product shown in this
tutorial is slower than a matrix-vector product using a sparse matrix for
linear and quadratic elements, but on par with third order elements and faster
for even higher order elements. An additional gain with this implementation is
that we do not have to build the sparse matrix itself, which can also be quite
expensive depending on the underlying differential equation.


<h4>Parallelization issues</h4>

We mentioned in the philosophical section above that parallelization with
multiple cores in a shared memory machine is an issue where sparse
matrix-vector products are not particularly well suited because processors are
memory bandwidth limited. There is a lot of data traffic involved, and the
access patterns in the source vector are not very regular. Also, different
rows might
have different %numbers of nonzero elements. The matrix-free implementation,
however, is more favorable in this respect. It does not need to save all the
elements (only the product of transposed Jacobian, weights, and Jacobian, for
all quadrature points on all cells, which is about 4 times the size of the
solution vector in 2D and 9 times the size of the solution vector in 3D),
whereas the number of nonzeros grows with the element order. Moreover, most of
the work is done on a very regular pattern with stride-one access to data:
Performing matrix-vector products with the same matrix, performing (equally
many) transformations on the vector related quadrature points, and doing one
more matrix-vector product. Only the read operation from the global vector
<code>src</code> and the write operation to <code>dst</code> in the end
require random access to a vector. This kind of rather uniform data access
should make it not too difficult to implement a matrix-free
matrix-vector product on a <a
href="http://en.wikipedia.org/wiki/GPGPU">graphics processing unit
(GP-GPU)</a>, for example. On the contrary, it would be quite complex to make
a sparse matrix-vector product implementation efficient on a GPU.

For our program, we choose to follow a simple strategy to make the code
%parallel: We let several processors work together by splitting the complete
set of all active cells on which we have to assemble into
several chunks. The threading building blocks implementation of a %parallel
pipeline implements this concept using the WorkStream::run() function. What
the pipeline does closely resembles the work done by a for loop. However, it
can be instructed to do some part of the loop body by just one process at a
time and in natural order. We need to use this for writing the local
contributions into the global vector, in order to avoid a race condition.


<h3>Combination with multigrid</h3>

Above, we have gone to significant lengths to implement a matrix-vector
product that does not actually store the matrix elements. In many user
codes, however, one wants more than just performing some uncertain number of
matrix-vector products &mdash; one wants to do as little of these operations
as possible when solving linear equation systems. In theory, we could use
the CG method without preconditioning; however, that would not be very
efficient. Rather, one uses preconditioners for improving speed. On the
other hand, most of the more frequently used preconditioners such as Jacobi,
SSOR, ILU or algebraic multigrid (AMG) can now no longer be used here
because their implementation requires knowledge of the elements of the
system matrix.

One solution is to use multigrid methods as shown in @ref step_16
"step-16". They are known to be very fast, and they are suitable for our
purpose since they can be designed based purely on matrix-vector products. All
one needs to do is to find a smoother that works with matrix-vector products
only (our choice requires knowledge of the diagonal entries of the matrix,
though). One such candidate would be a damped Jacobi iteration, but that is
often not sufficiently good in damping high-frequency errors.
A Chebyshev preconditioner, eventually, is what we use here. It can be
seen as an extension of the Jacobi method by using Chebyshev polynomials. With
degree zero, the Jacobi method with optimal damping parameter is retrieved,
whereas higher order corrections improve the smoothing properties if some
parameters are suitably chosen. The effectiveness of Chebyshev smoothing in
multigrid has been demonstrated, e.g., in the article <i>M. Adams, M. Brezina,
J. Hu, R. Tuminaro. Parallel multigrid smoothers: polynomial versus
Gauss&ndash;Seidel, J. Comput. Phys. 188:593&ndash;610, 2003</i>. This
publication also identifies one more advantage of Chebyshev smoothers that we
exploit here, namely that they are easy to parallelize, whereas
SOR/Gauss&ndash;Seidel smoothing relies on substitutions, which can often only
be parallelized by working on diagonal sub-blocks of the matrix, which
decreases efficiency.

The implementation into the multigrid framework is then straightforward. We
will only need some minor modifications compared to @ref step_16 "step-16".

<h3>The test case</h3>

In order to demonstrate the capabilities of the method, we work on a rather
general Poisson problem, based on a more or less unstructured mesh (where
the Jacobians are different from cell to cell), higher order mappings to a
curved boundary, and a non-constant coefficient in the equation. If we
worked on a constant-coefficient case with structured mesh, we could
decrease the operation count by a factor of 4 in 2D and 6 in 3D by building
a local matrix (which is then the same for all cells), and doing the
products as in the first developing step of the above code pieces.
