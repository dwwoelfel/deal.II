DEAL.II TESTSUITE README
========================

TODO: Introduction



General test layout
===================

A test usually consists of a source file and an output file for
comparison (under SOURCE_DIR/tests):

  category/test.cc
  category/test.output

test.cc must be a regular executable (i.e. having an int main() routine).
It will be compiled, linked and run. The executable should not output
anything to cout (at least under normal circumstances, i.e. no error
condition), instead the executable should output to a file "output" under
the current working directory.


As a last stage the generated output during the run stage will be compared
to category/test.output.

The full file signature for a comparison file is

  category/test.[with_<feature>=<on|off>.]*[mpirun=<x>.][<debug|release>.]output

which is explained in detail below.


Restrict tests for build configurations
---------------------------------------

Normally, a test will be set up for debug and release configuration (if
deal.II was configured with combined DebugRelease build type) or for the
available build configuration (if deal.II was configured either with Debug
or with Release only build type).

If a specific test can only be run in debug or release configurations but
not in both it is possible to restrict the setup by prepeding ".debug" or
".release" directly before ".output", e.g.:

  category/test.debug.output

This way, test will only be set up to build and run against the debug
library.

Note: It is possible to provide both configuration types at the same time:

  category/test.debug.output
  category/test.release.output

This will set up two seperate tests, one for the debug configuration that
will be tested against test.debug.output, and similarly one for release.


Restrict tests for feature configurations
-----------------------------------------

In a similar vain as for build configurations, it is possible to restrict
tests to specific feature configurations, e.g.:

  category/test.with_umfpack=on.output, or
  category/test.with_zlib=off.output

These tests will only be set up if the specified feature was configured
accordingly.

Note: It is possible to provide different output files for disabled/enabled
features, e.g.

  category/test.with_64bit_indices=on.output
  category/test.with_64bit_indices=off.output


Note: It is possible to declare multiple constraints subsequently, e.g.

  category/test.with_umfpack=on.with_zlib=on.output

Note: Quite a number of test categories are already guarded so that the
contained tests will only be set up if the feature is enabled. In this case
a feature constraint in the output file name is redundant and should be
avoided. (Folder with guards are distributed_grids, lapack, metis, petsc,
slepc, trilinos, umfpack, gla, mpi)


Run mpi tests with mpirun
-------------------------

If a test should be run with mpirun in parallel, specify the number x of
simultaneous processes in the following way:

  category/test.mpirun=x.output

Note: It is possible to provide multiple output files for different mpirun
values.


How to set up and run the testsuite
===================================

To enable the testsuite, configure and build deal.II in a build directory
as normal (installation is not necessary). After that you can setup the
testsuite via the "setup_test" target:

  # make setup_test

Now, the testsuite can be run in the _build directory_ via

  # ctest [-j x]

where x is the number of concurrent tests that should be run. If you only
want to run a subset of tests matching a regular expression, you can use

  # ctest [-j x] -R '<regular expression>'

To get verbose output of tests (which is otherwise just logged into
Testing/Temporary/LastTest.log) specify -V, alternatively if you're just
interested in verbose output of failing test, --output-on-failure.

Note: You can also invoke ctest under BUILD_DIR/tests or any subdirectory
under BUILD_DIR/tests. This will only invoke the tests that are located
under the subdirectory.

Note: TODO: Get and install numdiff to minimize false positives.

Note: The testsuite is huge (!) and will need around 12h on current
computer running single threaded. Consider configuring only a subset of
tests as discussed below.


CMake configuration variables for the testsuite
-----------------------------------------------

The testsuite has the following options:

  TEST_DIFF
    - the diff tool and command line to use for comparison. If numdiff is
      available it defaults to "numdiff -a 1e-6 -q", otherwise plain diff
      is used.

  TEST_TIME_LIMIT
    - The time limit (in seconds) a single test is allowed to run. Defaults
      to 180 seconds

  TEST_PICKUP_REGEX
    - A regular expression to filter tests. If this is a nonempty string
      only tests that match the regular expression will be set up. An empty
      string is interpreted as a catchall.

These options can be set as environment variables prior to the call to the
setup_test target:

  TEST_PICKUP_REGEX="build_tests/" TEST_TIME_LIMIT="120" make setup_test

Note: Specifying these options via environment variables is volatile, i.e.
if $ make setup_test is invoked a second time without the variables set in
environment, the option will be reset to the default value.

If you want to set these options permanently, set them via cmake as CMake
variable in the build directory:

  # cmake -DTEST_PICKUP_REGEX="<regular expression" .

Please note that a variable set via cmake always _overrides_ one set via
environment. If you wish to reset such a variable again, undefine it in the
cache:

  # cmake -UTEST_PICKUP_REGEX .


Setup only a subset of tests
----------------------------

It is possible to set up only a subset of tests that match a regular
expression during test setup.

  # make setup_test TEST_PICKUP_REGEX="<regular expression>"

Alternatively, you can specify TEST_PICKUP_REGEX upon configuration:

  # cmake -DTEST_PICKUP_REGEX="<regular expression" .

Note: A TEST_PICKUP_REGEX set via cmake always _overrides_ one set via
environment. If you wish to disable this filter again, undefine
TEST_PICKUP_REGEX in the cache:

  # cmake -UTEST_PICKUP_REGEX .


Use Ninja as a (GNU) Make replacement to speedup test significantly
-------------------------------------------------------------------

TODO


How to interpret the output?
============================

TODO: Write and document the following

  - How a normal run looks like.

  - Intermediate files generated under BUILD_DIR.

    BUILD_DIR/tests/category/test[.mpirun=x].<debug|release>

      output our failing_output
      diff or failing_diff

  - How to get verbose output for failing tests. -V and --output-on-failure

  - how to interpet the output

    test: BUILD successful - executable was compiled and linked successfully
    test: RUN successful   - the executable run successfully returning no error
                             condition
    test: DIFF successful  - The output matches the stored result; test
                             passed
